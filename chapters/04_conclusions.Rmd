# Conclusions

In this work, we introduced a novel cartridge case comparison algorithm designed with the explicit intention to be accessible, both in terms of literal acquisition as well as comprehensibility, to fellow researchers and others in the firearm and tool mark community.

In Chapter 2, we discussed and implemented a general pipeline structure for a cartridge case comparison algorithm that adheres to the "tidy" principles of design.
We demonstrated how this modularized structure makes it easy to experiment with and understand different components of the algorithm.
We hope that the structure available in the `cmcR` R package can be used in the future to easily improve individual pieces of the pipeline rather than re-inventing the wheel with an entirely new algorithm.

In Chapter 3, we introduced a suite of visual diagnostic statistics to aid the user of the algorithm in exploring its behavior.
We considered a variety of use-cases in which the diagnostic tools either indicated when a tweak to the algorithm was warranted or illuminated the similarities and differences between two cartridge case surfaces.
We hope that the diagnostic tools implemented in the `impressions` R package and accompanying `cartridgeInvestigatR` web application will prove useful to both researchers and other stakeholders in understanding the inner-workings of the comparison algorithm.

In Chapter 4, we developed the Automatic Cartridge Evidence Scoring (ACES) algorithm that fuses previously-established sub-procedures of the cartridge case comparison pipeline with novel pre-processing, comparing, and scoring techniques.
Using a train/test cross-validation procedure, we demonstrated how statistical models can be used to learn associations between numerical features to effectively distinguish between match and non-match comparisons.
We hope that the foundation laid by the ACES comparison pipeline, and available in the `scored` R package, will be built upon with future feature engineering and model exploration.

In future work, we plan on exploring methods for computing score-based likelihood ratios (SLRs) as means of measuring the probativity, rather than just the similarity, for two pieces of evidence.
Likelihood ratio methods take both similarity *and* typicality [@Morrison2018] into account when comparing evidence, which more directly addresses the degree to which the evidence supports the same vs. different source hypotheses.
For example, suppose a sample of blood from a suspect has the same ABO type as a sample found at a crime scene.
If we were to only consider blood type as a measure of similarity, then these two samples would be "perfectly" similar.
Of course, this is not conclusive evidence that the suspect was actually at the crime scene - there is a relatively large probability that any two randomly drawn blood samples from different humans would having matching ABO type.
That is, it is not atypical to observe this degree of similarity between two randomly drawn, different source samples.
The same can be extended to cartridge case evidence.
Even if two cartridge cases receive a relatively high similarity score, say 0.97, we must consider the likelihood that two randomly drawn, different source cartridge cases receive the same similarity score.
If this value is also large, then the original score is not particularly probative.
We hope to take a similar approach as @Reinders2022 to explore various SLR computation methods.

An important avenue of exploration is in the generalizability of a model like ACES to other conditions.
That is, how robust is our currently trained version of ACES to the make/model of the cartridge case or firearm, the scanning tool used, or the degree of wear/degradation on the casing?
If this current iteration of ACES turns out to be sensitive to these conditions, could it be made more robust by simply training on a more representative sample of cartridge cases, or will we need to individually train one model for each combination of factors?
Fundamentally, these questions point to whether breech face impressions are consistent enough that we can apply the same procedure to any pair of cartridge cases.
Although we don't have the answers to these questions, we do have the infrastructure across the various software we've created to start answering them.

Using ACES, we are able to achieve a much better balance between the true negative and positive classification compared to the examiner decision results reported in @Baldwin2014, which had a much lower true negative rate compared to the true positive.
In fact, if we use classification accuracy as an optimization during training, our results actually complement the @Baldwin2014 reported results in that we obtain a near-perfect true negative rate (99.9\%) and relatively high true positive rate (92.4\%).
Through our experimentation, it became clear to us that it is harder to algorithmically identify matching comparisons than non-matches.
Specifically, it seems to us that there are many more ways in which two scans can appear different to the algorithm than similar.
As we discussed in Chapter 3, extreme, non-breech face markings in the scan that aren't removed during pre-processing may "distract" the registration procedure on which our pipeline heavily relies.
Other cartridge cases might have unremarkable impressions, leading to a similarity score that doesn't strongly indicate matches or non-matches (what forensic examiners might call an "inconclusive").
The visual diagnostic tools developed in this work make it easier to manually inspect and identify these cases.
However, we are also interested in developing "automatic" techniques to characterize and identify markings.
For example, we might use texture modelling and segmentation to identify distinctive markings on a surface.
Alternatively, we could use a correlation measure and/or registration procedure that is more robust to extreme markings.

@Morrison2018 provide a compelling argument that "[p]rocedures based on similarity-only scores do not appropriately account for typicality with respect to the relevant population," which seems to directly oppose the approach we took with ACES.
However, we believe that the arguments proposed are relevant to specific types of "similarity-only" scores, such as a Euclidean distance or correlation, that are invariant to specific evidence being compared.
We do not believe that ACES exhibits the same invariance as these simpler scoring procedures and we hope to explore this further in the future.

Nonetheless, we do think that the alternative approaches to measuring probativity presented in @Morrison2018 as well as @Basu2022 have merit.
Specifically, rather than computing comparative features that map a pair of scans to a number, such as a correlation, these authors propose computing descriptive features independently for each scan and then modeling the joint distribution of these features as a means of computing SLRs.
For example, @Basu2022 considers decomposing a cartridge case scan using various two-dimensional orthonormal bases as a means of capturing different characteristics of the surface.
Doing this independently for two cartridge cases allows one to consider the joint likelihood of observing these feature values (or some dimension-reduced mapping of these features) under the same and different source hypotheses, which sidesteps distilling these features into a univariate similarity score as is done in ACES.
Possible descriptive features could come from sources such as decompositions like those used in @Basu2022, local feature detector methods such as "Speeded Up Robust Features" @Bay2006, or even autoencoding/generative neural network models although these features would be harder to interpret.

By now, we hope that it is clear to the reader that a goal pervading every aspect of this work is transparency.
Given the gravity of the application, we consider it necessary that tools used in making an evidentiary conclusion that could inform a judicial decision be as transparent and user-friendly as possible.
As discussed in Chapter 2, this means structuring an algorithm to be flexible and sharing data or code if at all possible.
As discussed in Chapter 3, this means creating supplementary tools that help the user understand the methods they're applying.
As discussed in Chapter 4, this means building interpretable and effective features and models.
Doing so would inevitably lead to stronger collaboration between research teams,  expedited improvements to the underlying methods, and a more equitable and trustworthy justice system.
We hope that this work stands as a testament to the firearm and tool mark community that algorithms applicable to casework can simultaneously be effective, accessible, *and* approachable if intention is put towards the endeavor.

