<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Diagnostic Tools for Cartridge Case Comparison Algorithms | A Cartridge Case Comparison Pipeline</title>
  <meta name="description" content="3 Diagnostic Tools for Cartridge Case Comparison Algorithms | A Cartridge Case Comparison Pipeline" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Diagnostic Tools for Cartridge Case Comparison Algorithms | A Cartridge Case Comparison Pipeline" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="jzemmels/cartridgeCaseLitReview" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Diagnostic Tools for Cartridge Case Comparison Algorithms | A Cartridge Case Comparison Pipeline" />
  
  
  

<meta name="author" content="Joseph Zemmels" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"/>
<link rel="next" href="automatic-matching-of-cartridge-case-impressions.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Literature Review</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preliminaries-forensic-examinations"><i class="fa fa-check"></i><b>1.1</b> Preliminaries: Forensic Examinations</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#firearm-and-toolmark-identification"><i class="fa fa-check"></i><b>1.1.1</b> Firearm and Toolmark Identification</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#why-should-firearm-and-toolmark-identification-change"><i class="fa fa-check"></i><b>1.1.2</b> Why Should Firearm and Toolmark Identification Change?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#forensic-comparison-pipelines"><i class="fa fa-check"></i><b>1.2</b> Forensic Comparison Pipelines</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#digital-representations-of-evidence"><i class="fa fa-check"></i><b>1.2.1</b> Digital Representations of Evidence</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#pre-processing-procedures-for-forensic-data"><i class="fa fa-check"></i><b>1.2.2</b> Pre-processing Procedures for Forensic Data</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#forensic-data-feature-extraction"><i class="fa fa-check"></i><b>1.2.3</b> Forensic Data Feature Extraction</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#similarity-scores-classification-rules-for-forensic-data"><i class="fa fa-check"></i><b>1.2.4</b> Similarity Scores &amp; Classification Rules for Forensic Data</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#reproducibility-of-comparison-pipelines"><i class="fa fa-check"></i><b>1.2.5</b> Reproducibility of Comparison Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#diagnostic-tools"><i class="fa fa-check"></i><b>1.3</b> Diagnostic Tools</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#visual-diagnostics"><i class="fa fa-check"></i><b>1.3.1</b> Visual Diagnostics</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#interactive-diagnostics"><i class="fa fa-check"></i><b>1.3.2</b> Interactive Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#automating-and-improving-the-cartridge-case-comparison-pipeline"><i class="fa fa-check"></i><b>1.4</b> Automating and Improving the Cartridge Case Comparison Pipeline</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#image-processing-techniques"><i class="fa fa-check"></i><b>1.4.1</b> Image Processing Techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#density-based-spatial-clustering-of-applications-with-noise"><i class="fa fa-check"></i><b>1.4.2</b> Density-Based Spatial Clustering of Applications with Noise</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#implementation-considerations"><i class="fa fa-check"></i><b>1.4.3</b> Implementation Considerations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><i class="fa fa-check"></i><b>2</b> A Study in Reproducibility: The Congruent Matching Cells Algorithm and cmcR package</a>
<ul>
<li class="chapter" data-level="" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="2.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#intro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#repeatability-and-reproducibility"><i class="fa fa-check"></i><b>2.1.1</b> Repeatability and reproducibility</a></li>
<li class="chapter" data-level="2.1.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#the-congruent-matching-cells-algorithm"><i class="fa fa-check"></i><b>2.1.2</b> The Congruent Matching Cells algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#cmcMethod"><i class="fa fa-check"></i><b>2.2</b> The CMC pipeline</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#initialData"><i class="fa fa-check"></i><b>2.2.1</b> Initial data</a></li>
<li class="chapter" data-level="2.2.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#preProcessing"><i class="fa fa-check"></i><b>2.2.2</b> Pre-processing procedures</a></li>
<li class="chapter" data-level="2.2.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#comparisonProcedure"><i class="fa fa-check"></i><b>2.2.3</b> “Correlation cell” comparison procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#decision-rule"><i class="fa fa-check"></i><b>2.2.4</b> Decision rule</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#discussion"><i class="fa fa-check"></i><b>2.3</b> Discussion</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#ambiguities"><i class="fa fa-check"></i><b>2.3.1</b> Ambiguity in algorithmic descriptions</a></li>
<li class="chapter" data-level="2.3.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#investigation"><i class="fa fa-check"></i><b>2.3.2</b> CMC pattern matching pipeline</a></li>
<li class="chapter" data-level="2.3.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#processing-condition-sensitivity"><i class="fa fa-check"></i><b>2.3.3</b> Processing condition sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#conclusion"><i class="fa fa-check"></i><b>2.4</b> Conclusion</a></li>
<li class="chapter" data-level="2.5" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#acknowledgement"><i class="fa fa-check"></i><b>2.5</b> Acknowledgement</a></li>
<li class="chapter" data-level="2.6" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#computational-details"><i class="fa fa-check"></i><b>2.6</b> Computational details</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><i class="fa fa-check"></i><b>3</b> Diagnostic Tools for Cartridge Case Comparison Algorithms</a>
<ul>
<li class="chapter" data-level="" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#abstract-1"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#background-and-introduction"><i class="fa fa-check"></i><b>3.2</b> Background and Introduction</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#notational-conventions"><i class="fa fa-check"></i><b>3.2.1</b> Notational Conventions</a></li>
<li class="chapter" data-level="3.2.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#registration-procedure"><i class="fa fa-check"></i><b>3.2.2</b> Registration Procedure</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostics-1"><i class="fa fa-check"></i><b>3.3</b> Visual Diagnostics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-x3p-plot"><i class="fa fa-check"></i><b>3.3.1</b> The X3P Plot</a></li>
<li class="chapter" data-level="3.3.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-comparison-plot"><i class="fa fa-check"></i><b>3.3.2</b> The Comparison Plot</a></li>
<li class="chapter" data-level="3.3.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics"><i class="fa fa-check"></i><b>3.3.3</b> Visual Diagnostic Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#statistical-learning-from-visual-diagnostics"><i class="fa fa-check"></i><b>3.4</b> Statistical Learning from Visual Diagnostics</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics-as-features"><i class="fa fa-check"></i><b>3.4.1</b> Visual Diagnostic Statistics as Features</a></li>
<li class="chapter" data-level="3.4.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#binary-classification-results"><i class="fa fa-check"></i><b>3.4.2</b> Binary Classification Results</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#discussion-1"><i class="fa fa-check"></i><b>3.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#case-studies"><i class="fa fa-check"></i><b>3.5.1</b> Case Studies</a></li>
<li class="chapter" data-level="3.5.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#sensitivity-to-filter-threshold"><i class="fa fa-check"></i><b>3.5.2</b> Sensitivity to Filter Threshold</a></li>
<li class="chapter" data-level="3.5.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#interactive-cartridgeinvestigatr-application"><i class="fa fa-check"></i><b>3.5.3</b> Interactive cartridgeInvestigatR application</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#conclusion-1"><i class="fa fa-check"></i><b>3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html"><i class="fa fa-check"></i><b>4</b> Automatic Matching of Cartridge Case Impressions</a>
<ul>
<li class="chapter" data-level="" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#abstract-2"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="4.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#previous-work"><i class="fa fa-check"></i><b>4.1.1</b> Previous Work</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#cartridge-case-data"><i class="fa fa-check"></i><b>4.2</b> Cartridge Case Data</a></li>
<li class="chapter" data-level="4.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#methods"><i class="fa fa-check"></i><b>4.3</b> Methods</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#pre-processing"><i class="fa fa-check"></i><b>4.3.1</b> Pre-processing</a></li>
<li class="chapter" data-level="4.3.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#comparing"><i class="fa fa-check"></i><b>4.3.2</b> Comparing</a></li>
<li class="chapter" data-level="4.3.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#scoring"><i class="fa fa-check"></i><b>4.3.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#results"><i class="fa fa-check"></i><b>4.4</b> Results</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#roc-curves"><i class="fa fa-check"></i><b>4.4.1</b> ROC Curves</a></li>
<li class="chapter" data-level="4.4.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#optimized-model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Optimized Model Comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#similarity-score-investigation"><i class="fa fa-check"></i><b>4.4.3</b> Similarity Score Investigation</a></li>
<li class="chapter" data-level="4.4.4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#feature-importance"><i class="fa fa-check"></i><b>4.4.4</b> Feature Importance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#discussion-2"><i class="fa fa-check"></i><b>4.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#comparison-to-cmc-methodology"><i class="fa fa-check"></i><b>4.5.1</b> Comparison to CMC Methodology</a></li>
<li class="chapter" data-level="4.5.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#sensitivity-to-parameter-choice"><i class="fa fa-check"></i><b>4.5.2</b> Sensitivity to Parameter Choice</a></li>
<li class="chapter" data-level="4.5.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#model-selection-considerations"><i class="fa fa-check"></i><b>4.5.3</b> Model Selection Considerations</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#conclusion-2"><i class="fa fa-check"></i><b>4.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="computational-details-1.html"><a href="computational-details-1.html"><i class="fa fa-check"></i>Computational Details</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="4.7" data-path="appendix.html"><a href="appendix.html#registration-procedure-details"><i class="fa fa-check"></i><b>4.7</b> Registration Procedure Details</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="appendix.html"><a href="appendix.html#cell-based-registration-details"><i class="fa fa-check"></i><b>4.7.1</b> Cell-Based Registration Details</a></li>
<li class="chapter" data-level="4.7.2" data-path="appendix.html"><a href="appendix.html#registration-based-feature-distributions"><i class="fa fa-check"></i><b>4.7.2</b> Registration-Based Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="appendix.html"><a href="appendix.html#dbscan-algorithm-details"><i class="fa fa-check"></i><b>4.8</b> DBSCAN Algorithm Details</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="appendix.html"><a href="appendix.html#density-based-feature-distributions"><i class="fa fa-check"></i><b>4.8.1</b> Density-Based Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="appendix.html"><a href="appendix.html#visual-diagnostic-details"><i class="fa fa-check"></i><b>4.9</b> Visual Diagnostic Details</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="appendix.html"><a href="appendix.html#visual-diagnostic-feature-distributions"><i class="fa fa-check"></i><b>4.9.1</b> Visual Diagnostic Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="appendix.html"><a href="appendix.html#model-specific-results"><i class="fa fa-check"></i><b>4.10</b> Model-Specific Results</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Cartridge Case Comparison Pipeline</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="diagnostic-tools-for-cartridge-case-comparison-algorithms" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3</span> Diagnostic Tools for Cartridge Case Comparison Algorithms<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#diagnostic-tools-for-cartridge-case-comparison-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="abstract-1" class="section level2 unnumbered hasAnchor">
<h2>Abstract<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#abstract-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Forensic comparison algorithms are useful for measuring the probative value of a collection of evidence.
A number of algorithms pre-process, compare, and return some measure of similarity for two pieces of evidence.
Rarely, however, are there mechanisms built into the algorithm to indicate when a step goes awry.
For example, poorly-calibrated pre-processing of the evidence can have substantial impact on downstream results, which may lead to incorrect or misleading conclusions.
Even for well-calibrated algorithms, the resulting similarity measure can be difficult to justify or explain.
Diagnostics are useful as a supplementary tool for explaining the behavior of a complex model or algorithm.
In this paper, we introduce a suite of visual and interactive diagnostic tools to assess algorithms that automatically compare forensic cartridge case evidence.
These tools are useful to both forensic researchers and practitioners.
Researchers can analyze and correct the (mis)behavior of the algorithm while forensic practitioners can more easily understand and explain the algorithm.
We develop a collection of tools to diagnose each step of a cartridge case comparison algorithm and implement in in an R package called <code>impressions</code>.
We then implement these visual diagnostics in an interactive web application called <code>cartridgeInvestigatR</code> to also allow non-programmers to interact with and explore these algorithms.
we provide both the <code>impressions</code> package and <code>cartridgeInvestigatR</code> as free, open-source software.</p>
</div>
<div id="introduction" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Introduction<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="background-and-introduction" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Background and Introduction<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#background-and-introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Forensic examinations are intended to provide an objective assessment of the probative value of a piece of evidence.
Typically, this assessment of probative value is performed by a forensic examiner who visually inspects the evidence to determine whether it matches evidence found on a suspect.
The process by which an examiner arrives at their evidentiary conclusion is largely opaque and has been criticized <span class="citation">(<a href="#ref-pcast2016" role="doc-biblioref">President’s Council of Advisors on Sci. &amp; Tech. 2016</a>)</span> because its subjectivity does not allow for an estimation of error rates.
In response, <span class="citation">National Research Council (<a href="#ref-council_strengthening_2009" role="doc-biblioref">2009</a>)</span> pushed to augment subjective decisions made by forensic examiners with automatic, statistically-founded algorithms that objectively assess evidence and can be explained during court testimony.
These algorithms enable the quantification of an examiner’s uncertainty by measuring the probative value of a piece of evidence.</p>
<p>A <em>cartridge case</em> (see Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:cartridgeCaseBullet">3.1</a>) is the portion of firearm ammunition that encases a projectile (e.g., bullet, shots, or slug) along with the explosive used to propel the projectile through the firearm.
When a firearm is discharged, the projectile is propelled down the barrel of the firearm, while the cartridge case is forced towards the back of the barrel.
The base of the cartridge case (Figure <a href="#fig:cartridgeCaseBase"><strong>??</strong></a>) strikes the back wall, known as the <em>breech face</em>, of the barrel with considerable force, thereby imprinting any markings on the breech face onto the cartridge case and creating the so-called <em>breech face impressions</em> (see Figure <a href="#fig:breechFaceImpressions"><strong>??</strong></a>).
These markings have been suggested to be unique to a firearm and are used in forensic examinations to determine whether two cartridge cases have been fired by the same firearm.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cartridgeCaseBullet"></span>
<img src="images/chapter3-images/cartridgeCaseZoomIn.png" alt="(Left) Fired cartridge case and bullet, (Middle) Base of the cartridge case that comes into contact with the breech face of the firearm, (Right) Zoom into the *primer* region showing breech face impressions." width="\textwidth" />
<p class="caption">
Figure 3.1: (Left) Fired cartridge case and bullet, (Middle) Base of the cartridge case that comes into contact with the breech face of the firearm, (Right) Zoom into the <em>primer</em> region showing breech face impressions.
</p>
</div>
<p>We measure the surface of a cartridge case using a TopMatch-3D High-Capacity Scanner by Cadre Forensics<span class="math inline">\(^{\text{TM}}\)</span>.
This scanner collects images under various lighting conditions of a gel pad into which the cartridge case surface is impressed and combines these images into a regular 2D array called a <em>surface matrix</em>. Examples of two such surface matrices are shown in Figure <a href="#fig:referenceAnnotated"><strong>??</strong></a> and Figure <a href="#fig:targetAnnotate"><strong>??</strong></a>}.
The physical dimensions of these objects are about 5.5 <span class="math inline">\(mm^2\)</span> captured at a resolution of 1.84 microns per pixel (1000 microns equals 1 mm).
Each element of the surface matrix corresponds to the height of the impressed gel at that location.
The nominal resolution for height measurements depends on the viscosity of the gel and is reported to be better than 1 micron.
The breech face impression regions have been manually annotated (in red).
Using this manual annotation, we isolate the breech face impression region.
Note that this introduces structurally missing values into the scan.</p>
<p>Figure <a href="#fig:processedSideBySide"><strong>??</strong></a> shows the isolated breech face impression regions where the height values of the surface have been mapped to a diverging purple (low) to orange (high) color scale and missing values are shown in gray.</p>
<p><strong>Note that due to the scanning process the physical location of any measured values is relative, but the relationship of the measurements to each other is fixed. This means that we can, without affecting any structures, translate and even rotate measurements in 3d space. For the purpose of making scans comparable to each other, the scan surfaces are translated into XXXX what is being done exactly?</strong></p>
<p><strong>XXX I’m not sure what step in the algorithm you’re referring to here. By “translated into,” do you mean a horizontal/vertical shift? </strong></p>
<p><strong>Yes, I used translation in a mathematical sense here, so any shift in x, y or z direction. Sometimes we do these steps implicitly rather than in an explicit … this is a step way. e.g. by changing from a grid to matrix we lose the physical extensions in x and y. Those are replaced by integer values first, and then rescaled into micron measurements later. That is an implicit translation in x and y. shifting a scan into a mean zero is an explicit translation, just as shifting a breech face impression is. I would very much like to use a robust estimate of the 2d breech face surface as the zero plane. their mean and any tilts in xy direction are removed. While those tilts might stem from a small misalignments of the breech face, a bigger source of variability stems from a slight misalignment during the scanning. </strong></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:referenceAnnotated-1"></span>
<img src="images/chapter3-images/topMatchAnnotatedReference.png" alt="(Left) Cartridge case scan K013sA1 with breech face (BF) impressions manually annotated in red, (Middle) Processed surface matrices, (Right) Cartridge case scan K013sA2 with breech face (BF) impressions manually annotated in red. Raw scans (left and right) and processed versions of the surface matrices (in the middle) for a pair of cartridge cases fired by the same handgun (Ruger SR9, Gun A1, SerialNo 331-96383)." width="\textwidth" />
<p class="caption">
Figure 3.2: (Left) Cartridge case scan K013sA1 with breech face (BF) impressions manually annotated in red, (Middle) Processed surface matrices, (Right) Cartridge case scan K013sA2 with breech face (BF) impressions manually annotated in red. Raw scans (left and right) and processed versions of the surface matrices (in the middle) for a pair of cartridge cases fired by the same handgun (Ruger SR9, Gun A1, SerialNo 331-96383).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:referenceAnnotated-2"></span>
<img src="images/chapter3-images/referenceTargetSideBySide.png" alt="(Left) Cartridge case scan K013sA1 with breech face (BF) impressions manually annotated in red, (Middle) Processed surface matrices, (Right) Cartridge case scan K013sA2 with breech face (BF) impressions manually annotated in red. Raw scans (left and right) and processed versions of the surface matrices (in the middle) for a pair of cartridge cases fired by the same handgun (Ruger SR9, Gun A1, SerialNo 331-96383)." width="\textwidth" />
<p class="caption">
Figure 3.3: (Left) Cartridge case scan K013sA1 with breech face (BF) impressions manually annotated in red, (Middle) Processed surface matrices, (Right) Cartridge case scan K013sA2 with breech face (BF) impressions manually annotated in red. Raw scans (left and right) and processed versions of the surface matrices (in the middle) for a pair of cartridge cases fired by the same handgun (Ruger SR9, Gun A1, SerialNo 331-96383).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:referenceAnnotated-3"></span>
<img src="images/chapter3-images/topMatchAnnotatedTarget.png" alt="(Left) Cartridge case scan K013sA1 with breech face (BF) impressions manually annotated in red, (Middle) Processed surface matrices, (Right) Cartridge case scan K013sA2 with breech face (BF) impressions manually annotated in red. Raw scans (left and right) and processed versions of the surface matrices (in the middle) for a pair of cartridge cases fired by the same handgun (Ruger SR9, Gun A1, SerialNo 331-96383)." width="\textwidth" />
<p class="caption">
Figure 3.4: (Left) Cartridge case scan K013sA1 with breech face (BF) impressions manually annotated in red, (Middle) Processed surface matrices, (Right) Cartridge case scan K013sA2 with breech face (BF) impressions manually annotated in red. Raw scans (left and right) and processed versions of the surface matrices (in the middle) for a pair of cartridge cases fired by the same handgun (Ruger SR9, Gun A1, SerialNo 331-96383).
</p>
</div>
<p><strong>XXX Great! It feels like we have reached the end of the introduction.</strong>
<strong>todo list: (1) problem statement, (2) outline what this paper is about and order in which we go about it. XXX We probably have to go back to this a couple times.</strong></p>
<p><strong>Problem statement (draft): Can we characterize when a cartridge case alignment fails?</strong></p>
<p>Even among matching cartridge case pairs, we have found results like those shown in Figure @ref(fig:cmcPlot_match), where only a small number of the total cells are classified as CMCs, to be quite common.
An underlying assumption of the CMC methodology is that many source cells should find the “correct” registration in a matching target scan.
However, it can be difficult to determine why a particular cell did or did not register correctly.
In this paper, we introduce diagnostic tools that can be used to visually and numerically characterize the quality of alignment of cells.
We also demonstrate how the numerical diagnostics can be used as features to classify matching and non-matching cartridge case pairs.</p>
<div id="notational-conventions" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Notational Conventions<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#notational-conventions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As the name implies, surface matrices are two-dimensional arrays whose elements contain relative height values of the corresponding cartridge case surface <span class="citation">(<a href="#ref-ISO25178-72" role="doc-biblioref"><span>“<span class="nocase">Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p</span>”</span> 2017</a>)</span>.
For notational simplicity, we assume that the matrices are square, i.e. <span class="math inline">\(A \in \mathbb{R}^{k \times k}\)</span> for some matrix <span class="math inline">\(A\)</span> and <span class="math inline">\(k &gt; 0\)</span>.
Note, that any assumption of sizing of matrices are easily enforced by padding with additional missing values. Due to the presence of (structural) missing values around the breech face impression, additional padding does not interfere with the structure of the scan.
We use lowercase letters with subscripts to denote a particular value of a matrix; e.g., <span class="math inline">\(a_{ij}\)</span> is the value in the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column, starting from the top left corner, of <span class="math inline">\(A\)</span>.</p>
<p>For the purpose of dealing with missing values mathematically, we adapt standard matrix algebra as follows: if an element of either matrix <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> is missing, then any element-wise operation including this element is also missing, otherwise standard matrix algebra holds.
For example, for matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B \in \mathbb{R}^{k \times k}\)</span> we define the addition operator as:
<span class="math display">\[
A \oplus_{\tiny{NA}} B = \left( a_{ij} \oplus_{\tiny{NA}} b_{ij} \right)_{1 \le i,j \le k} :=
\begin{cases}
a_{ij} + b_{ij} &amp; \text{ if both } a_{ij} \text{ and } b_{ij} \text{ are numbers}\\
\text{NA } &amp; \text{ otherwise}
\end{cases}
\]</span>
Operations <span class="math inline">\(\ominus_{\tiny{NA}}\)</span> as well as all comparisons are defined similarly. For the purpose of readability, we will use the standard algebraic operators <span class="math inline">\(+, -, &gt;, &lt;, ...\)</span> and apply the extended operations as defined above.</p>
</div>
<div id="registration-procedure" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Registration Procedure<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#registration-procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>[Include visual illustrating the full scan and cell-based registrations]</strong></p>
<p>A critical step in comparing <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is to find a transformation of <span class="math inline">\(B\)</span> such that it aligns best to <span class="math inline">\(A\)</span> (or vice versa).
In image processing, this is called <em>image registration</em>. Noting that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are essentially grayscale images, we rely on a standard image registration technique <span class="citation">(<a href="#ref-Brown1992" role="doc-biblioref">Brown 1992</a>)</span>.</p>
<p>In our application, a registration is composed of a discrete translation by <span class="math inline">\((m,n) \in \mathbb{Z}^2\)</span> and rotation by <span class="math inline">\(\theta \in [-180^\circ,180^\circ]\)</span>.
To determine the optimal registration, we calculate the <em>cross-correlation function</em> (CCF) between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, which measures the similarity between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for every possible translation of <span class="math inline">\(B\)</span>, denoted <span class="math inline">\((A \star B)\)</span>.
We estimate the registration by calculating the maximum CCF value across a range of rotations of matrix <span class="math inline">\(B\)</span>.
Let <span class="math inline">\(B_\theta\)</span> denote <span class="math inline">\(B\)</span> rotated by an angle <span class="math inline">\(\theta \in [-180^\circ,180^\circ]\)</span> and <span class="math inline">\(b_{\theta_{mn}}\)</span> the <span class="math inline">\(m,n\)</span>-th element of <span class="math inline">\(B_\theta\)</span>.
Then the estimated registration <span class="math inline">\((m^*,n^*,\theta^*)\)</span> is:</p>
<p><span class="math display">\[
(m^*,n^*,\theta^*) = \arg \max_{m,n,\theta} (a \star b_\theta)_{mn}.
\]</span></p>
<p>In practice we consider a discrete grid of rotations <span class="math inline">\(\pmb{\Theta} \subset [-180^\circ,180^\circ]\)</span>.
The registration procedure is outlined in Figure <a href="#alg:registration"><strong>??</strong></a>.
We refer to the matrix that is rotated as the “target.”
The result is the estimated registration of the target matrix to the “reference” matrix.</p>
<p><strong>Image Registration Algorithm</strong></p>
<div class="line-block"><strong>Data</strong>: Source matrix <span class="math inline">\(A\)</span>, target matrix <span class="math inline">\(B\)</span>, and rotation grid <span class="math inline">\(\pmb{\Theta}\)</span></div>
<div class="line-block"><strong>Result</strong>: Estimated registration of <span class="math inline">\(B\)</span> to <span class="math inline">\(A\)</span>, <span class="math inline">\((m^*, n^*, \theta^*)\)</span>, and cross-correlation function maximum <span class="math inline">\(CCF_{\max}\)</span></div>
<div class="line-block"><strong>for</strong> <span class="math inline">\(\theta \in \pmb{\Theta}\)</span> <strong>do</strong></div>
<div class="line-block">   Rotate <span class="math inline">\(B\)</span> by <span class="math inline">\(\theta\)</span> to obtain <span class="math inline">\(B_\theta\)</span>;</div>
<div class="line-block">   Calculate <span class="math inline">\(CCF_{\max,\theta} = \max_{m,n} (a \star b)_{mn}\)</span>;</div>
<div class="line-block">   Calculate translation <span class="math inline">\([m_{\theta}^*, n_{\theta}^*] = \arg \max_{m,n} (a \star b_\theta)_{mn}\)</span>;</div>
<div class="line-block"><strong>end</strong></div>
<div class="line-block">Calculate overall maximum correlation <span class="math inline">\(CCF_{\max} = \max_{\theta} \{CCF_{\max, \theta} : \theta \in \pmb{\Theta}\}\)</span>;</div>
<div class="line-block">Calculate rotation <span class="math inline">\(\theta^* = \arg \max_{\theta} \{CCF_{\max,\theta} : \theta \in \pmb{\Theta}\}\)</span>;</div>
<div class="line-block"><strong>return</strong> Estimated rotation <span class="math inline">\(\theta^*\)</span>, translation <span class="math inline">\(m^* = m_{\theta^*}^*\)</span>, and <span class="math inline">\(n^* = n_{\theta^*}^*\)</span>, and <span class="math inline">\(CCF_{\max}\)</span></div>
<!-- ```{r registration,echo=FALSE,fig.align='center',fig.pos="htbp",fig.cap=" "} -->
<!-- knitr::include_graphics("images/registrationAlgorithm.png") -->
<!-- ``` -->
<p></br></p>
<p>We can apply the image registration in both directions to align not only scan <span class="math inline">\(B\)</span> to <span class="math inline">\(A\)</span>, but also <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span>.
Theoretically, these two registrations should be exact opposites of each other.
However, depending on the scans this may not happen in practice because we use “nearest-neighbor” interpolation to rotate the discretely-indexed surface matrices.
To accommodate these two comparison directions, we now introduce a subscript <span class="math inline">\(d = A,B\)</span> that refers to the source scan used in the image registration.
For example, <span class="math inline">\((m_A^*, n_A^*, \theta_A^*, CCF_{\max,A})\)</span> refers to the estimated registration and CCF from aligning scan <span class="math inline">\(B\)</span> to <span class="math inline">\(A\)</span>.</p>
<p><span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> points out that two matching cartridge cases may only have a handful of regions with distinguishable, matching impressions due to inherent variability in the firing process.
Calculating a correlation between two full scans as in image registration algorithm may not highlight their similarities.
Instead, <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> proposes partitioning one of the scans into a grid of “cells” and estimating the registration between each cell and the other scan.</p>
<p>We now extend the surface matrix notation introduced previously to accommodate cells.
Let <span class="math inline">\(A_{t}\)</span> denote the <span class="math inline">\(t\)</span>th cell of matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(t = 1,...,T_A\)</span> where <span class="math inline">\(T_A\)</span> is the total number of cells containing non-missing values in scan <span class="math inline">\(A\)</span> and let <span class="math inline">\((a_t)_{ij}\)</span> denote the <span class="math inline">\(i,j\)</span>-th element of <span class="math inline">\(A_t\)</span>.
This procedure can be viewed as a generalization of image registration algorithm that we call the “cell-based comparison procedure” and outlined below.</p>
<p><strong>Cell-Based Comparison Algorithm</strong></p>
<div class="line-block"><strong>Data</strong>: Source matrix <span class="math inline">\(A\)</span>, target matrix <span class="math inline">\(B^*\)</span>, grid size <span class="math inline">\(R \times C\)</span>, and rotation grid <span class="math inline">\(\pmb{\Theta}_{A}&#39;\)</span></div>
<div class="line-block"><strong>Result</strong>: Estimated translations and <span class="math inline">\(CCF_{\max}\)</span> values per cell, per rotation</div>
<div class="line-block">Partition <span class="math inline">\(A\)</span> into a grid of <span class="math inline">\(R \times C\)</span> cells;</div>
<div class="line-block">Discard cells containing only missing values, leaving <span class="math inline">\(T_A\)</span> remaining cells;</div>
<div class="line-block"><strong>for</strong> <span class="math inline">\(\theta \in \pmb{\Theta}_{A}&#39;\)</span> <strong>do</strong></div>
<div class="line-block">   Rotate <span class="math inline">\(B^*\)</span> by <span class="math inline">\(\theta\)</span> to obtain <span class="math inline">\(B_{\theta}^*\)</span>;</div>
<div class="line-block">   <strong>for</strong> <span class="math inline">\(t = 1,...,T_A\)</span> <strong>do</strong></div>
<div class="line-block">      Calculate <span class="math inline">\(CCF_{\max,A,t,\theta} = \max_{m,n} (a_t \star b_{\theta}^*)_{mn}\)</span>;</div>
<div class="line-block">      Calculate translation <span class="math inline">\([m_{A,t,\theta}^*, n_{A,t,\theta}^*] = \arg \max_{m,n} (a_t \star b_{\theta}^*)_{mn}\)</span>;</div>
<div class="line-block">   <strong>end</strong></div>
<div class="line-block"><strong>end</strong></div>
<div class="line-block"><strong>return</strong> <span class="math inline">\(\pmb{F}_A = \{(m_{A,t,\theta}^*, n_{A,t,\theta}^*, CCF_{\max,A,t,\theta}, \theta) : \theta \in \pmb{\Theta}_{A}&#39;, t = 1,...,T_A\}\)</span></div>
<p>The output of the cell-based comparison is a set of estimated registrations - one registration for each cell in the source scan for each <span class="math inline">\(\theta\)</span>.
For a particular cell <span class="math inline">\(t \in \{1,...,T_A\}\)</span>, we select the registration that maximizes the CCF across all <span class="math inline">\(\theta \in \pmb{\Theta}_A&#39;\)</span> as its estimated registration.</p>
<p>Similar to image registration algorithm, we can use cell-based comparison to align not only cells from <span class="math inline">\(A\)</span> to <span class="math inline">\(B^*\)</span>, but also cells from <span class="math inline">\(B\)</span> to <span class="math inline">\(A^*\)</span>, which is an aligned version of scan <span class="math inline">\(A\)</span> from image registration algorithm using <span class="math inline">\(B\)</span> as source.
We again use a direction subscript <span class="math inline">\(d = A,B\)</span> to refer to the source scan in cell-based comparison.
For example, the outcome of cell-based comparison using <span class="math inline">\(B\)</span> as source and <span class="math inline">\(A^*\)</span> as target is the set <span class="math inline">\(\pmb{F}_B\)</span> of estimated registrations per cell, <span class="math inline">\(t = 1,...,T_B\)</span>, per rotation, <span class="math inline">\(\theta \in \pmb{\Theta}_B&#39;\)</span>.</p>
<p>One challenge with using registration algorithms like image registration algorithm and cell-based comparison is understanding when and how they “work” as expected.
For example, we expect for truly matching cartridge cases that these estimated registrations should “agree” across various cells.
In other words, that <span class="math inline">\((m^*_{A,t,\theta},n^*_{A,t,\theta}, \theta)\)</span> should be the same for all <span class="math inline">\(t = 1,...T_A\)</span>.
We don’t assume such agreement will occur for truly non-matching cartridge cases.
Rarely do <em>all</em> cells agree with the same registration in practice, even for truly matching cartridge cases.
Instead, depending on the quality of the impressions on two matching cartridge cases, there may be a handful of cells that have similar <span class="math inline">\((m^*_{A,t,\theta},n^*_{A,t,\theta}, \theta)\)</span> values.
A natural question is: why do some scans/cells find the correct registration while others do not?
In the next section, we introduce a set of diagnostic tools we developed to answer such questions.</p>
</div>
</div>
<div id="visual-diagnostics-1" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Visual Diagnostics<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostics-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-x3p-plot" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> The X3P Plot<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-x3p-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first visual diagnostic tool we discuss is the “X3P plot” which is used to visualize the values of a scan’s surface matrix.
We show an example of an X3P plot in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:preProcessEffectExample">3.6</a>, which will be discussed in more detail below.
The orientation of the X3P plot is the same as its underlying surface matrix, meaning the top left-most pixel represents the <span class="math inline">\([1,1]\)</span>-th element of the surface matrix followed the <span class="math inline">\([1,2]\)</span>-th element to its immediate right and so on.
To construct the X3P plot, we map 11 percentiles of the non-missing values in a surface matrix to the continuous, divergent purple-white-orange color scheme illustrated in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:x3pPlot-colorscheme">3.5</a>.
The darkest shades of purple and orange represent the minimum and maximum surface values, respectively.
We use a very light shade of gray to represent the median surface value to ensure symmetry in the percentile mapping.
Rather than mapping deciles to the 11 colors (minimum, 10th percentile, 20th percentile, etc.), we’ve found the more polarized mapping shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:x3pPlot-colorscheme">3.5</a> to be more effective at emphasizing extreme surface values, which are commonly associated with the most prominent impressions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:x3pPlot-colorscheme"></span>
<img src="images/chapter3-images/x3pPlot-colorscheme.png" alt="The percentiles (top) and hexidecimal color values (bottom) used in the color mapping of the X3P plot." width="\textwidth" />
<p class="caption">
Figure 3.5: The percentiles (top) and hexidecimal color values (bottom) used in the color mapping of the X3P plot.
</p>
</div>
<p>To visualize two or more scans using the X3P plot, we map percentiles of the pooled surface values to the same color scale.
This allows us to compare the relative sizes of impressions across multiple cartridge cases.
The registration procedure outlined in Figure <a href="#alg:registration"><strong>??</strong></a> is highly sensitive to extreme values on either surface, so performing a visual comparison of two scans using the X3P plot is useful for identifying whether further processing of either scan is needed.</p>
<p>Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:preProcessEffectExample">3.6</a> shows two examples of the registration output for a matching pair of scans labeled K002eG2 and K227iG3.
In the top plot we see that both scans contain large, extraneous markings that may affect the registration procedure.
There is a ring of raised observations around the center of K002eG2 that is an artifact of the deformation that occurs when the firing pin strikes the cartridge case primer.
Additionally, there are large dent-like markings on both K002eG2 and K227iG3.
Registering these two scans using the image registration algorithm results in <span class="math inline">\(CCF_{\max} = 0.14\)</span> at registration <span class="math inline">\((m^*,n^*,\theta^*) = (4,19,-6^\circ)\)</span>.</p>
<p>The bottom plot of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:preProcessEffectExample">3.6</a> shows the registrations of K002eG2 and K227iG3 after applying the additional pre-processing of removing the extraneous regions.
The output of the image registration algorithm is now <span class="math inline">\(CCF_{\max} = 0.29\)</span> at registration <span class="math inline">\((m^*,n^*,\theta^*) = (6,18,-6^\circ)\)</span>.
Although the registrations are similar for both pairs of scans, the cross-correlation value more than doubles when we remove the extraneous regions.
We also note that removal of the extreme values makes it easier to visually identify similar markings between K002eG2 and K227iG3, such as the “striped” impressions at the top of the two scans.
Highlighting such similarities is one of the strengths of the X3P plot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:preProcessEffectExample"></span>
<img src="images/chapter3-images/preProcessEffectExample.png" alt="Registration results from comparing two versions of a matching pair of cartridge case scans. In the first comparison (top), extraneous values are left in the scan which causes the overall $CCF_{\max}$ value to be relatively low (0.14). When these values are removed (bottom), the CCF value more than doubles to 0.29. The X3P plot is useful for identifying scans that are in need of additional pre-processing." width="\textwidth" />
<p class="caption">
Figure 3.6: Registration results from comparing two versions of a matching pair of cartridge case scans. In the first comparison (top), extraneous values are left in the scan which causes the overall <span class="math inline">\(CCF_{\max}\)</span> value to be relatively low (0.14). When these values are removed (bottom), the CCF value more than doubles to 0.29. The X3P plot is useful for identifying scans that are in need of additional pre-processing.
</p>
</div>
</div>
<div id="the-comparison-plot" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> The Comparison Plot<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-comparison-plot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The “comparison plot” is a visual diagnostic tool that uses the X3P plot to directly compare the surface values of two scans.
The comparison plot provides a quick, intuitive assessment by partitioning the surfaces into similarities and differences.
To construct the comparison plot we first obtain two aligned scans <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> using Figure <a href="#alg:registration"><strong>??</strong></a>.
A comparison plot like the one shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a> depicts aligned versions of the two scans <span class="math inline">\(A\)</span> = K013sA1 and <span class="math inline">\(B\)</span> = K013sA2 shown in Figure <a href="#fig:processedSideBySide"><strong>??</strong></a>.
The first columns shows the scan <span class="math inline">\(A\)</span> and aligned scan <span class="math inline">\(B^*\)</span> in the top left and right, respectively.
The dark gray (gray40) elements in these two visualizations represent the non-overlapping elements from the other scan.
Next, we wish to partition these <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> into similarities and differences using a filter operation.</p>
<p>For a matrix <span class="math inline">\(X \in \mathbb{R}^{k \times k}\)</span> and Boolean-valued condition matrix <span class="math inline">\(cond: \mathbb{R}^{k \times k} \to \{TRUE,FALSE\}^{k \times k}\)</span>, we define an element-wise filter operation <span class="math inline">\(\mathcal{F}: \mathbb{R}^{k \times k} \to \mathbb{R}^{k \times k}\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
\mathcal{F}_{cond}(X) =
(f_{ij})_{1 \leq i,j \leq k} =
\begin{cases}
x_{ij} &amp;\text{if $cond$ is $TRUE$ for element $i,j$} \\
NA &amp;\text{otherwise.}
\end{cases}
\end{align*}\]</span></p>
<p>The resulting <span class="math inline">\(\mathcal{F}_{cond}(X)\)</span> is a copy of the matrix <span class="math inline">\(X\)</span> where elements for which <span class="math inline">\(cond\)</span> is <span class="math inline">\(TRUE\)</span> are replaced with <span class="math inline">\(NA\)</span>.
The filtering operation allows us to isolate elements of a surface matrix that satisfy some criterion.
For example, we can isolate a surface matrix to only those elements that are close to the elements of another surface matrix.</p>
<p>Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:averageFilterExample">3.7</a> shows the construction of a filtered element-wise average between <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span>.
We compute the element-wise average <span class="math inline">\(\frac{1}{2}(A + B^*)\)</span> and absolute difference <span class="math inline">\(|A - B^*|\)</span> as shown in the left and right of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:averageFilterExample">3.7</a>.
We then consider values of <span class="math inline">\(|A - B^*|\)</span> that are greater than some threshold <span class="math inline">\(\tau &gt; 0\)</span>.
We construct Boolean-valued matrices <span class="math inline">\(|A - B^*| \leq \tau\)</span> and <span class="math inline">\(|A - B^*| &gt; \tau\)</span> based on whether the element-wise absolute difference is at most or greater than <span class="math inline">\(\tau\)</span>.
For example, the right side of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:averageFilterExample">3.7</a> shows the elements of matrix <span class="math inline">\(|A - B^*| \leq 1\)</span> with <span class="math inline">\(TRUE\)</span> elements represented as white pixels and <span class="math inline">\(FALSE\)</span> elements as black pixels.
We then filter <span class="math inline">\(\frac{1}{2}(A + B^*)\)</span> using <span class="math inline">\(|A - B^*| \leq \tau\)</span> as the <span class="math inline">\(cond\)</span> matrix, resulting in tge filtered element-wise average <span class="math inline">\(\mathcal{F}_{|A - B^*| \leq \tau}\left(\frac{1}{2}(A + B^*)\right)\)</span>, an example of which is shown at in the bottom of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:averageFilterExample">3.7</a> using <span class="math inline">\(\tau = 1\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:averageFilterExample"></span>
<img src="images/chapter3-images/filteringIllustration.png" alt="To construct the comparison plot after aligning the scans K013sA1 and K013sA2, we compute their element-wise average (left) and element-wise absolute difference (right). We then compute a Boolean-valued matrix based on whether the elements of the element-wise absolute difference is greater or less than 1 micron (right). We use this Boolean matrix to distinguish between similarities and differences in the scan impressions." width=".8\textwidth" />
<p class="caption">
Figure 3.7: To construct the comparison plot after aligning the scans K013sA1 and K013sA2, we compute their element-wise average (left) and element-wise absolute difference (right). We then compute a Boolean-valued matrix based on whether the elements of the element-wise absolute difference is greater or less than 1 micron (right). We use this Boolean matrix to distinguish between similarities and differences in the scan impressions.
</p>
</div>
<p>Complementary to the filtered element-wise average, the right column of the comparison plot shows differences between the two scans.
While it is visually obvious when scans share similar markings, characterizing different markings can be challenging.
For example, two markings may be different in their depth, shape, orientation, or spatial relationship to other markings.
As such, we visualize two filtered versions of the aligned scans <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(A)\)</span> and <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(B^*)\)</span> to emphasize differences.
Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:differenceFilterExample">3.8</a> shows an example of the results of this filtering using <span class="math inline">\(\tau = 1\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:differenceFilterExample"></span>
<img src="images/chapter3-images/filteringDifferencesIllustration.png" alt="To construct the comparison plot, we filter two scans $K013sA1$ and $K013sA2$ based on regions where their element-wise absolute difference exceeds 1 micron, which are represented as white pixels in the black and white image. This emphasizes regions where the two surfaces differ, which complements the similarities that are emphasized in the element-wise average." width=".8\textwidth" />
<p class="caption">
Figure 3.8: To construct the comparison plot, we filter two scans <span class="math inline">\(K013sA1\)</span> and <span class="math inline">\(K013sA2\)</span> based on regions where their element-wise absolute difference exceeds 1 micron, which are represented as white pixels in the black and white image. This emphasizes regions where the two surfaces differ, which complements the similarities that are emphasized in the element-wise average.
</p>
</div>
<p>The comparison plot visualizes the surface values of the original scans <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span>, the filtered element-wise average <span class="math inline">\(\mathcal{F}_{|A - B^*| \leq \tau}\left(\frac{1}{2}(A + B^*)\right)\)</span>, and filtered element-wise differences <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(A)\)</span> and <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(B^*)\)</span>.
Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a> shows an example of a comparison plot using <span class="math inline">\(A=\)</span> K013sA1 and <span class="math inline">\(B=\)</span> K013sA2.</p>
<p>The middle column of the comparison plot shows the filtered element-wise matrix <span class="math inline">\(\mathcal{F}_{|A - B^*| \leq \tau}\left(\frac{1}{2}(A + B^*)\right)\)</span>.
By isolating the element-wise average to “close” surface values between <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span>, the filtered element-wise average emphasizes similar markings between the two scans.
In Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a>, we use a filtering threshold of <span class="math inline">\(\tau = 1\)</span> microns and represent filtered elements in light gray (gray80).
This filtered element-wise average illustrates that there are many similarities between the two surfaces.
To identify similarities using the element-wise average, we have found it most effective to scan the filtered element-wise average plot for distinctive markings, such as the deep purple and orange markings in the 5 o’clock position of the firing pin hole in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a>.
After identifying distinctive markings, it is relatively easy to identify the contributing markings from <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> by considering the same region in the two plots in the left column.
For example, we see deep purple and orange striped impressions in the 5 o’clock positions of <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a>.
Cross-referencing the filtered element-wise average with the individual scans allows us to assess the degree of similarity and spatial relationship between markings on the two scans.</p>
<p>The right column of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a> shows the filtered differences between scans <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> using a cutoff threshold of <span class="math inline">\(\tau = 1\)</span> microns.
Again, we’ve found it useful to study the two filtered plots to identify differences that can be cross-referenced against the original scans.
For example, scan <span class="math inline">\(B^*\)</span> has a dent-like marking at the 11 o’clock position of the firing pin hole that is not present in <span class="math inline">\(A\)</span>.
On the other hand, we note a dark orange region in the 5 o’clock position, where we had previously noted similarities, that is treated as a difference.
Considering the original scans in the left column, we see that these orange regions are indeed part of the striped purple and orange impression region.
It can be safely assumed that these two regions should be treated as “similar” markings despite being at least 1 micron apart.
These two examples illustrate the fundamental challenge with characterizing differences - there are many ways in which two markings can be “different” from one another.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:complementCompPlot-fullScan"></span>
<img src="figures/complementCompPlot-fullScan.png" alt="The comparison plot provides an intuitive visualization of the similarities and differences between two aligned surface matrices. The left column of the comparison plot shows two aligned scans. The middle column shows the element-wise average between the two aligned scans after filtering out surface values that are at least 1 micron apart. The right column shows these filtered surface values of the aligned scans. Together, the middle and right column show the &quot;similarities&quot; and &quot;differences&quot; between the two aligned scans." width="\textwidth" />
<p class="caption">
Figure 3.9: The comparison plot provides an intuitive visualization of the similarities and differences between two aligned surface matrices. The left column of the comparison plot shows two aligned scans. The middle column shows the element-wise average between the two aligned scans after filtering out surface values that are at least 1 micron apart. The right column shows these filtered surface values of the aligned scans. Together, the middle and right column show the “similarities” and “differences” between the two aligned scans.
</p>
</div>
<p>By construction, the X3P and comparison plots emphasize the most extreme values in the two surface matrices.
This means that similar, yet less extreme markings are harder to visually identify using the full scan X3P and comparison plots.
To visually assess the similarity between these markings, we can “zoom in” to specific regions of the cartridge case surfaces using the cell-based comparison procedure outlined in the cell-based comparison procedure.
Recall that the cell-based comparison returns a set of estimated registrations, one for each source cell <span class="math inline">\(t = 1,...,T_A\)</span>.
Using these estimated registrations, we extract a matrix from the target scan that represents the patch in the target scan that maximized the CCF with the source cell.
That is, we extract the target “mate” for each source cell.
Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-cellBased">3.10</a> shows the comparison plot of cell 3, 8 from scan K013sA1 and its target mate in K013sA2.
The left column again shows the two aligned surface matrices, the middle column the filtered element-wise average using <span class="math inline">\(\tau = 1\)</span> microns, and the right column the filtered differences.
Compared to the depiction of this region in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a>, it is much easier to identify similar and different markings using this zoomed-in visualization.
For example, the dark purple “dots” on the upper-left side of the two scans are more prominent in this visualization.
We note that the color scale mapping now ranges from -5 to 4.1 microns compared to -12.3 to 20.3 microns in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a>.
Small, local markings are more prominent when we use the comparison plot on the output of the cell-based comparison procedure.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:complementCompPlot-cellBased"></span>
<img src="figures/complementCompPlot-cellBased.png" alt="The comparison plot for cell 3, 8 of scan K013sA1 and its aligned mate in scan K013sA2. The left column shows the surface values of these two cells. Note that non-overlapping pixels are shown in dark gray in the bottom left plot. The middle column shows similarities between the surfaces in the form of the filtered element-wise average. The right column shows the surface values with the opposite filtering used in the filtered element-wise average plot. We use a gray border to emphasize the filtered vs. non-filtered regions." width="\textwidth" />
<p class="caption">
Figure 3.10: The comparison plot for cell 3, 8 of scan K013sA1 and its aligned mate in scan K013sA2. The left column shows the surface values of these two cells. Note that non-overlapping pixels are shown in dark gray in the bottom left plot. The middle column shows similarities between the surfaces in the form of the filtered element-wise average. The right column shows the surface values with the opposite filtering used in the filtered element-wise average plot. We use a gray border to emphasize the filtered vs. non-filtered regions.
</p>
</div>
<p>The comparison plot is particularly useful for understanding the results of the cell-based registration procedure outlined in the cell-based comparison.
For example, the top of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:nonMatchCell-comparisonPlot">3.11</a> shows an aligned cell 6, 1 between two non-match scans K013sA1 and K002eG1.
It’s difficult to visually identify many similarities between the two scans, which is expected given that the cartridge cases originate from different firearms.
However, the bottom of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:nonMatchCell-comparisonPlot">3.11</a> depicting the comparison plot between the aligned cell 6, 1 pair demonstrates that there are actually local similarities.
From the element-wise average we see that there are similar purple and orange regions shared between these two cells.</p>
<p>This example underscores the need to analyze both similarities <em>and</em> differences when comparing two scans.
We’re bound to find similarities if this is all we look for, so we also need to describe the differences between the surfaces.
In the next section, we discuss a set of summary statistics computed from the Comparison Plot that quantify both the similarities and differences between two scans.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nonMatchCell-comparisonPlot"></span>
<img src="images/chapter3-images/nonMatchCell-comparisonPlot.png" alt="(Top) We consider a non-match pair of cartridge cases K013sA1 and K002eG1 that don't appear to share many similar markings at first glance. (Bottom) After registering cell 6, 1 from K013sA1 in K002eG1 using the cell-based comparison, we then consider the Comparison Plot between the aligned pair of cells. Using this zoomed-in view, we can clearly see that there are actually local similarities between the two cartridge cases despite being fired from different firearms. This demonstrates how the comparison plot is useful for understanding registration results from cell-based comparison." width="\textwidth" />
<p class="caption">
Figure 3.11: (Top) We consider a non-match pair of cartridge cases K013sA1 and K002eG1 that don’t appear to share many similar markings at first glance. (Bottom) After registering cell 6, 1 from K013sA1 in K002eG1 using the cell-based comparison, we then consider the Comparison Plot between the aligned pair of cells. Using this zoomed-in view, we can clearly see that there are actually local similarities between the two cartridge cases despite being fired from different firearms. This demonstrates how the comparison plot is useful for understanding registration results from cell-based comparison.
</p>
</div>
</div>
<div id="visual-diagnostic-statistics" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Visual Diagnostic Statistics<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the last section, we used the comparison plot to make a number of qualitative observations about the similarities and differences between the impressions of two pairs of cartridge cases.
These observations aligned with what our intuition says should be true for two matching/non-matching cartridge cases.
For example, we would expect there to be many more similarities than differences for a matching pair compared to a non-matching pair.
In this section, we translate these sorts of qualitative observations into a set of numerical features that can be used to determine whether two cartridge case scans were fired from the same firearm.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:similaritiesDifferencesRatio"></span>
<img src="images/chapter3-images/similaritiesDifferencesRatio.png" alt="We compute the ratio between the number of similar and different elements of two aligned scans, which are defined to be elements for which the two surfaces are at most or greater than 1 micron, respectively. These are represented above as white pixels in the two images on the left. We then count the number of white pixels in each image and compute their ratio, resulting in this example in a value of 2.84. We expect this ratio to be larger for matching comparisons, on average, than non-matching comparisons." width="\textwidth" />
<p class="caption">
Figure 3.12: We compute the ratio between the number of similar and different elements of two aligned scans, which are defined to be elements for which the two surfaces are at most or greater than 1 micron, respectively. These are represented above as white pixels in the two images on the left. We then count the number of white pixels in each image and compute their ratio, resulting in this example in a value of 2.84. We expect this ratio to be larger for matching comparisons, on average, than non-matching comparisons.
</p>
</div>
<p>The first statistic is the ratio between the number of similarities and differences for a pair of scans.
To compute this, we consider the number of <span class="math inline">\(TRUE\)</span> elements the <span class="math inline">\(cond\)</span> matrices <span class="math inline">\(|A - B^*| \leq \tau\)</span> and <span class="math inline">\(|A - B^*| &gt; \tau\)</span>.
Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:similaritiesDifferencesRatio">3.12</a> shows an example of this ratio computed for the matching pair K013sA1 and K013sA2, which results in a value of 2.84 meaning there are almost three times as many similarities as there are differences between the two scans.</p>
<p>Mathematically, the similarities vs. differences ratio is given by
<span class="math display">\[\begin{align*}
r_{d} = \frac{\pmb{1}^T I(|A - B^*| \leq \tau) \pmb{1}}{\pmb{1}^T I(|A - B^*| &gt; \tau) \pmb{1}}
\end{align*}\]</span>
where <span class="math inline">\(\pmb{1} \in \mathbb{R}^k\)</span> is a column vector of ones and <span class="math inline">\(I(\cdot)\)</span> is the element-wise, matrix-valued indicator function.
The inner products in the numerator and denominator act to count the number of TRUE elements in the two complementary <span class="math inline">\(cond\)</span> matrices.</p>
<p>We also compute similarities vs. differences ratios for the results of the cell-based comparison procedure outlined in cell-based comparison.
Specifically, we compute the ratio for a cell from the source matrix and its aligned mate in the target matrix, resulting in the ratio value <span class="math inline">\(r_{d,t}\)</span>.
We expect that the similarities vs. differences ratio will be larger for matching comparisons compared to non-matching comparisons.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:filterLabeling-figure"></span>
<img src="images/chapter3-images/filterLabeling.png" alt="(Left) After aligning two scans, we filter regions that are &quot;different&quot; from each other, meaning the absolute difference between surface values is larger than some threshold. We binarize the scan into different vs. similar regions - shown in white and black, respectively. (Middle) Using a connected components labeling algorithm, we identify connected &quot;neighborhoods&quot; of filtered elements, which are distinguished here by fill color. (Right) Considering the distribution of the region sizes, we see that the vast majority of the regions are relatively small, under 1000 square microns, although there are some outliers. We assume that the average region size will be relatively small for truly matching comparisons." width="\textwidth" />
<p class="caption">
Figure 3.13: (Left) After aligning two scans, we filter regions that are “different” from each other, meaning the absolute difference between surface values is larger than some threshold. We binarize the scan into different vs. similar regions - shown in white and black, respectively. (Middle) Using a connected components labeling algorithm, we identify connected “neighborhoods” of filtered elements, which are distinguished here by fill color. (Right) Considering the distribution of the region sizes, we see that the vast majority of the regions are relatively small, under 1000 square microns, although there are some outliers. We assume that the average region size will be relatively small for truly matching comparisons.
</p>
</div>
<p>We assume markings on the surfaces of two aligned, matching cartridge cases will line up with each other.
This implies that regions that we define as “different” should be relatively small in area.
We translate this into a numerical feature by considering the <span class="math inline">\(TRUE\)</span> elements of the <span class="math inline">\(cond\)</span> matrix <span class="math inline">\(|A - B^*| &gt; \tau\)</span>, which are elements where the surfaces differ by at least <span class="math inline">\(\tau\)</span>.
For example, the left side of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:filterLabeling">4.6</a> shows the matrix <span class="math inline">\(|A - B^*| &gt; 1\)</span> for <span class="math inline">\(A\)</span> = K013sA1 and <span class="math inline">\(B\)</span> = K013sA2 with <span class="math inline">\(TRUE\)</span> elements represented in white.
We then use a connected components labeling algorithm detailed in <span class="citation">Hesselink, Meijster, and Bron (<a href="#ref-hesselink_concurrent_2001" role="doc-biblioref">2001</a>)</span> and implemented in <span class="citation">Barthelme (<a href="#ref-imager" role="doc-biblioref">2019</a>)</span> to identify connected “neighborhoods” of <span class="math inline">\(TRUE\)</span> elements.
Specifically, the algorithm returns a set of sets <span class="math inline">\(\pmb{S}_d = \{S_{d,1},S_{d,2},...,S_{d,L_d}\}\)</span> where each <span class="math inline">\(S_{d,l}\)</span> is a set of indices of the <span class="math inline">\(cond\)</span> matrix that have a value of <span class="math inline">\(TRUE\)</span> and are connected by a chained-together sequence of 4 (Rook’s) neighborhoods.
The middle of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:filterLabeling">4.6</a> shows each <span class="math inline">\(S_{d,l}\)</span> distinguished by different fill colors, <span class="math inline">\(l = 1,...,L_d\)</span>.</p>
<p>The right side of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:filterLabeling">4.6</a> shows a histogram of the region sizes, denoted <span class="math inline">\(|S_{d,l}|\)</span> for <span class="math inline">\(l = 1,...,L_d\)</span>.
We see that most of the sizes are relatively small, which agrees with our initial assumption.
There are a handful of larger regions that, when cross-referenced with the surface values in the original scans (see Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:complementCompPlot-fullScan">3.9</a>, for example), are clearly different from each other, meaning the visual diagnostic works as intended.
We assume that the distribution of region sizes for a matching pair will have far fewer extreme values compared to a non-matching pair.
Again, we extend our notation to accommodate individual cells.
Let <span class="math inline">\(\pmb{S}_{d,t} = \{S_{d,t,1},...,S_{d,t,L_{d,t}}\}\)</span> denote the set of labeled neighborhoods for a cell <span class="math inline">\(t = 1,...,T_d\)</span>, <span class="math inline">\(d = A,B\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:matchingCellDifferenceCorrelation"></span>
<img src="images/chapter3-images/matchingCellDifferenceCorrelation.png" alt="The plots show regions of two aligned cells from a matching comparison. Specifically, we filter these cells to only elements for which the surfaces are at least 1 micron apart. We note here that even among these &quot;different&quot; regions, the trends in the surface values are similar, which may occur because of inconsistent contact with markings on the breech face of a firearm across repeated fires. The relatively high correlation between these two cells of 0.84 reflects this similarity." width="\textwidth" />
<p class="caption">
Figure 3.14: The plots show regions of two aligned cells from a matching comparison. Specifically, we filter these cells to only elements for which the surfaces are at least 1 micron apart. We note here that even among these “different” regions, the trends in the surface values are similar, which may occur because of inconsistent contact with markings on the breech face of a firearm across repeated fires. The relatively high correlation between these two cells of 0.84 reflects this similarity.
</p>
</div>
<p>The final statistic we compute is based on the observation that, even among regions that we define as “different,” the surface values of two matching cartridge cases should follow similar trends.
There may be variability in the depth of markings impressed by a firearm’s breech face across repeated fires, but the overall shape/trend of the markings should remain consistent.
For example, Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:matchingCellDifferenceCorrelation">3.14</a> shows regions of two cells from a comparison between matching scans <span class="math inline">\(A\)</span> = K013sA1 and <span class="math inline">\(B\)</span> = K013sA2.
We filter these two cells to elements where their surfaces are at least one micron apart.
The surface values of these “differences” vary in a similar manner despite being far from each other.
This observation is represented in the correlation value 0.84, which is relatively high for cartridge case comparisons.
We calculate the correlation by vectorizing the two filtered surface matrices and treating missing values by case-wise deletion.</p>
<p>To measure the similarity in the surface value trends, we calculate the correlation <span class="math inline">\(cor_{d,\text{full},\text{diff}}\)</span> between the filtered matrices <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(A)\)</span> and <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(B^*)\)</span> for <span class="math inline">\(d = A\)</span> and <span class="math inline">\(\mathcal{F}_{|A^* - B| &gt; \tau}(A^*)\)</span> and <span class="math inline">\(\mathcal{F}_{|A^* - B| &gt; \tau}(B)\)</span> for <span class="math inline">\(d = B\)</span>.
We assume that <span class="math inline">\(cor_{d,\text{full},\text{diff}}\)</span> will be large for matching cartridge case pairs relative to non-matching pairs.
Said another way, we assume that regions of matching cartridge cases that are different will still follow similar trends.
This can occur due to variability in the amount of contact between a cartridge case and breech face across multiple fires of a single firearm.</p>
<p>We extend our notation to accommodate cell comparisons <span class="math inline">\(t = 1,...,T_d\)</span> for <span class="math inline">\(d = A,B\)</span> using subscripts: <span class="math inline">\(cor_{d,t,\text{diff}}\)</span>.
For example, <span class="math inline">\(cor_{A,t,\text{diff}}\)</span> is the correlation between cell filtered surface matrices <span class="math inline">\(\mathcal{F}_{|A_t - B_{t,\theta_t^*}^*| &gt; \tau}(A_t)\)</span> and <span class="math inline">\(\mathcal{F}_{|A_t - B_{t,\theta_t^*}^*| &gt; \tau}(B_{t,\theta_t^*}^*)\)</span> where <span class="math inline">\(B_{t,\theta_t^*}^*\)</span> is the matrix extracted from <span class="math inline">\(B^*\)</span> that maximizes the CCF with <span class="math inline">\(A_t\)</span>.
Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:matchingCellDifferenceCorrelation">3.14</a> shows an example of computing the correlation between cell 2, 8 from scan K013sA1 and its mate in K013sA2.</p>
<p>These visual diagnostic statistics provide a quantitative complement to the qualitative observations we draw from the Comparison Plot.
They are useful by themselves to understand or justify why a source scan or cell aligned to a specific region in the target.
In the next section, we explore their use as numerical features to distinguish between matching and non-matching comparisons.</p>
</div>
</div>
<div id="statistical-learning-from-visual-diagnostics" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Statistical Learning from Visual Diagnostics<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#statistical-learning-from-visual-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we explore using the visual diagnostic statistics discussed above as features in a statistical classifier model to differentiate between matching and non-matching comparisons.
We consider a data set of 210 cartridge cases scanned at the Roy J. Carver High Resolution Microscopy Facility at Iowa State University that were collected as part of a study by <span class="citation">(<a href="#ref-Baldwin2014" role="doc-biblioref">Baldwin et al. 2014</a>)</span>.
The researchers fired the Remington 9mm centerfire cartridge cases from 10 Ruger SR9 pistols.
We scanned these cartridge cartridge cases using the Cadre<span class="math inline">\(^{\text{TM}}\)</span> 3D-TopMatch High Capacity Scanner.
See Chapter 4 for more information on these cartridge case data.</p>
<div id="visual-diagnostic-statistics-as-features" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Visual Diagnostic Statistics as Features<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics-as-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> denote cartridge case scans.
We first perform the registration procedure of image registration algorithm in both comparison directions using a rotation grid of <span class="math inline">\(\pmb{\Theta} = \{-30^{\circ} , -27^{\circ}, ..., 27^{\circ}, 30^{\circ}\}\)</span>, resulting in estimated full scan registrations <span class="math inline">\((m_{A}^*, n_{A}^*, \theta_A^*, CCF_{\max,A})\)</span> and <span class="math inline">\((m_{B}^*, n_{B}^*, \theta_B^*, CCF_{\max,B})\)</span>.
Using these registrations, we obtain the aligned versions of the target scans; <span class="math inline">\(B^*\)</span> for <span class="math inline">\(d = A\)</span> and <span class="math inline">\(A^*\)</span> for <span class="math inline">\(d = B\)</span>.</p>
<p>Next, we perform the cell-based comparison procedure of cell-based comparison using rotation grids <span class="math inline">\(\pmb{\Theta}_d&#39; = \{\theta_d^* - 2^{\circ}, \theta_d^* - 1^{\circ},\theta_d^*,\theta_d^* + 1^{\circ}, \theta_d^* + 2^{\circ}\}\)</span> for <span class="math inline">\(d = A,B\)</span> in both comparison directions, resulting in cell-wise registration sets <span class="math inline">\(\pmb{F}_A\)</span> and <span class="math inline">\(\pmb{F}_B\)</span>.
For each cell <span class="math inline">\(t = 1,...,T_d\)</span>, we compute its estimated registration as:</p>
<p><span class="math display">\[\begin{align*}
\theta_{d,t}^* &amp;= \arg \max_\theta \{CCF_{\max, d, t, \theta} : \theta \in \pmb{\Theta}_d&#39; \} \\
m_{d,t}^* &amp;= m_{d,t,\theta_{d,t}^*}^* \\
n_{d,t}^* &amp;= n_{d,t,\theta_{d,t}^*}^*.
\end{align*}\]</span></p>
<p>Using this estimated registration, we extract the cell’s mate from the target scan.
For <span class="math inline">\(d = A\)</span> and some cell <span class="math inline">\(t\)</span>, let <span class="math inline">\(B_{t,\theta_{d,t}^*}^*\)</span> denote its aligned mate in scan <span class="math inline">\(B^*\)</span> and assume the converse for <span class="math inline">\(d = B\)</span>.</p>
<p>At this point, we have the aligned mates for the source scans in both comparison directions at two both the full scan and cell scales.
Following the notion that many cartridge cases may only have a few areas with distinguishable markings, we expect the features at these scales to give us qualitatively different information.
Features at the cell scale may help illuminate regions of high similarity between two scans that the full scan features are unable to discern.
However, we’ve seen in the example of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:nonMatchCell-comparisonPlot">3.11</a> that even non-matching scans can share local similarities, in which case full scan features would prove more useful.</p>
<p>We consider the average <strong>full-scan similarities vs. differences ratio</strong> across the two comparison directions:</p>
<p><span class="math display">\[\begin{align*}
r_{\text{full}} = \frac{1}{2}(r_A + r_B).
\end{align*}\]</span></p>
<p>We expect <span class="math inline">\(r_{\text{full}}\)</span> to be large for matching pairs compared to non-matching pairs.
That is, truly matching pairs will have more similarities than differences.</p>
<p>We also calculate features based on the ratio for cell comparisons <span class="math inline">\(t = 1,...,T_d\)</span>, <span class="math inline">\(d = A,B\)</span>.
Let <span class="math inline">\(r_{d,t}\)</span> denote the ratio for cell comparison <span class="math inline">\(t\)</span> in direction <span class="math inline">\(d\)</span>.
We consider the <strong>average</strong> and <strong>standard deviation of the cell-based similarities vs. differences ratio</strong>:</p>
<p><span class="math display">\[\begin{align*}
\bar{r}_{\text{cell}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t = 1}^{T_d} r_{d,t} \\
s_{\text{cell}, r} &amp;= \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t = 1}^{T_d} (r_{d,t} - \bar{r}_{\text{cell}})^2}.
\end{align*}\]</span></p>
<p>We expect <span class="math inline">\(\bar{r}_{\text{cell}}\)</span> and <span class="math inline">\(s_{\text{cell}, r}\)</span> to be large for matching cartridge case pairs relative to non-match pairs.</p>
<p>We calculate the following features using the full-scan labeled neighborhoods:</p>
<p><span class="math display">\[\begin{align*}
\overline{|S|}_{\text{full}} &amp;= \frac{1}{L_A + L_B} \sum_{d \in \{A,B\}} \sum_{l=1}^{L_d} |S_{d,l}| \\
s_{\text{full},|S|} &amp;= \sqrt{\frac{1}{L_A + L_B - 1} \sum_{d \in \{A,B\}} \sum_{l=1}^{L_d} (|S_{d,l}| - \overline{|S|}_{\text{full}})^2}.
\end{align*}\]</span></p>
<p>We assume that the <strong>average</strong> and <strong>standard deviation of the full-scan neighborhood sizes</strong> will be small for matching cartridge case pairs relative to non-matching pairs.
That is, we assume that the the regions of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> that are different will all be small, on average, and vary little in size.
This assumption is appropriate assuming that the breech face leaves consistent markings on fired cartridge cases.</p>
<p>We calculate the per-cell average and standard deviation of the labeled neighborhood cell size:</p>
<p><span class="math display">\[\begin{align*}
\overline{|S|}_{d,t} &amp;= \frac{1}{L_{d,t}} \sum_{l=1}^L |S_{d,t,l}| \\
s_{d,t,|S|} &amp;= \sqrt{\frac{1}{L_{d,t} - 1} \sum_{l=1}^{L_{d,t}} (|S_{d,t,l}| - \overline{|S|}_{\text{cell},d,t})^2}.
\end{align*}\]</span></p>
<p>We assume that the cell-based <span class="math inline">\(\overline{|S|}_{d,t}\)</span> and <span class="math inline">\(s_{d,t,|S|}\)</span> will be small, on average, for truly matching cartridge cases.
Consequently, we use the sample average of these as features:</p>
<p><span class="math display">\[\begin{align*}
\overline{|S|}_{\text{cell}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} \overline{|S|}_{d,t} \\
\bar{s}_{\text{cell},|S|} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} s_{d,t,|S|}.
\end{align*}\]</span></p>
<p>We assume that the <strong>average cell-wise neighborhood size</strong> and the <strong>average standard deviation of the cell-wise neighborhood sizes</strong> will be small for matching cartridge case pairs relative to non-match pairs.</p>
<p>We use the average <strong>full-scan differences correlation</strong> as a feature:</p>
<p><span class="math display">\[\begin{align*}
cor_{\text{full},\text{diff}} = \frac{1}{2}\left(cor_{A,\text{full},\text{diff}} + cor_{B,\text{full},\text{diff}}\right).
\end{align*}\]</span></p>
<p>We calculate the <strong>average cell-based differences correlation</strong> across all cells and both directions:</p>
<p><span class="math display">\[\begin{align*}
\overline{cor}_{\text{cell},\text{diff}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} cor_{d,t,\text{diff}}.
\end{align*}\]</span></p>
<p>Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualFeaturePairs">3.15</a> shows a “generalized pairs plot” <span class="citation">(<a href="#ref-gpp" role="doc-biblioref">Emerson et al. 2012</a>; <a href="#ref-ggally" role="doc-biblioref">Schloerke et al. 2021</a>)</span> of the 9 visual diagnostic features distinguished by the ground-truth nature of the comparisons for the 21,945 training comparisons.
The ground truth is represented by the left column/top row of the plot.
The bar plot in top left corner shows there are 19,756 non-matching comparisons to 2,199 matching comparisons.
This imbalance is due to the fact that we consider every pairwise comparison between cartridge cases from 10 training firearms.</p>
<p>The other visuals in the first column show box plots of the 9 features, which complement the density plots along the main diagonal of each feature as well as the un-normalized histograms in the first row.
Although these three visuals depict the same data, they convey different information about the data.
For example, the density plots in the main diagonal show the estimated conditional distributions of the 9 features given ground-truth, which gives us intuition about the discriminative nature of the features.
The cell-based average different region correlation, denoted <span class="math inline">\(\overline{cor}_{\text{cell, diff}}\)</span> in the previous section, has greater separation between the matching and non-matching distributions compared to the cell based average different region size, <span class="math inline">\(\overline{|S|}_{\text{cell}}\)</span> in the last section.
The histograms in the first column convey similar information as the density plots, yet emphasize the class imbalance between the matching and non-matching comparisons.</p>
<p>On the other hand, the box plots in the first column provide a more succinct summary of rank statistics and outliers for each feature.
For example, we see that the full scan average different region size feature, <span class="math inline">\(\overline{|S|}_{\text{full}}\)</span>, has an extreme non-match outlier with a value over 700.
<strong>[More to say about this specific outlier? Look up example and talk about why it’s an outlier]</strong>
We discuss the univariate distribution of features in the “Case Studies” section below.</p>
<p>Barring the first row/column, the off-diagonal visuals show summaries of the pairwise relationships between the 9 features.
This provides us with intuition on which features “share” information.
The lower-triangle visuals show density plots for each pair of features.
We visualize the 50th, 80th, and 95th percentile highest-density regions for the matching and non-matching comparisons using concentric, progressively lighter regions.
This density visualization helps us understand where most of the matching and non-matching feature values lie without needing to visualize every pairwise comparison as a single point.
Instead, we visualize only those comparisons outside of the 95th highest-density region, which makes it easier to identify outliers.
The upper-triangle shows correlation summaries between each feature, also distinguished by ground-truth.
We see that the variables with the strongest relationship are the average and standard deviation of the cell-based similarities vs. differences ratio (7th column, 8th row), denoted <span class="math inline">\(\bar{r}_{\text{cell}}\)</span> and <span class="math inline">\(s_{\text{cell},r}\)</span> in the previous section, although the relationship isn’t surprising given the mathematical relationship between the two statistics.
We see a similar, albeit more linear, relationship between the average and standard deviation of the cell-based region sizes (9th column, 10th row).</p>
<p>There are also notable relationships between full scan and cell-based features.
For example, the relationship between full scan and cell-based different region correlations (2nd column, 6th row), denoted <span class="math inline">\(cor_{\text{full, diff}}\)</span> and <span class="math inline">\(cor_{\text{cell, diff}}\)</span>, is particularly strong for the matching compared to non-matching comparisons as evidenced by the starkly different correlations of 0.775 and 0.358, respectively.
A pair of non-matching scans may have differences that vary in a similar manner at one scale, but not another.
This could be an artifact of how matching and non-matching comparisons behave during the the full scan and cell-based registration procedures.
For example, the full scan vs. cell-based estimated registrations from a matching comparison are more likely to agree, meaning the same markings are overlaid on top of one another at both scales.
On the other hand, we wouldn’t expect the full scan and cell-based registrations to agree for a non-matching comparison, so different markings may be compared at the full scan and cell-based scales.
For some feature pairs, such as the full scan different region correlation and the standard deviation of the full scan different region size (2nd column, 4th row), the differing behavior of the matching and non-matching joint distributions suggests a higher-order interaction between these features.
This can be incorporated explicitly into a statistical classifier model like a logistic regression or can be “captured” in models like a decision tree or random forest <span class="citation">(<a href="#ref-hastie_elements_2008" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:visualFeaturePairs"></span>
<img src="images/chapter3-images/featureDensity-visualDiag-pairs.png" alt="Pairs plot for visual diagnostic feature values computed for 2,189 matching and 19,756 non-matching pairwise comparisons." width="\textwidth" />
<p class="caption">
Figure 3.15: Pairs plot for visual diagnostic feature values computed for 2,189 matching and 19,756 non-matching pairwise comparisons.
</p>
</div>
<p>Overall, there are only a handful of feature pairs that have strong linear relationships, and many of these exhibit different behavior between the matching and non-matching comparisons.
This indicates that the discriminatory power of one feature isn’t fully accounted for by that of another feature and that each feature can more or less stand on its own merits to be included in a statistical model.
In the next section, we explore results from fitting and testing binary classifiers using the visual diagnostic features.</p>
</div>
<div id="binary-classification-results" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Binary Classification Results<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#binary-classification-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the 21,945 training comparisons, we train three binary classifier models: based on a decision tree <span class="citation">(<a href="#ref-Breiman2017" role="doc-biblioref">Breiman et al. 2017</a>; <a href="#ref-rpart" role="doc-biblioref">Therneau and Atkinson 2022</a>)</span>, random forest <span class="citation">(<a href="#ref-breiman" role="doc-biblioref">Breiman 2001</a>; <a href="#ref-randomForest" role="doc-biblioref">Liaw and Wiener 2002</a>)</span>, and logistic regression <span class="citation">(<a href="#ref-stats" role="doc-biblioref">R Core Team 2023</a>)</span>.
Note that do not intend to present a “final” model recommendation that should be used in forensic casework here - our present goal is merely to explore the discriminatory power of the nine visual diagnostic features when combined.
See Chapter 4 for a broader exploration of cartridge case similarity scoring algorithms.</p>
<p>Using the <code>caret</code> R package <span class="citation">(<a href="#ref-caret" role="doc-biblioref">Kuhn 2022</a>)</span>, we perform 10-fold cross-validation, repeated three times, to train each model.
Ultimately, we choose the model that maximizes the area under the receiver operating characteristic (ROC) curve, abbreviated “AUC.”
We consider fitting each of the three classifier models to three subsets of the nine visual diagnostic features introduced in the last section: only the 4 full scan features, only the 5 cell-based features, and all 9 visual diagnostic features.
This helps us understand the relative discriminatory power of the full scan an cell-based features, as well as their utility when combined.
Finally, we also explore the use of up-sampling the matching comparisons and down-sampling the non-matching comparisons as a means of addressing the class imbalance in the training and testing data.
Note that this sub-sampling is performed within the re-sampling of the 10-fold cross-validation.
For comparison, we also fit each model without any sub-sampling.
In total, we train 27 models (3 classifiers <span class="math inline">\(\times\)</span> 3 feature groups <span class="math inline">\(\times\)</span> 3 sampling techniques) using the 10-fold, thrice-repeated cross-validation.</p>
<p>Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualDiag-rocPlt">3.16</a> shows ROC curve and AUC results for the 27 classifiers.
Comparing first across models (columns), we see that the random forest and logistic regression models have notably higher AUC values compared to the decision tree (CART) models.
In fact, the random forest model achieves perfect training classification, as evidenced by the AUC values equal to 1, when either no sampling is performed or the matching comparisons are up-sampled.
It makes sense that the random forest performs better than the CART model, since a random forest consists of an ensemble of CART models.
It is surprising that the logistic regression classifier performs comparable to the random forest due to its relative simplicity.
Considering sub-sampling procedures (rows), we see that the behavior of the AUC differs across the three models.
For the CART model, performing either sub-sampling techniques resulted in higher AUC values compared to no sampling.
The random forest and logistic regression models are comparatively much more consistent.</p>
<p>Finally, across feature groups (color) we see for the logistic regression and random forest models that the AUC is largest when all 9 visual diagnostic features are used.
For the logistic regression model, training based on the 4 full scan features results in the lowest AUC values, followed by the 5 cell-based features, and finally all 9 features.
Considering the feature distributions along the main diagonal of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualFeaturePairs">3.15</a>, we note that features including the cell-based different region correlation and average similarities vs. differences ratio appear to yield greater separation between matching and non-matching comparisons than the full scan versions of these features.
For generalized linear models like the logistic regression classifier, it makes sense that the greater separation for the cell-based features would to better classification results over the full scan features.
However, the fact that the random forest achieves an AUC of 1 based on both the full scan and cell-based features indicates that there is a non-linear decision boundary in both feature spaces that perfectly separates matches from non-matches.
It isn’t a surprise that training the random forest on the combined features also results in an AUC of 1.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:visualDiag-rocPlt"></span>
<img src="images/chapter3-images/visualDiag-rocPlt.png" alt="ROC curves for three binary classifier models (columns) trained using three sampling schemes (rows) on three subsets of the 9 visual diagnostic features (color). Overall, the random forest models have the highest AUC, specifically achieving perfect training classification when either no sampling is performed or the matching comparisons are up-sampled. The logistic regression models perform second best and is relatively invariant to sub-sampling scheme. The decision tree (CART) models have considerably lower AUC values overall and are sensitive to sub-sampling scheme. For the random forest and logistic regression models, using all 9 visual diagnostic features leads to higher AUCs compared to the smaller subsets, except for when the AUCs are all 1." width="\textwidth" />
<p class="caption">
Figure 3.16: ROC curves for three binary classifier models (columns) trained using three sampling schemes (rows) on three subsets of the 9 visual diagnostic features (color). Overall, the random forest models have the highest AUC, specifically achieving perfect training classification when either no sampling is performed or the matching comparisons are up-sampled. The logistic regression models perform second best and is relatively invariant to sub-sampling scheme. The decision tree (CART) models have considerably lower AUC values overall and are sensitive to sub-sampling scheme. For the random forest and logistic regression models, using all 9 visual diagnostic features leads to higher AUCs compared to the smaller subsets, except for when the AUCs are all 1.
</p>
</div>
<p>Consider the distributions of training match probabilities shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualDiagProb-density">3.17</a>, distinguished by firearm ID pair.
These match probabilities are computed using the random forest model trained on all 9 visual diagnostic features without any sub-sampling.
We distinguish the 10 training firearm IDs by rows/columns, meaning the main diagonal plots show match probabilities for the truly matching comparisons while the off-diagonal plots show the same for non-matching comparisons.
The horizontal axis for all plots represents the estimated match probability for all pairwise comparisons between scans from “Firearm 1” (columns) and scans from “Firearm 2” (rows).
Note that we transform the horizontal axis according the density of a Beta(4,4) distribution under the canonical shape Beta distribution parameterization.
Considering that we draw horizontal axis breaks (gray vertical lines) at <span class="math inline">\(\{0, 0.25, .5, 0.75, 1\}\)</span>, this transformation causes a “stretching” of values near 0 and 1 and “contracting” of values near 0.5.
We perform this transformation to provide a clearer visual of the match probabilities, which tend to “bunch up” near the extremes of the interval.
The vertical axis represents the density of the match probabilities for the density plots in the main diagonal and upper-triangle.
Because the pairwise match probability is invariant to which firearm is labeled “1” or “2,” the box plots in the lower-triangle depict the same data in the upper-triangle.
We combine the box plot and densities for the matching comparisons along the main diagonal.</p>
<p>Considering the matching to non-matching distributions, we see that the match probabilities tend to be more variable than the non-match probabilities.
For example, matching Firearm Z comparisons have associated match probabilities ranging from 0.6 to 1.0 with a median probability value of 0.8.
Comparatively, the non-match comparisons are more strongly right-skewed - the classifier is more “sure” when a pair doesn’t match.
About 15% (338 of 2,199) of all matching comparisons and 49% (9693 of 19,756) of all non-matching comparisons have an assigned match probability of 1.0 and 0.0, respectively.
In the case of the random forest classifier, this simply means that none of the constituent, ensembled decision tree classifiers “voted” for the incorrect class for these comparisons, but this further underscores the match vs. non-match imbalance.
We considered the feature distributions of these specific comparisons and noted that they exhibited excellent separation in a few, key features - namely, the full scan and cell-based difference correlations - that the random forest also considered highly “important” as measured by the mean Gini Index decrease <span class="citation">(<a href="#ref-randomForest" role="doc-biblioref">Liaw and Wiener 2002</a>)</span>.
In contrast, matching comparisons from Firearm Z exhibit poorer separation in these important features compared to non-match comparisons, which explains the lower match probabilities in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualDiagProb-density">3.17</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:visualDiagProb-density"></span>
<img src="images/chapter3-images/visualDiagProb-density.png" alt="Distribution of match probabilities estimated using a random forest classifier. We distinguish these probabilities by firearm ID pair, meaning each plot represents the pairwise comparisons in which one cartridge case originated from the &quot;column&quot; firearm and the other from the &quot;row&quot; firearm. Matching comparisons are represented along the main diagonal plots while non-match comparisons are shown in the off-diagonal." width="\textwidth" />
<p class="caption">
Figure 3.17: Distribution of match probabilities estimated using a random forest classifier. We distinguish these probabilities by firearm ID pair, meaning each plot represents the pairwise comparisons in which one cartridge case originated from the “column” firearm and the other from the “row” firearm. Matching comparisons are represented along the main diagonal plots while non-match comparisons are shown in the off-diagonal.
</p>
</div>
</div>
</div>
<div id="discussion-1" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Discussion<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#discussion-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="case-studies" class="section level3 hasAnchor" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Case Studies<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#case-studies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we explore specific examples of cartridge case comparisons to understand the relationship between qualitative observations we can make using the visual diagnostic tools and the quantitative measures of similarity we obtain from a binary classifier.
The phenomenon of classifier models not being as adept at identifying matching comparisons has been observed in many other cartridge evidence scoring methods <span class="citation">(<a href="#ref-song_proposed_2013" role="doc-biblioref">Song 2013</a>; <a href="#ref-chen_convergence_2017" role="doc-biblioref">Chen et al. 2017</a>)</span> and Chapter 4.
There are a variety of factors that may lead to this discrepancy, but one of the most important factors that we’ve identified is whether extraneous, non-breech face markings are correctly identified and removed during pre-processing.
We have consistently noted that extreme values in a scan tend to heavily impact the registration procedure.
For example, when applying the cell-based comparison procedure of cell-based comparison, large markings in the target scan seem to “attract” source cells, even if those cells do not contain similar markings when visually compared.
Diagnostic tools like the X3P and Comparison plots are useful for understanding why a cell registers in these areas.</p>
<p>We return to the pair of matching cartridge cases shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:preProcessEffectExample">3.6</a>.
Recall that we computed the cross-correlation function (CCF) between these scans before and after extraneous, non-breech face observations were removed from the scans.
Removing the non-breech face impressions made it easier to visually identify similar impressions between the two scans and increased the CCF value, implying higher similarity.
We now consider the similarity score for between these two scans estimated using the random forest binary classifier.
The left side of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:firearmG-preprocessAlignedCell">3.18</a> shows the cell-wise registrations for this comparison using K002eG2 as reference and K227iG3 as target.
Similar to Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:preProcessEffectExample">3.6</a>, the top and bottom show results from two cases: before and after removing the non-breech face observations from the two scans.
Estimating a similarity between these two scans using the “no sampling,” “all features” random forest classifier results in similarity scores of 0.66 and 1.00, respectively, for these two cases.</p>
<p>The source cells align in a more grid-like pattern in “K227iG3 + Additional Pre-processing” than in “K227iG3 Original”, where the extreme values along the inner rim seem to “pull” cells towards the center.
Cells like 1, 1 or 1, 4 do not register in a grid-like pattern in either case, although the registration in the “Additional Pre-processing” case is easier to justify as being due to the removal of observations in that region of K227iG3.
We would rather a registration fail due to a lack of shared information between the two scans than due to spurious similarities between extreme values.
As a specific example, we we depict the comparison plot for cell 3, 4 with its aligned mate in the target scan on the right side of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:firearmG-preprocessAlignedCell">3.18</a>.
In the “original” case, we see that cell aligns to the non-breech face values along the inner rim of K227iG3.
The filtered element-wise average between these cells actually does uncover some similarities, such as the faint orange values on the upper-right side of the two scans, yet these are insignificant compared to the large dissimilar regions in the center of the cells.
In contrast, the similarities are much more obvious in the “Additional Pre-processing” case based on the deeper purple/orange shades in the filtered element-wise average plot.
Further, careful study of the filtered differences between these cells shows that there are similar trends between the different regions of these scans, which upholds the use of the “differences correlation” feature.</p>
<p>This example demonstrates how the visual diagnostic tools both corroborate and inform the results from a trained classifier model.
Both the visual diagnostics and algorithm-based similarity score indicate that the original versions of the scans are not particularly similar.
However, only the visual diagnostics indicate that the dissimilarity is due to the presence of observations that “distract” the algorithm from comparing the actual breech face markings.
Upon removing these observations, both the visual diagnostics and similarity score reflect the similarity between the breech face impressions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:firearmG-preprocessAlignedCell"></span>
<img src="images/chapter3-images/firearmG-preprocessAlignedCell.png" alt="Aligned cells from the matching comparison shown in \@ref(fig:preProcessEffectExample). In the top row, non-breech face values are left in the scan, which leads to poor alignment of cells and an overall low similarity score (0.66). In the bottom row, we remove the non-breech values, which leads to improved alignment and a higher similarity score (1.00). In both cases, we show the comparison plot for cell 3, 4 and note that similar markings are more easily identified once the extraneous observations are removed. This illustrates how removing &quot;distracting&quot; values from the cartridge case scans during pre-processing can improve downstream similarity results." width="\textwidth" />
<p class="caption">
Figure 3.18: Aligned cells from the matching comparison shown in <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:preProcessEffectExample">3.6</a>. In the top row, non-breech face values are left in the scan, which leads to poor alignment of cells and an overall low similarity score (0.66). In the bottom row, we remove the non-breech values, which leads to improved alignment and a higher similarity score (1.00). In both cases, we show the comparison plot for cell 3, 4 and note that similar markings are more easily identified once the extraneous observations are removed. This illustrates how removing “distracting” values from the cartridge case scans during pre-processing can improve downstream similarity results.
</p>
</div>
<p>Next, we consider cartridge cases pairs that exhibited behavior in their similarity scores.
Specifically, we consider the matching and non-matching comparisons with the smallest and largest associated similarity score, respectively, as computed by the “no sampling,” “all features” random forest model.
These examples help us understand conditions under which the algorithm doesn’t behave as desired.
The first row of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:extremeProbExampleScans">3.19</a> shows matching K011sR1 and K046uR2 that have an estimated similarity score of 0.59.
The second row of Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:extremeProbExampleScans">3.19</a> shows non-matching K013pC1 and K027gA3 that have an estimated similarity score of 0.38.
Compared to the matching comparison between K002eG2 and K227iG3, it is harder to identify similar, distinctive impressions for these two pairs.
For example, the surfaces of both K011sR1 and K046uR2 appear more mottled with small-scale markings than imprinted with large striations like those visible in K002eG2 and K227iG3.
There are some striated markings visible in the north-east corner of scan K027gA3, but the same region in K013pC1 has only a thin strip of observations.
As such, there isn’t enough shared information in this region to say confidently that the impressions are similar or different.
The remaining surface of these two scans are similar to K011sR1 and K046uR2 in that there aren’t particularly distinctive markings.
A case could be made to apply additional pre-processing to remove the arc of orange and purple values along the south to south-west outer edge in K046uR2, but we don’t expect the results to change drastically.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:extremeProbExampleScans"></span>
<img src="figures/extremeProbExampleScans.png" alt="In the first row, we show the pair of matching scans with a low similarity score of 0.59. In the second row, we show a pair of non-match scans with a relatively high similarity score of 0.38. In both cases, the associated similarity scores seem attributable not to definite similarities or dissimilarities, but instead to a lack of distinctive markings." width="\textwidth" />
<p class="caption">
Figure 3.19: In the first row, we show the pair of matching scans with a low similarity score of 0.59. In the second row, we show a pair of non-match scans with a relatively high similarity score of 0.38. In both cases, the associated similarity scores seem attributable not to definite similarities or dissimilarities, but instead to a lack of distinctive markings.
</p>
</div>
<p>As further support for the middling similarity scores for these two pairs, consider Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualDiag-extremeProb-featureDensities">3.20</a> that shows numerical feature and score values for the 21,945 comparisons considered in the last section.
On the left, we visualize the densities for the 9 visual diagnostic feature values - these are the same as the densities shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualFeaturePairs">3.15</a>.
On the right, we visualize the similarity score densities - this is a combination of the densities shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualDiagProb-density">3.17</a>.
On top of each density plot, we visualize the value associated with the two pairs considered in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:extremeProbExampleScans">3.19</a> as an orange (match) and black (non-match) line.
This visual allows us to compare the feature values of these specific match and non-match pairs to each other and to the rest of the 21,943 pairwise comparisons.</p>
<p>We see that many feature values associated with these pairs fall between or around the modes of the matching and non-matching densities, which suggests that none of the features clearly exhibit the behavior of a matching or non-matching comparison.
The two pairs are “unexceptional” based on these features, which explains why the are assigned similarity scores close to the middle of the interval.
Interestingly, if we compare the two lines to each other across the various feature densities, we see that the non-match comparison between K013pC1 and K027gA3 has feature values more similar to a match comparison than the actually matching comparison between K011sR1 and K046uR2.
For example, we expect the cell-based average different region correlation (top-left) to be large if two cartridge cases match.
In this instance, however, the non-match comparison has a larger associated correlation value, 0.26, than that of the match comparison, 0.22.
Only for the cell-based average difference region size (top-center) and the standard deviation of the full scan difference region sizes (bottom-center) does the match comparison “look” more like a match comparison.
Despite this, the similarity score associated with the match comparisons is still larger than that of the non-match comparison.
This suggests the existence of higher-order interactions between these 9 features that aren’t obvious from a one-dimensional density plot, but that can be “learned” by the random forest classifier model.
For example, the lower-diagonal of the pairs plot in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:visualFeaturePairs">3.15</a> shows the pairwise relationship of some features differs for matching vs. non-matching comparisons.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:visualDiag-extremeProb-featureDensities"></span>
<img src="images/chapter3-images/visualDiag-extremeProb-featureDensities.png" alt="Density plots of 9 visual diagnostic features (left) and estimated similarity score (right) for 21,945 pairwise comparisons, distinguished by match and non-match comparisons. On top of each plot we visualize the values associated with the two cartridge case pairs shown in Figure \@ref(fig:extremeProbExampleScans). We see that the feature values for these two pairs fall close to the intersection of the match and non-match densities, which suggests the cartridge cases are fairly unexceptional. This explains the middling similarity scores observed in the right plot." width="\textwidth" />
<p class="caption">
Figure 3.20: Density plots of 9 visual diagnostic features (left) and estimated similarity score (right) for 21,945 pairwise comparisons, distinguished by match and non-match comparisons. On top of each plot we visualize the values associated with the two cartridge case pairs shown in Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:extremeProbExampleScans">3.19</a>. We see that the feature values for these two pairs fall close to the intersection of the match and non-match densities, which suggests the cartridge cases are fairly unexceptional. This explains the middling similarity scores observed in the right plot.
</p>
</div>
<p>In this section, we discuss how sensitive the final similarity scores are to the filter threshold <span class="math inline">\(\tau\)</span> used to partition two cartridge cases into “similarities” and “differences.”</p>
</div>
<div id="sensitivity-to-filter-threshold" class="section level3 hasAnchor" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Sensitivity to Filter Threshold<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#sensitivity-to-filter-threshold" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>[Final results don’t seem heavily impacted using different multiples of the absolute difference standard deviation. Interestingly, the importance measure of the 9 features rearrange when we use the standard deviation of the “pooled” surface values.]</strong></p>
</div>
<div id="interactive-cartridgeinvestigatr-application" class="section level3 hasAnchor" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Interactive cartridgeInvestigatR application<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#interactive-cartridgeinvestigatr-application" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We developed an interactive web application called cartridgeInvestigatR to give non-programmers access to the visual diagnostic tools.
The application is accessible at <a href="https://csafe.shinyapps.io/cartridgeInvestigatR/" class="uri">https://csafe.shinyapps.io/cartridgeInvestigatR/</a>.
In this section, we describe basic functionality of the application.</p>
</div>
</div>
<div id="conclusion-1" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Conclusion<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#conclusion-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>[Algorithms rarely have built-in mechanisms to determine when they work as expected. Diagnostic tools fill this gap. Visual diagnostics specifically provide intuitive ways to interpret the behavior of the algorithm. The visual diagnostics we developed can be used both as a quick reference to determine whether changes to earlier stages of the pipeline are warranted and as a tool to carefully study the behavior of the algorithm.]</strong></p>
<p><strong>[We note that the workflow of dealing with such cartridge cases in practice would be iteratively applying pre-processing steps followed by using the visual diagnostic tools to ensure that the pre-processing removes as much extraneous information from the scans as possible.]</strong></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Baldwin2014" class="csl-entry">
Baldwin, David P., Stanley J. Bajic, Max Morris, and Daniel Zamzow. 2014. <span>“A Study of False-Positive and False-Negative Error Rates in Cartridge Case Comparisons.”</span> Defense Technical Information Center. <a href="https://doi.org/10.21236/ada611807">https://doi.org/10.21236/ada611807</a>.
</div>
<div id="ref-imager" class="csl-entry">
Barthelme, Simon. 2019. <em>Imager: Image Processing Library Based on ’CImg’</em>. <a href="https://CRAN.R-project.org/package=imager">https://CRAN.R-project.org/package=imager</a>.
</div>
<div id="ref-breiman" class="csl-entry">
Breiman, Leo. 2001. <span>“<span>Random Forests</span>.”</span> <em>Machine Learning</em> 45 (1): 5–32. <a href="https://doi.org/10.1023/a:1010933404324">https://doi.org/10.1023/a:1010933404324</a>.
</div>
<div id="ref-Breiman2017" class="csl-entry">
Breiman, Leo, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. 2017. <em>Classification and Regression Trees</em>. Routledge. <a href="https://doi.org/10.1201/9781315139470">https://doi.org/10.1201/9781315139470</a>.
</div>
<div id="ref-Brown1992" class="csl-entry">
Brown, Lisa Gottesfeld. 1992. <span>“A Survey of Image Registration Techniques.”</span> <em><span>ACM</span> Computing Surveys</em> 24 (4): 325–76. <a href="https://doi.org/10.1145/146370.146374">https://doi.org/10.1145/146370.146374</a>.
</div>
<div id="ref-chen_convergence_2017" class="csl-entry">
Chen, Zhe, John Song, Wei Chu, Johannes A. Soons, and Xuezeng Zhao. 2017. <span>“A Convergence Algorithm for Correlation of Breech Face Images Based on the Congruent Matching Cells (<span>CMC</span>) Method.”</span> <em>Forensic Science International</em> 280 (November): 213–23. <a href="https://doi.org/10.1016/j.forsciint.2017.08.033">https://doi.org/10.1016/j.forsciint.2017.08.033</a>.
</div>
<div id="ref-gpp" class="csl-entry">
Emerson, John W, Walton A Green, Barret Schloerke, Jason Crowley, Dianne Cook, Heike Hofmann, and Hadley Wickham. 2012. <span>“The Generalized Pairs Plot.”</span> <em>Journal of Computational and Graphical Statistics</em> 22 (1): 79–91. <a href="http://www.tandfonline.com/doi/ref/10.1080/10618600.2012.694762">http://www.tandfonline.com/doi/ref/10.1080/10618600.2012.694762</a>.
</div>
<div id="ref-ISO25178-72" class="csl-entry">
<span>“<span class="nocase">Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p</span>.”</span> 2017. Standard. Vol. 2014. Geneva, CH: International Organization for Standardization. <a href="https://www.iso.org/standard/62310.html">https://www.iso.org/standard/62310.html</a>.
</div>
<div id="ref-hastie_elements_2008" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. New York, NY, USA: Springer New York Inc.
</div>
<div id="ref-hesselink_concurrent_2001" class="csl-entry">
Hesselink, Wim H., Arnold Meijster, and Coenraad Bron. 2001. <span>“Concurrent Determination of Connected Components.”</span> <em>Science of Computer Programming</em> 41 (2): 173–94. <a href="https://doi.org/10.1016/S0167-6423(01)00007-7">https://doi.org/10.1016/S0167-6423(01)00007-7</a>.
</div>
<div id="ref-caret" class="csl-entry">
Kuhn, Max. 2022. <em>Caret: Classification and Regression Training</em>. <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.
</div>
<div id="ref-randomForest" class="csl-entry">
Liaw, Andy, and Matthew Wiener. 2002. <span>“Classification and Regression by randomForest.”</span> <em>R News</em> 2 (3): 18–22. <a href="https://CRAN.R-project.org/doc/Rnews/">https://CRAN.R-project.org/doc/Rnews/</a>.
</div>
<div id="ref-council_strengthening_2009" class="csl-entry">
National Research Council. 2009. <em>Strengthening <span>Forensic</span> <span>Science</span> in the <span>United</span> <span>States</span>: <span>A</span> <span>Path</span> <span>Forward</span></em>. Washington, DC: The National Academies Press. <a href="https://doi.org/10.17226/12589">https://doi.org/10.17226/12589</a>.
</div>
<div id="ref-pcast2016" class="csl-entry">
President’s Council of Advisors on Sci. &amp; Tech. 2016. <span>“Forensic Science in Criminal Courts: Ensuring Scientific Validity of Feature-Comparison Methods.”</span> <a href="https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf">https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf</a>.
</div>
<div id="ref-stats" class="csl-entry">
———. 2023. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-ggally" class="csl-entry">
Schloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2021. <em>GGally: Extension to ’Ggplot2’</em>. <a href="https://CRAN.R-project.org/package=GGally">https://CRAN.R-project.org/package=GGally</a>.
</div>
<div id="ref-song_proposed_2013" class="csl-entry">
Song, John. 2013. <span>“Proposed <span>‘<span>NIST</span> <span>Ballistics</span> <span>Identification</span> <span>System</span> (<span>NBIS</span>)’</span> <span>Based</span> on <span>3d</span> <span>Topography</span> <span>Measurements</span> on <span>Correlation</span> <span>Cells</span>.”</span> <em>American Firearm and Tool Mark Examiners Journal</em> 45 (2): 11. <a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868">https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868</a>.
</div>
<div id="ref-rpart" class="csl-entry">
Therneau, Terry, and Beth Atkinson. 2022. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="automatic-matching-of-cartridge-case-impressions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
