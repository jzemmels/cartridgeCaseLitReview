<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Automatic Matching of Cartridge Case Impressions | A Cartridge Case Comparison Pipeline</title>
  <meta name="description" content="4 Automatic Matching of Cartridge Case Impressions | A Cartridge Case Comparison Pipeline" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Automatic Matching of Cartridge Case Impressions | A Cartridge Case Comparison Pipeline" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="jzemmels/cartridgeCaseLitReview" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Automatic Matching of Cartridge Case Impressions | A Cartridge Case Comparison Pipeline" />
  
  
  

<meta name="author" content="Joseph Zemmels" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"/>
<link rel="next" href="computational-details-1.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Literature Review</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preliminaries-forensic-examinations"><i class="fa fa-check"></i><b>1.1</b> Preliminaries: Forensic Examinations</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#firearm-and-toolmark-identification"><i class="fa fa-check"></i><b>1.1.1</b> Firearm and Toolmark Identification</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#why-should-firearm-and-toolmark-identification-change"><i class="fa fa-check"></i><b>1.1.2</b> Why Should Firearm and Toolmark Identification Change?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#forensic-comparison-pipelines"><i class="fa fa-check"></i><b>1.2</b> Forensic Comparison Pipelines</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#digital-representations-of-evidence"><i class="fa fa-check"></i><b>1.2.1</b> Digital Representations of Evidence</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#pre-processing-procedures-for-forensic-data"><i class="fa fa-check"></i><b>1.2.2</b> Pre-processing Procedures for Forensic Data</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#forensic-data-feature-extraction"><i class="fa fa-check"></i><b>1.2.3</b> Forensic Data Feature Extraction</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#similarity-scores-classification-rules-for-forensic-data"><i class="fa fa-check"></i><b>1.2.4</b> Similarity Scores &amp; Classification Rules for Forensic Data</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#reproducibility-of-comparison-pipelines"><i class="fa fa-check"></i><b>1.2.5</b> Reproducibility of Comparison Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#diagnostic-tools"><i class="fa fa-check"></i><b>1.3</b> Diagnostic Tools</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#visual-diagnostics"><i class="fa fa-check"></i><b>1.3.1</b> Visual Diagnostics</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#interactive-diagnostics"><i class="fa fa-check"></i><b>1.3.2</b> Interactive Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#automating-and-improving-the-cartridge-case-comparison-pipeline"><i class="fa fa-check"></i><b>1.4</b> Automating and Improving the Cartridge Case Comparison Pipeline</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#image-processing-techniques"><i class="fa fa-check"></i><b>1.4.1</b> Image Processing Techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#density-based-spatial-clustering-of-applications-with-noise"><i class="fa fa-check"></i><b>1.4.2</b> Density-Based Spatial Clustering of Applications with Noise</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#implementation-considerations"><i class="fa fa-check"></i><b>1.4.3</b> Implementation Considerations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><i class="fa fa-check"></i><b>2</b> A Study in Reproducibility: The Congruent Matching Cells Algorithm and cmcR package</a>
<ul>
<li class="chapter" data-level="" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="2.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#intro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#repeatability-and-reproducibility"><i class="fa fa-check"></i><b>2.1.1</b> Repeatability and reproducibility</a></li>
<li class="chapter" data-level="2.1.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#the-congruent-matching-cells-algorithm"><i class="fa fa-check"></i><b>2.1.2</b> The Congruent Matching Cells algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#cmcMethod"><i class="fa fa-check"></i><b>2.2</b> The CMC pipeline</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#initialData"><i class="fa fa-check"></i><b>2.2.1</b> Initial data</a></li>
<li class="chapter" data-level="2.2.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#preProcessing"><i class="fa fa-check"></i><b>2.2.2</b> Pre-processing procedures</a></li>
<li class="chapter" data-level="2.2.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#comparisonProcedure"><i class="fa fa-check"></i><b>2.2.3</b> “Correlation cell” comparison procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#decision-rule"><i class="fa fa-check"></i><b>2.2.4</b> Decision rule</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#discussion"><i class="fa fa-check"></i><b>2.3</b> Discussion</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#ambiguities"><i class="fa fa-check"></i><b>2.3.1</b> Ambiguity in algorithmic descriptions</a></li>
<li class="chapter" data-level="2.3.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#investigation"><i class="fa fa-check"></i><b>2.3.2</b> CMC pattern matching pipeline</a></li>
<li class="chapter" data-level="2.3.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#processing-condition-sensitivity"><i class="fa fa-check"></i><b>2.3.3</b> Processing condition sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#conclusion"><i class="fa fa-check"></i><b>2.4</b> Conclusion</a></li>
<li class="chapter" data-level="2.5" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#acknowledgement"><i class="fa fa-check"></i><b>2.5</b> Acknowledgement</a></li>
<li class="chapter" data-level="2.6" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#computational-details"><i class="fa fa-check"></i><b>2.6</b> Computational details</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><i class="fa fa-check"></i><b>3</b> Diagnostic Tools for Cartridge Case Comparison Algorithms</a>
<ul>
<li class="chapter" data-level="" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#abstract-1"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#background-and-introduction"><i class="fa fa-check"></i><b>3.2</b> Background and Introduction</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#notational-conventions"><i class="fa fa-check"></i><b>3.2.1</b> Notational Conventions</a></li>
<li class="chapter" data-level="3.2.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#registration-procedure"><i class="fa fa-check"></i><b>3.2.2</b> Registration Procedure</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostics-1"><i class="fa fa-check"></i><b>3.3</b> Visual Diagnostics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-x3p-plot"><i class="fa fa-check"></i><b>3.3.1</b> The X3P Plot</a></li>
<li class="chapter" data-level="3.3.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-comparison-plot"><i class="fa fa-check"></i><b>3.3.2</b> The Comparison Plot</a></li>
<li class="chapter" data-level="3.3.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics"><i class="fa fa-check"></i><b>3.3.3</b> Visual Diagnostic Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#statistical-learning-from-visual-diagnostics"><i class="fa fa-check"></i><b>3.4</b> Statistical Learning from Visual Diagnostics</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics-as-features"><i class="fa fa-check"></i><b>3.4.1</b> Visual Diagnostic Statistics as Features</a></li>
<li class="chapter" data-level="3.4.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#binary-classification-results"><i class="fa fa-check"></i><b>3.4.2</b> Binary Classification Results</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#discussion-1"><i class="fa fa-check"></i><b>3.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#case-studies"><i class="fa fa-check"></i><b>3.5.1</b> Case Studies</a></li>
<li class="chapter" data-level="3.5.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#sensitivity-to-filter-threshold"><i class="fa fa-check"></i><b>3.5.2</b> Sensitivity to Filter Threshold</a></li>
<li class="chapter" data-level="3.5.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#interactive-cartridgeinvestigatr-application"><i class="fa fa-check"></i><b>3.5.3</b> Interactive cartridgeInvestigatR application</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#conclusion-1"><i class="fa fa-check"></i><b>3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html"><i class="fa fa-check"></i><b>4</b> Automatic Matching of Cartridge Case Impressions</a>
<ul>
<li class="chapter" data-level="" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#abstract-2"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="4.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#previous-work"><i class="fa fa-check"></i><b>4.1.1</b> Previous Work</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#cartridge-case-data"><i class="fa fa-check"></i><b>4.2</b> Cartridge Case Data</a></li>
<li class="chapter" data-level="4.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#methods"><i class="fa fa-check"></i><b>4.3</b> Methods</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#pre-processing"><i class="fa fa-check"></i><b>4.3.1</b> Pre-processing</a></li>
<li class="chapter" data-level="4.3.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#comparing"><i class="fa fa-check"></i><b>4.3.2</b> Comparing</a></li>
<li class="chapter" data-level="4.3.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#scoring"><i class="fa fa-check"></i><b>4.3.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#results"><i class="fa fa-check"></i><b>4.4</b> Results</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#roc-curves"><i class="fa fa-check"></i><b>4.4.1</b> ROC Curves</a></li>
<li class="chapter" data-level="4.4.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#optimized-model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Optimized Model Comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#similarity-score-investigation"><i class="fa fa-check"></i><b>4.4.3</b> Similarity Score Investigation</a></li>
<li class="chapter" data-level="4.4.4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#feature-importance"><i class="fa fa-check"></i><b>4.4.4</b> Feature Importance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#discussion-2"><i class="fa fa-check"></i><b>4.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#comparison-to-cmc-methodology"><i class="fa fa-check"></i><b>4.5.1</b> Comparison to CMC Methodology</a></li>
<li class="chapter" data-level="4.5.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#sensitivity-to-parameter-choice"><i class="fa fa-check"></i><b>4.5.2</b> Sensitivity to Parameter Choice</a></li>
<li class="chapter" data-level="4.5.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#model-selection-considerations"><i class="fa fa-check"></i><b>4.5.3</b> Model Selection Considerations</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#conclusion-2"><i class="fa fa-check"></i><b>4.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="computational-details-1.html"><a href="computational-details-1.html"><i class="fa fa-check"></i>Computational Details</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="4.7" data-path="appendix.html"><a href="appendix.html#registration-procedure-details"><i class="fa fa-check"></i><b>4.7</b> Registration Procedure Details</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="appendix.html"><a href="appendix.html#cell-based-registration-details"><i class="fa fa-check"></i><b>4.7.1</b> Cell-Based Registration Details</a></li>
<li class="chapter" data-level="4.7.2" data-path="appendix.html"><a href="appendix.html#registration-based-feature-distributions"><i class="fa fa-check"></i><b>4.7.2</b> Registration-Based Feature Distributions</a></li>
<li class="chapter" data-level="4.7.3" data-path="appendix.html"><a href="appendix.html#density-based-feature-distributions"><i class="fa fa-check"></i><b>4.7.3</b> Density-Based Feature Distributions</a></li>
<li class="chapter" data-level="4.7.4" data-path="appendix.html"><a href="appendix.html#visual-diagnostic-feature-distributions"><i class="fa fa-check"></i><b>4.7.4</b> Visual Diagnostic Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="appendix.html"><a href="appendix.html#model-specific-results"><i class="fa fa-check"></i><b>4.8</b> Model-Specific Results</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Cartridge Case Comparison Pipeline</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="automatic-matching-of-cartridge-case-impressions" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Automatic Matching of Cartridge Case Impressions<a href="automatic-matching-of-cartridge-case-impressions.html#automatic-matching-of-cartridge-case-impressions" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="abstract-2" class="section level2 unnumbered hasAnchor">
<h2>Abstract<a href="automatic-matching-of-cartridge-case-impressions.html#abstract-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Forensic examinations attempt to solve the binary classification problem of whether two pieces of evidence originated from the same source.
For example, a cartridge case found at a crime scene may be compared to a cartridge case fired from a suspect’s firearm.
Forensic examiners traditionally rely on high-powered comparison microscopes, case facts, and their own experience to arrive at a source conclusion.
Automatic comparison algorithms have grown in prevalence in a number of forensic
disciplines following the reports from <span class="citation">National Research Council (<a href="#ref-council_strengthening_2009" role="doc-biblioref">2009</a>)</span> and <span class="citation">President’s Council of Advisors on Sci. &amp; Tech. (<a href="#ref-pcast2016" role="doc-biblioref">2016</a>)</span>.
Many of these algorithms objectively measure the similarity between evidence, such as two fired cartridge cases, based on markings left on their surface, such as impressions left by a firearm’s breech face during the firing process.
We introduce the Automatic Cartridge Evidence Scoring (ACES) algorithm to compare pairs of three-dimensional topographical surface scans of breech face impressions.
The ACES algorithm pre-processes the scans, extracts numeric features, and returns a similarity score indicating whether two cartridge cases were fired from the same firearm.
The numeric features are computed based on a cell-by-cell registration procedure, results from a density-based unsupervised clustering algorithm, and derived from visual diagnostic tools we developed to investigate the performance of cartridge case comparison algorithms.
We use scans taken at the Roy J Carver High Resolution Microscopy Facility of cartridge cases collected by <span class="citation">Baldwin et al. (<a href="#ref-Baldwin2014" role="doc-biblioref">2014</a>)</span> to train and test the ACES algorithm.
The performance of ACES compares favorably to several other methods, such as random forests on smaller feature sets, logistic regressions, decision trees, and some variants of previous Congruent Matching Cells methods <span class="citation">(<a href="#ref-song_proposed_2013" role="doc-biblioref">Song 2013</a>; <a href="#ref-zhang_convergence_2021" role="doc-biblioref">Zhang et al. 2021</a>)</span>.
We implement the ACES algorithm’s scoring functionality in a free, open-source R package called <code>scored</code>.</p>
</div>
<div id="introduction-1" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Introduction<a href="automatic-matching-of-cartridge-case-impressions.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A <em>cartride case</em> is the part of firearm ammunition that houses the projectile and propulsive device.
When a firearm is discharged and the projectile travels down the barrel, the cartridge case moves in the opposite direction and slams against the back wall, the <em>breech face</em>, of the firearm.
Markings on the breech face are “stamped” into the surface of the cartridge case leaving so-called <em>breech face impressions</em>.</p>
<p>In a traditional examination, forensic examiners use these impressions analogous to a fingerprint to determine whether two cartridge cases were fired from the same firearm.
The top of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:examinationAlgorithmComparison">4.1</a> illustrates this procedure <span class="citation">(<a href="#ref-xiaoHui_seminar" role="doc-biblioref">Xiao Hui Tai 2018</a>; <a href="#ref-Zheng2014" role="doc-biblioref">X. Zheng et al. 2014</a>; <a href="#ref-Vorburger2015" role="doc-biblioref">Vorburger, Song, and Petraco 2015</a>)</span>.
First, two cartridge cases are collected - perhaps one is from a crime scene and the other is collected from a suspect’s gun.
An examiner places the two cartridge cases beneath a “comparison microscope” that merges the views of two compound microscopes into a single split view, similar to the side-by-side cartridge case image in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:examinationAlgorithmComparison">4.1</a>.
The examiner assesses the degree of similarity between the markings on the cartridge cases and chooses one of four conclusions <span class="citation">AFTE Criteria for Identification Committee (<a href="#ref-AFTE1992" role="doc-biblioref">1992</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Identification:</strong> cartridge cases were fired from the same firearm</p></li>
<li><p><strong>Elimination:</strong> cartridge cases were not fired from the same firearm</p></li>
<li><p><strong>Inconclusive:</strong> the evidence is insufficient to make an identification or elimination</p></li>
<li><p><strong>Unsuitable:</strong> the evidence is unsuitable for examination</p></li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:examinationAlgorithmComparison"></span>
<img src="images/chapter4-images/examination_vs_algorithm_comparison_oneScanSideView.png" alt="Comparison of the traditional examination vs. the currently proposed method for comparing cartridge cases. Both start with two fired cartridge cases. In traditional examination, an examiner uses a microscope to assess the &quot;agreement&quot; of markings on the two cartridge case surfaces. They decide whether or not the cartridge cases were fired from the same firearm, or if there is inconclusive evidence to decide. In the ACES algorithm, we take a topographical scan of the cartridge case surfaces and manually identify the regions containing distinguishable markings (shown in red). We pass these scans to the ACES algorithm, which processes and compares the two scans. The final result is a numerical measure of similarity of the two cartridge cases." width="\textwidth" />
<p class="caption">
Figure 4.1: Comparison of the traditional examination vs. the currently proposed method for comparing cartridge cases. Both start with two fired cartridge cases. In traditional examination, an examiner uses a microscope to assess the “agreement” of markings on the two cartridge case surfaces. They decide whether or not the cartridge cases were fired from the same firearm, or if there is inconclusive evidence to decide. In the ACES algorithm, we take a topographical scan of the cartridge case surfaces and manually identify the regions containing distinguishable markings (shown in red). We pass these scans to the ACES algorithm, which processes and compares the two scans. The final result is a numerical measure of similarity of the two cartridge cases.
</p>
</div>
<p>Critics of traditional forensic examinations cite a lack of “foundational validity” underlying the procedures used by firearm and toolmark examiners <span class="citation">(<a href="#ref-council_strengthening_2009" role="doc-biblioref">National Research Council 2009</a>; <a href="#ref-pcast2016" role="doc-biblioref">President’s Council of Advisors on Sci. &amp; Tech. 2016</a>)</span>.
In particular, examiners rely largely on their subjective findings rather than on a well-defined procedure to measure similarity.
<span class="citation">President’s Council of Advisors on Sci. &amp; Tech. (<a href="#ref-pcast2016" role="doc-biblioref">2016</a>)</span> pushed for “developing and testing image-analysis algorithms” to objectively measure the similarity between cartridge cases.
An automatic comparison algorithm could supplement, inform, or dictate an examiner’s conclusion <span class="citation">(<a href="#ref-Swofford2021" role="doc-biblioref">Swofford and Champod 2021</a>)</span>.</p>
<p>We introduce a novel Automatic Cartridge Evidence Scoring (ACES) algorithm to objectively compare cartridge case evidence based on their breech face impressions.
Our algorithm encompasses all stages of the comparison procedure after collecting a scan of the cartridge case surface including pre-processing, comparing, and scoring.
Our ACES algorithm is available open-source as part of the <a href="https://jzemmels.github.io/scored/">scored</a> R package.</p>
<p>In the following sections, we first review recently proposed algorithms to compare firearm evidence.
We then discuss the data collection procedure to obtain 510 cartridge scans used in training and validating the ACES algorithm.
To our knowledge, this is the largest published study of a cartridge case comparison algorithm to-date, with the next largest analyzing four different data sets totaling 195 cartridge cases <span class="citation">(<a href="#ref-chen_convergence_2017" role="doc-biblioref">Chen et al. 2017</a>)</span>.
After describing the ACES algorithm, we present summary results from training and testing three binary classifier models: base on a random forest, logistic regression, and classification tree.
We discuss the strengths and weaknesses of the three classifier models and compare the relative importance of the ACES features.
We also argue that the ACES algorithm combines the classification rules of previously proposed cartridge case comparison algorithms while incorporating additional nuance.
We conclude with our thoughts on how cartridge case comparison algorithms should be developed, validated, and shared going forward.</p>
<div id="previous-work" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Previous Work<a href="automatic-matching-of-cartridge-case-impressions.html#previous-work" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recent proposals for automatic cartridge case scoring algorithms borrow from image processing and computer vision techniques.
For example, <span class="citation">Vorburger et al. (<a href="#ref-vorburger_surface_2007" role="doc-biblioref">2007</a>)</span> proposed using the cross-correlation function (CCF) to compare images or scans of cartridge case surfaces.
The CCF measures the similarity between two matrices for all possible translations of one matrix against the other.
Calculating the CCF while rotating one of the scans therefore allows for estimation of the optimal translation and rotation, together referred to as the <em>registration</em>, between the two scans; simply choose the rotation/translation at which the CCF is maximized.
<span class="citation">Hare, Hofmann, and Carriquiry (<a href="#ref-hare_automatic_2016" role="doc-biblioref">2017</a>)</span> used the CCF, among other features, to compare scans of bullets.
<span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span> developed an open-source cartridge case comparison pipeline that compared cartridge case images using the CCF.</p>
<p><span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> noted that two matching cartridge cases often share similar impressions in specific regions, so calculating the CCF between two full scans may not highlight their similarities.
Instead, <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> proposed partitioning one cartridge case scan into a grid of “cells” and calculating the CCF between each cell and the other scan.
If two cartridge cases are truly matching, then the maximum CCF value between each cell and the other scan, particularly the cells containing distinguishable breech face impressions, should be relatively large.
Furthermore, the cells should “agree” on the registration at which the CCF is maximized.
<span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> outlined the “Congruent Matching Cells” algorithm to determine the number of cells that agree on a particular registration.
A cell is classified as a Congruent Matching Cell (CMC) if its estimated registration is within some threshold of the median registration across all cells and its CCF value is above some threshold.
A number of follow-up papers proposed alterations to the the original CMC method <span class="citation">(<a href="#ref-tong_improved_2015" role="doc-biblioref">Tong, Song, and Chu 2015</a>; <a href="#ref-chen_convergence_2017" role="doc-biblioref">Chen et al. 2017</a>)</span>.
<span class="citation">Joe Zemmels, Hofmann, and VanderPlas (<a href="#ref-cmcR" role="doc-biblioref">2022</a>)</span> introduced an open-source implementation of the CMC method in the cmcR R package.
As an alternative to defining Congruent Matching Cells, <span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span> proposed using a clustering algorithm from <span class="citation">Ester et al. (<a href="#ref-Ester1996" role="doc-biblioref">1996</a>)</span> to determine the number of cells in agreement on a specific registration.</p>
<p>Currently, none of these papers have proposed rigorous procedure for comparing different cartridge case comparison algorithms.
This includes selecting optimal parameters for a specific algorithm.
<span class="citation">Joseph Zemmels, VanderPlas, and Hofmann (<a href="#ref-Zemmels2023" role="doc-biblioref">2023</a>)</span> proposed an optimization criterion to select parameters for the CMC algorithm.
Analogously, <span class="citation">Hare, Hofmann, and Carriquiry (<a href="#ref-hare_automatic_2016" role="doc-biblioref">2017</a>)</span> developed a validation procedure to select parameters for a bullet comparison algorithm.
In this work, we introduce a novel cross-validation procedure to learn and test optimal parameters for the ACES algorithm.</p>
</div>
</div>
<div id="cartridge-case-data" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Cartridge Case Data<a href="automatic-matching-of-cartridge-case-impressions.html#cartridge-case-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We use 510 cartridge cases collected as part of a study by <span class="citation">Baldwin et al. (<a href="#ref-Baldwin2014" role="doc-biblioref">2014</a>)</span>.
The authors of the original study fired 800 Remington 9mm pistol cartridge cases from each of 25 new Ruger SR9, 9mm Luger centerfire pistols..
They separated the collected cartridge cases into 15 sets of four to be sent to each of 218 forensic examiner participants.
Each set of four consisted of three cartridge cases labeled as originating from the same firearm, the “known-match” cartridge cases.
Participants performed an examination to determine whether a fourth “questioned” cartridge case shared a common source with the known-match triplet (or whether the evidence was inconclusive).</p>
<p>Across all 218 examiners, the true positive rate - proportion of correctly classified matching sets - was reported to be 99.6%.
The reported true negative rate - the proportion of correctly classified non-matching sets - was 65.2%
The discrepancy between the true positive and true negative rates can be partially explained by the number of “inconclusive” decisions made by the examiners.
Examiners reach an inconclusive decision when there is some agreement or disagreement in the characteristics between two cartridge cases, but not enough to make a match or non-match conclusion <span class="citation">(<a href="#ref-AFTE1992" role="doc-biblioref">AFTE Criteria for Identification Committee 1992</a>)</span>.
Roughly one out of five comparisons, 22.9%, were reported as inconclusive.
The vast majority, 98.5%, of these inconclusives were truly non-matching comparisons, which justifies the true negative rate of 65.2%.
There has recently been some debate about how to incorporate inconclusive decisions into accuracy/error rate estimation <span class="citation">(<a href="#ref-hofmann_inconclusives_2021" role="doc-biblioref">Hofmann, Carriquiry, and Vanderplas 2021</a>)</span>, so we do not report an overall accuracy here.</p>
<p>We scanned the 510 cartridge cases using the Cadre<span class="math inline">\(^{\text{TM}}\)</span> 3D-TopMatch High Capacity Scanner.
Briefly, this scanner collects images under various lighting conditions of a gel pad into which the base of a cartridge case is impressed.
Proprietary software that accompanies this scanner combines these images into a 2D array called a <em>surface matrix</em>.
The elements of a surface matrix represent the relative height values of the associated cartridge case surface.
This surface matrix, along with metadata concerning parameters under which the scan was taken (dimension, resolution, author, etc.), are stored in the ISO standard XML 3D Surface Profile (<code>x3p</code>) file type <span class="citation">(<a href="#ref-ISO25178-72" role="doc-biblioref"><span>“<span class="nocase">Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p</span>”</span> 2017</a>)</span>.
These x3p files can be found at <a href="https://github.com/heike/DFSC-scans" class="uri">https://github.com/heike/DFSC-scans</a>.</p>
<p>As discussed in the next section, our design differs from that used in <span class="citation">Baldwin et al. (<a href="#ref-Baldwin2014" role="doc-biblioref">2014</a>)</span>.
Rather than basing error rates on the comparison of three known-match cartridge cases to one questioned cartridge case (3 to 1), we consider the classification error rate of pairwise comparisons (1 to 1).
Further, we split the 510 cartridge cases by randomly selecting 10 of the 25 firearms for training and use the remaining 15 firearms for testing.
This resulted in a training set of 210 cartridge cases, <span class="math inline">\(\binom{210}{2} = 21,945\)</span> pairwise comparisons, and a testing set of 300 cartridge cases, <span class="math inline">\(\binom{300}{2} = 44,850\)</span> pairwise comparisons.
We note that there is a large class imbalance between the matching and non-matching comparisons in these data sets (90% of train and 93% of test comparisons are truly non-matching).
We discuss how we address this class imbalance in the methods section.</p>
</div>
<div id="methods" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Methods<a href="automatic-matching-of-cartridge-case-impressions.html#methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now discuss the methods behind the Automatic Cartridge Evidence Scoring (ACES) algorithm. We divide the methods into three stages:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Pre-processing</strong>: prepare cartridge case scans for comparison</p></li>
<li><p><strong>Comparing</strong>: compare two cartridge cases and compute similarity features</p></li>
<li><p><strong>Scoring</strong>: measure the similarity between the two cartridge cases using a trained classifier</p></li>
</ol>
<p>The following sections detail each of these stages.
Throughout, we treat “surface matrix” and “scan” synonymously.</p>
<p>The bottom of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:examinationAlgorithmComparison">4.1</a> shows a summary of our procedure.
After taking a topographical scan of the cartridge case surfaces, we manually annotate the breech face impression region (shown in red).
ACES automatically pre-processes and compares the scans resulting in a similarity score, either a binary classification or class probability, derived from a classifier model.</p>
<div id="pre-processing" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Pre-processing<a href="automatic-matching-of-cartridge-case-impressions.html#pre-processing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We first use the open-source FiX3P web application (<a href="https://github.com/talenfisher/fix3p" class="uri">https://github.com/talenfisher/fix3p</a>) to manually annotate the breech face impression region.
An example of a manually-annotated cartridge case scan is shown in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:examinationAlgorithmComparison">4.1</a>.
The FiX3P software includes functionality to “paint” the surface of a cartridge case using a computer cursor and save the painted regions to a <em>mask.</em> A mask is a 2D array of hexidecimal color values of the same dimension as its associated surface matrix.
When initialized, every element of a mask is a shade of brown (#cd7f32) by default.
Any elements painted over by the user will be replaced with the user’s selected color value.
In Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:examinationAlgorithmComparison">4.1</a>, the breech face impression region was manually annotated using a shade of red (#ff0000).</p>
<p>We pre-process the raw scans by applying a sequence of functions available in the R packages x3ptools <span class="citation">(<a href="#ref-x3ptools" role="doc-biblioref">Hofmann et al. 2020</a>)</span> and cmcR <span class="citation">(<a href="#ref-cmcR" role="doc-biblioref">Joe Zemmels, Hofmann, and VanderPlas 2022</a>)</span>.
Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:preProcessEffect">4.2</a> shows the effect that each function has on the scan surface values.
Gray pixels in each plot represent missing values in the surface matrix.
The <code>x3p_delete</code> function removes values in the scan based on the associated mask.
Next, the <code>preProcess_removeTrend</code> function subtracts a fitted conditional median plane from the surface values to “level-out” any global tilt in the scan.
The <code>preProcess_gaussFilter()</code> function applies a bandpass Gaussian filter to remove small-scale noise and other large-scale structure, which better highlights the medium-scale breech face impressions.
Finally, the <code>preProcess_erode()</code> function applies the morphological operation of erosion on the edge of the non-missing surface values <span class="citation">(<a href="#ref-Haralick1987" role="doc-biblioref">Haralick, Sternberg, and Zhuang 1987</a>)</span>.
This has the effect of shaving off values on the interior and exterior edge of the surface, which are often extreme “roll-off” values that unduly affect the comparing stage if not removed.
The final result is a cartridge case surface matrix with emphasized breech face impressions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:preProcessEffect"></span>
<img src="figures/preProcessEffect.png" alt="We apply a sequence of pre-processing functions to each scan. Each pre-processing step further emphasizes the breech face impressions in the scan." width="\textwidth" />
<p class="caption">
Figure 4.2: We apply a sequence of pre-processing functions to each scan. Each pre-processing step further emphasizes the breech face impressions in the scan.
</p>
</div>
<p>Next, we compute a set of similarity features for two pre-processed cartridge case scans.</p>
</div>
<div id="comparing" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Comparing<a href="automatic-matching-of-cartridge-case-impressions.html#comparing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we introduce a set of similarity features for two cartridge case scans.
We calculate features at two scales: between two full scans and between individual cells.
Analogous to how a forensic examiner uses a comparison microscope with different magnification levels, this allows us to assess the similarity between two scans at the macro and micro levels.</p>
<div id="notational-conventions-1" class="section level4 hasAnchor" number="4.3.2.1">
<h4><span class="header-section-number">4.3.2.1</span> Notational Conventions<a href="automatic-matching-of-cartridge-case-impressions.html#notational-conventions-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First, we introduce notation that will be used to define the features.
Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> denote two surfaces matrices that we wish to compare.
For simplicity, we assume that <span class="math inline">\(A,B \in \mathbb{R}^{k \times k}\)</span> for <span class="math inline">\(k &gt; 0\)</span>.
We use lowercase letters and subscripts to denote a particular value of a matrix: <span class="math inline">\(a_{ij}\)</span> is the value in the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column, starting from the top-left corner, of matrix <span class="math inline">\(A\)</span>.
We refer to the two known-match cartridge cases in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:matchPair">4.3</a> as exemplar matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>To accommodate structurally missing values, we adapt standard matrix algebra by encoding the notion of “missingness” into the space of real values as follows: if an element of either matrix <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> is missing, then any element-wise operation including this element is also missing.
Standard matrix algebra holds for non-missing elements.
For example, the addition operator is defined as:
<span class="math display">\[\begin{align*}
A \oplus_{NA} B = (a_{ij} \oplus_{NA} b_{ij})_{1 \leq i,j \leq k} =
\begin{cases}
a_{ij} + b_{ij} &amp; \text{if both $a_{ij}$ and $b_{ij}$ are numbers} \\
NA &amp;\text{otherwise}
\end{cases}
\end{align*}\]</span>
Other element-wise operations such as <span class="math inline">\(\ominus_{NA}\)</span> are defined similarly.
For readability, we will use standard operator notation <span class="math inline">\(+, -, &gt;, &lt;, I(\cdot), ...\)</span> and assume the extended, element-wise operations as defined above.
Note that this definition of dealing with missing values is consistent with a setting of <code>na.rm = FALSE</code> in terms of calculations in R <span class="citation">(<a href="#ref-Rlanguage" role="doc-biblioref">R Core Team 2017</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:matchPair"></span>
<img src="figures/matchPair.png" alt="A matching pair of processed cartridge case scans. We measure the similarity between these cartridge cases using the distinguishable breech face impressions on their surfaces." width="\textwidth" />
<p class="caption">
Figure 4.3: A matching pair of processed cartridge case scans. We measure the similarity between these cartridge cases using the distinguishable breech face impressions on their surfaces.
</p>
</div>
</div>
<div id="registration-estimation" class="section level4 hasAnchor" number="4.3.2.2">
<h4><span class="header-section-number">4.3.2.2</span> Registration Estimation<a href="automatic-matching-of-cartridge-case-impressions.html#registration-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A critical step in comparing <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is to find a transformation of <span class="math inline">\(B\)</span> such that it aligns best to <span class="math inline">\(A\)</span> (or vice versa).
In image processing, this is called <em>image registration.</em>
Noting that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are essentially grayscale images with structurally missing values, we rely on a standard image registration technique <span class="citation">(<a href="#ref-Brown1992" role="doc-biblioref">Brown 1992</a>)</span>.</p>
<p>In our application, a registration is composed of a discrete translation by <span class="math inline">\((m,n) \in \mathbb{Z}^2\)</span> and rotation by <span class="math inline">\(\theta \in [-180^\circ,180^\circ]\)</span>.
To determine the optimal registration, we calculate the <em>cross-correlation function</em> (CCF) between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, which measures the similarity between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for every possible translation of <span class="math inline">\(B\)</span>, denoted <span class="math inline">\((A \star B)\)</span>.
We estimate the registration by calculating the maximum CCF value across a range of rotations of matrix <span class="math inline">\(B\)</span>.
Let <span class="math inline">\(B_\theta\)</span> denote <span class="math inline">\(B\)</span> rotated by an angle <span class="math inline">\(\theta \in [-180^\circ,180^\circ]\)</span> and <span class="math inline">\(b_{\theta_{mn}}\)</span> the <span class="math inline">\(m,n\)</span>-th element of <span class="math inline">\(B_\theta\)</span>.
Then the estimated registration <span class="math inline">\((m^*,n^*,\theta^*)\)</span> is:</p>
<p><span class="math display">\[
(m^*,n^*,\theta^*) = \arg \max_{m,n,\theta} (a \star b_\theta)_{mn}.
\]</span></p>
<p>In practice we consider a discrete grid of rotations <span class="math inline">\(\pmb{\Theta} \subset [-180^\circ,180^\circ]\)</span>.
The registration procedure is outlined in Image Registration Algorithm.
We refer to the matrix that is rotated as the “target.”
The result is the estimated registration of the target matrix to the “source” matrix.</p>
<p><strong>Image Registration Algorithm</strong></p>
<div class="line-block"><strong>Data</strong>: Source matrix <span class="math inline">\(A\)</span>, target matrix <span class="math inline">\(B\)</span>, and rotation grid <span class="math inline">\(\pmb{\Theta}\)</span></div>
<div class="line-block"><strong>Result</strong>: Estimated registration of <span class="math inline">\(B\)</span> to <span class="math inline">\(A\)</span>, <span class="math inline">\((m^*, n^*, \theta^*)\)</span>, and cross-correlation function maximum <span class="math inline">\(CCF_{\max}\)</span></div>
<div class="line-block"><strong>for</strong> <span class="math inline">\(\theta \in \pmb{\Theta}\)</span> <strong>do</strong></div>
<div class="line-block">   Rotate <span class="math inline">\(B\)</span> by <span class="math inline">\(\theta\)</span> to obtain <span class="math inline">\(B_\theta\)</span>;</div>
<div class="line-block">   Calculate <span class="math inline">\(CCF_{\max,\theta} = \max_{m,n} (a \star b)_{mn}\)</span>;</div>
<div class="line-block">   Calculate translation <span class="math inline">\([m_{\theta}^*, n_{\theta}^*] = \arg \max_{m,n} (a \star b_\theta)_{mn}\)</span>;</div>
<div class="line-block"><strong>end</strong></div>
<div class="line-block">Calculate overall maximum correlation <span class="math inline">\(CCF_{\max} = \max_{\theta} \{CCF_{\max, \theta} : \theta \in \pmb{\Theta}\}\)</span>;</div>
<div class="line-block">Calculate rotation <span class="math inline">\(\theta^* = \arg \max_{\theta} \{CCF_{\max,\theta} : \theta \in \pmb{\Theta}\}\)</span>;</div>
<div class="line-block"><strong>return</strong> Estimated rotation <span class="math inline">\(\theta^*\)</span>, translation <span class="math inline">\(m^* = m_{\theta^*}^*\)</span>, and <span class="math inline">\(n^* = n_{\theta^*}^*\)</span>, and <span class="math inline">\(CCF_{\max}\)</span></div>
<p></br></p>
<p>To accommodate missing values, we also compute the <em>pairwise-complete correlation</em> using only the complete value pairs, meaning neither value is missing, between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
</div>
<div id="registration-based-features" class="section level4 hasAnchor" number="4.3.2.3">
<h4><span class="header-section-number">4.3.2.3</span> Registration-Based Features<a href="automatic-matching-of-cartridge-case-impressions.html#registration-based-features" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="full-scan-registration" class="section level5 hasAnchor" number="4.3.2.3.1">
<h5><span class="header-section-number">4.3.2.3.1</span> Full-Scan Registration<a href="automatic-matching-of-cartridge-case-impressions.html#full-scan-registration" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We first estimate the registration between two full scans <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> using Image Registration Algorithm with a rotation grid <span class="math inline">\(\pmb{\Theta} = \{-30^\circ, -27^\circ,...,27^\circ,30^\circ\}\)</span>.
This results in an estimated registration <span class="math inline">\((m^*,n^*,\theta^*)\)</span> and similarity measure <span class="math inline">\(CCF_{\max}\)</span>.
We also perform Image Registration Algorithm with the roles of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> reversed, meaning the target scan <span class="math inline">\(A\)</span> is aligned to source scan <span class="math inline">\(B\)</span>.</p>
<p>To accommodate these two comparison directions, we introduce a new subscript <span class="math inline">\(d = A,B\)</span>, referring to the source scan in Image Registration Algorithm.
Consequently, we obtain two sets of sets of estimated registrations, <span class="math inline">\((m^*_d,n^*_d,\theta^*_d)\)</span> and <span class="math inline">\(CCF_{\max,d}\)</span>, for <span class="math inline">\(d=A,B\)</span>.
For <span class="math inline">\(d = A\)</span>, we then apply the registration transformation <span class="math inline">\((m^*_A,n^*_A,\theta^*_A)\)</span> to <span class="math inline">\(B\)</span> to obtain <span class="math inline">\(B^*\)</span> and compute the pairwise-complete correlation, <span class="math inline">\(cor_{\text{full},A}\)</span>, between <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span>.
We repeat this in the other comparison direction to obtain <span class="math inline">\(cor_{\text{full},B}\)</span> and average the two:</p>
<p><span class="math display">\[
cor_{\text{full}} = \frac{1}{2}\left(cor_{A,\text{full}} + cor_{B,\text{full}}\right).
\]</span></p>
<p>We assume that the <strong>full-scan pairwise-complete correlation</strong> is large for truly matching cartridge cases.</p>
</div>
<div id="cell-based-registration" class="section level5 hasAnchor" number="4.3.2.3.2">
<h5><span class="header-section-number">4.3.2.3.2</span> Cell-Based Registration<a href="automatic-matching-of-cartridge-case-impressions.html#cell-based-registration" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>We next perform a cell-based comparison procedure, which begins with selecting one of the matrices, say <span class="math inline">\(A\)</span>, as the “source” matrix that is partitioned into a grid of cells.
The left side of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:cellGridExample">4.4</a> shows an example of such a cell grid overlaid on a scan.
Each of these source cells will be compared to the “target” matrix, in this case <span class="math inline">\(B^*\)</span>.
Because <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> are already partially aligned from the full-scan registration procedure, we compare each source cell to <span class="math inline">\(B^*\)</span> using a new rotation grid of <span class="math inline">\(\pmb{\Theta}&#39;_A = \{\theta^*_A - 2^\circ, \theta^*_A - 1^\circ,\theta^*_A,\theta^*_A + 1^\circ,\theta^*_A + 2^\circ\}\)</span>.</p>
<p>We now extend the surface matrix notation introduced previously to accommodate cells.
Let <span class="math inline">\(A_{t}\)</span> denote the <span class="math inline">\(t\)</span>-th cell of matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(t = 1,...,T_A\)</span> where <span class="math inline">\(T_A\)</span> is the total number of cells containing non-missing values in scan <span class="math inline">\(A\)</span> (e.g., <span class="math inline">\(T_A = 43\)</span> in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:cellGridExample">4.4</a>) and let <span class="math inline">\((a_t)_{ij}\)</span> denote the <span class="math inline">\(i,j\)</span>-th element of <span class="math inline">\(A_t\)</span>.</p>
<p>The cell-based comparison procedure is outlined in Cell-Based Comparison Algorithm.</p>
<p><strong>Cell-Based Comparison Algorithm</strong></p>
<div class="line-block"><strong>Data</strong>: Source matrix <span class="math inline">\(A\)</span>, target matrix <span class="math inline">\(B^*\)</span>, grid size <span class="math inline">\(R \times C\)</span>, and rotation grid <span class="math inline">\(\pmb{\Theta}_{A}&#39;\)</span></div>
<div class="line-block"><strong>Result</strong>: Estimated translations and <span class="math inline">\(CCF_{\max}\)</span> values per cell, per rotation</div>
<div class="line-block">Partition <span class="math inline">\(A\)</span> into a grid of <span class="math inline">\(R \times C\)</span> cells;</div>
<div class="line-block">Discard cells containing only missing values, leaving <span class="math inline">\(T_A\)</span> remaining cells;</div>
<div class="line-block"><strong>for</strong> <span class="math inline">\(\theta \in \pmb{\Theta}_{A}&#39;\)</span> <strong>do</strong></div>
<div class="line-block">   Rotate <span class="math inline">\(B^*\)</span> by <span class="math inline">\(\theta\)</span> to obtain <span class="math inline">\(B_{\theta}^*\)</span>;</div>
<div class="line-block">   <strong>for</strong> <span class="math inline">\(t = 1,...,T_A\)</span> <strong>do</strong></div>
<div class="line-block">      Calculate <span class="math inline">\(CCF_{\max,A,t,\theta} = \max_{m,n} (a_t \star b_{\theta}^*)_{mn}\)</span>;</div>
<div class="line-block">      Calculate translation <span class="math inline">\([m_{A,t,\theta}^*, n_{A,t,\theta}^*] = \arg \max_{m,n} (a_t \star b_{\theta}^*)_{mn}\)</span>;</div>
<div class="line-block">   <strong>end</strong></div>
<div class="line-block"><strong>end</strong></div>
<div class="line-block"><strong>return</strong> <span class="math inline">\(\pmb{F}_A = \{(m_{A,t,\theta}^*, n_{A,t,\theta}^*, CCF_{\max,A,t,\theta}, \theta) : \theta \in \pmb{\Theta}_{A}&#39;, t = 1,...,T_A\}\)</span></div>
<p></br></p>
<p>Rather than exclusively returning the registration that maximizes the overall CCF as in Image Registration Algorithm, Cell-Based Comparison Algorithm returns the set <span class="math inline">\(\pmb{F}_A\)</span> of translations and CCF values for each of the <span class="math inline">\(T_A\)</span> cells and each rotation in <span class="math inline">\(\pmb{\Theta}&#39;_A\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cellGridExample"></span>
<img src="images/chapter4-images/cellGridExample_nonMatch.png" alt="Estimated registrations of cells from a non-match pair of cartridge cases. A source scan (left) is separated into an $8 \times 8$ grid of cells. We exclude cells containing only missing values (visualized here as gray pixels). Each source cell is compared to a target scan (right) to estimate where it aligns best. We show a handful of cells at their estimated alignment in the target scan and magnify the surfaces captured by cell pairs 5, 1 and 7, 7. Although the cartridge case pair is non-matching, we note that there are similarities in the surface markings for these cell pairs." width="\textwidth" />
<p class="caption">
Figure 4.4: Estimated registrations of cells from a non-match pair of cartridge cases. A source scan (left) is separated into an <span class="math inline">\(8 \times 8\)</span> grid of cells. We exclude cells containing only missing values (visualized here as gray pixels). Each source cell is compared to a target scan (right) to estimate where it aligns best. We show a handful of cells at their estimated alignment in the target scan and magnify the surfaces captured by cell pairs 5, 1 and 7, 7. Although the cartridge case pair is non-matching, we note that there are similarities in the surface markings for these cell pairs.
</p>
</div>
<p>Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:cellGridExample">4.4</a> shows the estimated registrations of cells between two non-match cartridge cases.
We magnify the surface values captured by cell pairs 5, 1 and 7, 7 and note the similarities in the surface values; for example, the dark purple region in the middle of the cell 7, 7 pair.</p>
<p>Just as with the whole-scan registration, we calculate the pairwise-complete correlation between each cell <span class="math inline">\(A_t\)</span> and a matrix <span class="math inline">\(B_{\theta,t}^*\)</span> of the same size extracted from <span class="math inline">\(B^*_{\theta}\)</span> after translating by <span class="math inline">\([m^*_{A,\theta},n^*_{A,\theta}]\)</span>.
From this we obtain a set of pairwise-complete correlations for each cell and rotation: <span class="math inline">\(\{cor_{A,t,\theta} : t = 1,...,T_A, \theta \in \pmb{\Theta}&#39;_A\}\)</span>.</p>
<p>We repeat Cell-Based Comparison Algorithm and the pairwise-complete correlation calculation using <span class="math inline">\(B\)</span> as the source scan and <span class="math inline">\(A^*\)</span> as the target, resulting in cell-based registration set <span class="math inline">\(\pmb{F}_B\)</span> and pairwise-complete correlations <span class="math inline">\(\{cor_{B,t,\theta} : t = 1,...,T_B, \theta \in \pmb{\Theta}&#39;_B\}\)</span>.</p>
<p>For <span class="math inline">\(d = A,B\)</span> and <span class="math inline">\(t = 1,...,T_d\)</span>, define the cell-wise maximum pairwise-complete correlation as:</p>
<p><span class="math display">\[
cor_{d,t} = \max_{\theta} \{cor_{d,t,\theta} : \theta \in \pmb{\Theta}&#39;_d\}
\]</span></p>
<p>We compute two features, the <strong>average</strong> and <strong>standard deviation of the cell-based pairwise-complete correlations</strong>, using the correlation data:</p>
<p><span class="math display">\[\begin{align*}
\overline{cor}_{\text{cell}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} cor_{d,t} \\
s_{cor} &amp;= \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} (cor_{d,t} - \overline{cor}_{\text{cell}})^2}
\end{align*}\]</span></p>
<p>We expect <span class="math inline">\(\overline{cor}_{\text{cell}}\)</span> and <span class="math inline">\(s_{cor}\)</span> to be large for truly matching cartridge case pairs relative to non-matching pairs.</p>
<p>For <span class="math inline">\(d = A,B\)</span> and <span class="math inline">\(t = 1,...,T_d\)</span>, define the per-cell estimated translations and rotation as:
<span class="math display">\[\begin{align*}
\theta^*_{d,t} &amp;= \arg \max_{\theta} \{CCF_{\max,d,t,\theta} : \theta \in \pmb{\Theta}&#39;_d\} \\
m^*_{d,t} &amp;= m^*_{\theta^*_{d,t},d,t} \\
n^*_{d,t} &amp;= n^*_{\theta^*_{d,t},d,t}
\end{align*}\]</span></p>
<p>We compute the <strong>standard deviation of the cell-based estimated registrations</strong> using the estimated translations and rotations:</p>
<p><span class="math display">\[\begin{align*}
s_{\theta^*} =  \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} (\theta^*_{d,t} - \bar{\theta}^*)^2} \\
s_{m^*} =  \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} (m^*_{d,t} - \bar{m}^*)^2} \\
s_{n^*} = \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} (n^*_{d,t} - \bar{n}^*)^2}
\end{align*}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align*}
\bar{m}^* &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}}\sum_{t=1}^{T_d} m^*_{d,t} \\
\bar{n}^* &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} n^*_{d,t} \\
\bar{\theta}^* &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} \theta^*_{d,t}.
\end{align*}\]</span></p>
<p>We expect <span class="math inline">\(s_{\theta^*}, s_{m^*},s_{n^*}\)</span> to be small for truly matching cartridge case pairs relative to non-matching pairs.</p>
<p>From the full-scan and cell-based registration procedures, we obtain six features summarized in Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:registrationFeatures-html">4.1</a>.</p>
<table>
<caption>
<span id="tab:registrationFeatures-html">Table 4.1: </span>Six similarity features based on registering full scans and cells.
</caption>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(cor_{\text{full}}\)</span>
</td>
<td style="text-align:left;">
Full-scan pairwise-complete correlation
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\overline{cor}_{\text{cell}}\)</span>
</td>
<td style="text-align:left;">
Average cell-based pairwise-complete correlation
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(s_{cor}\)</span>
</td>
<td style="text-align:left;">
Standard deviation of the cell-based pairwise-complete correlations
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(s_{m^*}\)</span>
</td>
<td style="text-align:left;">
Standard deviation of the cell-based vertical translaitons (in microns)
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(s_{n^*}\)</span>
</td>
<td style="text-align:left;">
Standard deviation of the cell-based horizontal translations (in microns)
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(s_{\theta^*}\)</span>
</td>
<td style="text-align:left;">
Standard deviation of the cell-based rotations (degrees)
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="density-based-features" class="section level4 hasAnchor" number="4.3.2.4">
<h4><span class="header-section-number">4.3.2.4</span> Density-Based Features<a href="automatic-matching-of-cartridge-case-impressions.html#density-based-features" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We wish to identify when multiple cells agree on, or cluster around, a particular registration value.
However, pursuant with the notion that only certain regions of matching cartridge cases contain distinctive markings, it is unreasonable to assume and empirically rare that <strong>all</strong> cells agree on a single registration.
In fact, it is common for many cells to disagree on a registration.
For example, the left scatterplot in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:dbscanScatterplot">4.5</a> shows the per-cell estimated translations <span class="math inline">\([m^*_{A,t,\theta}, n^*_{A,t,\theta}]\)</span> when scan <span class="math inline">\(A\)</span> is used as source and <span class="math inline">\(B^*\)</span> as target rotated by <span class="math inline">\(\theta = 3^\circ\)</span>.
The right scatterplot shows the per-cell estimated translations with the roles of <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> reversed for <span class="math inline">\(\theta = -3^\circ\)</span>.
We see distinctive clusters, the black points, in both plots among many noisy, gray points.
The task is to isolate the clusters amongst such noise.</p>
<p>We use the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm proposed by <span class="citation">Ester et al. (<a href="#ref-Ester1996" role="doc-biblioref">1996</a>)</span> to identify clusters.
Compared to other clustering algorithms such as k-means <span class="citation">(<a href="#ref-MacQueen1967" role="doc-biblioref">MacQueen 1967</a>)</span>, DBSCAN does not require a pre-defined number of expected clusters.
Instead, the algorithm forms clusters if the number of points within an <span class="math inline">\(\epsilon &gt; 0\)</span> distance of a point exceeds some pre-defined threshold, <span class="math inline">\(minPts &gt; 1\)</span>.
If a point does not belong to a cluster, then DBSCAN labels that point as “noise.”
In Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:dbscanScatterplot">4.5</a>, we use DBSCAN with <span class="math inline">\(\epsilon = 5\)</span> and <span class="math inline">\(minPts = 5\)</span> to identify clusters of size 14 and 13, respectively, visualized as black points.
These cluster sizes suggest that the scans match.
Additionally, the mean cluster centers are approximately opposites of each other: <span class="math inline">\((\hat{m}_A,\hat{n}_A,\hat{\theta}_A) \approx (16.9, -16.7, 3^\circ)\)</span> when <span class="math inline">\(A\)</span> is used as source compared to <span class="math inline">\((\hat{m}_B,\hat{n}_B,\hat{\theta}_B) \approx (-16.2, 16.8, -3^\circ)\)</span> when <span class="math inline">\(B^*\)</span> is used as source.
This provides further evidence of a match.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dbscanScatterplot"></span>
<img src="figures/dbscanScatterplot.png" alt="Cluster assignments based on the Density Based Spatial Clustering with Applications to Noise (DBSCAN) algorithm for estimated translations in two comparison directions. Using scan $A$ as source results in a cluster of size 14 (left) compared to 13 when scan $B^*$ is used as source (right). Noting the reversed axes in the right plot, we see that the clusters are located approximately opposite of each other. Points are jittered for visibility." width=".8\textwidth" />
<p class="caption">
Figure 4.5: Cluster assignments based on the Density Based Spatial Clustering with Applications to Noise (DBSCAN) algorithm for estimated translations in two comparison directions. Using scan <span class="math inline">\(A\)</span> as source results in a cluster of size 14 (left) compared to 13 when scan <span class="math inline">\(B^*\)</span> is used as source (right). Noting the reversed axes in the right plot, we see that the clusters are located approximately opposite of each other. Points are jittered for visibility.
</p>
</div>
<p>To calculate the density-based features, we first use a 2D kernel density estimator <span class="citation">(<a href="#ref-MASS" role="doc-biblioref">Venables and Ripley 2002</a>)</span> to identify the rotation <span class="math inline">\(\hat{\theta}_d\)</span> at which the per-cell translations achieve the highest density.
Next, we compute clusters using the DBSCAN algorithm amongst the estimated translations <span class="math inline">\(\{(m^*_{d,t,\hat{\theta}_d},n^*_{d,t,\hat{\theta}_d}) : t = 1,...,T_d\}\)</span> like those shown in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:dbscanScatterplot">4.5</a>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>
Let <span class="math inline">\(\pmb{C}_d\)</span> denote the set of cells in the DBSCAN cluster.
We treat the mean cluster centers as the estimated translations <span class="math inline">\([\hat{m}_d,\hat{n}_d]\)</span>.</p>
<p>We calculate four features from the density-based clustering procedure: <strong>average DBSCAN cluster size</strong> <span class="math inline">\(C\)</span>, the <strong>DBSCAN cluster indicator</strong> <span class="math inline">\(C_0\)</span>, and the <strong>root sum of squares of the dens</strong>ity-estimated registrations <span class="math inline">\((\Delta_\theta, \Delta_{\text{trans}})\)</span> defined as:</p>
<p><span class="math display">\[\begin{align*}
C &amp;= \frac{1}{2}\left(|\pmb{C}_A| + |\pmb{C}_B|\right) \\
C_0 &amp;= I(|\pmb{C}_A| &gt; 0 \text{ and } |\pmb{C}_B| &gt; 0)\\
\Delta_\theta &amp;= |\hat{\theta}_A + \hat{\theta}_B| \\
\Delta_{\text{trans}} &amp;= \sqrt{(\hat{m}_A + \hat{m}_B)^2 + (\hat{n}_A + \hat{n}_B)^2}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(|\pmb{C}_d|\)</span> denotes the cardinality of <span class="math inline">\(\pmb{C}_d\)</span> and <span class="math inline">\(I(\cdot)\)</span> is the identity function equal to 1 if the predicate argument “<span class="math inline">\(\cdot\)</span>” evaluates to TRUE and 0 otherwise.
We use both <span class="math inline">\(C\)</span> and <span class="math inline">\(C_0\)</span> because of potential missingness in the values of <span class="math inline">\(C\)</span> if no cluster is identified.
Missing <span class="math inline">\(C\)</span> values are imputed using the median non-missing value when fitting classifiers, so the missingness information is retained in <span class="math inline">\(C_0\)</span>.</p>
<p>For truly matching cartridge case pairs, we expect <span class="math inline">\(C\)</span> to be large, <span class="math inline">\(C_0\)</span> to be 1, and <span class="math inline">\(\Delta_\theta, \Delta_{\text{trans}}\)</span> to be small relative to non-matching pairs. We obtain four density-based features summarized in Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:dbscanFeatures-html">4.2</a>.</p>
<table>
<caption>
<span id="tab:dbscanFeatures-html">Table 4.2: </span>Four similarity features based on the density-based clustering procedure.
</caption>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(C\)</span>
</td>
<td style="text-align:left;">
Average DBSCAN cluster size
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(C_0\)</span>
</td>
<td style="text-align:left;">
DBSCAN cluster indicator
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\Delta_{\theta}\)</span>
</td>
<td style="text-align:left;">
Root sum of squares of the cluster-estimated translations (in microns)
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\Delta_{\text{trans}}\)</span>
</td>
<td style="text-align:left;">
Root sum of squares of the cluster-estimated translations (in microns)
</td>
</tr>
</tbody>
</table>
</div>
<div id="visual-diagnostic-features" class="section level4 hasAnchor" number="4.3.2.5">
<h4><span class="header-section-number">4.3.2.5</span> Visual Diagnostic Features<a href="automatic-matching-of-cartridge-case-impressions.html#visual-diagnostic-features" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The final set of features we calculate are based on visual diagnostic tools described in Chapter 3.
These numerical features quantify the qualitative observations one can make from the diagnostics.
We provide a broad summary of the visual diagnostics features here, but we encourage the reader to see Chapter 3 to learn more.</p>
<p>To construct the visual diagnostics, we perform element-wise operations on two matrices.
For the full scan comparisons, we perform these operations on the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span>, which is the aligned version of <span class="math inline">\(B\)</span> after applying the image registration algorithm.
We introduce the visual diagnostics in the context of the full scan comparisons, but they generalize smoothly to the case of cell comparisons as well.</p>
<p>For a matrix <span class="math inline">\(X \in \mathbb{R}^{k \times k}\)</span> and Boolean-valued condition matrix <span class="math inline">\(cond: \mathbb{R}^{k \times k} \to \{TRUE,FALSE\}^{k \times k}\)</span>, we define an element-wise filter operation <span class="math inline">\(\mathcal{F}: \mathbb{R}^{k \times k} \to \mathbb{R}^{k \times k}\)</span> as:</p>
<p><span class="math display">\[\begin{align*}
\mathcal{F}_{cond}(X) =
(f_{ij})_{1 \leq i,j \leq k} =
\begin{cases}
x_{ij} &amp;\text{if $cond$ is $TRUE$ for element $i,j$} \\
NA &amp;\text{otherwise}
\end{cases}
\end{align*}\]</span></p>
<p>For example, <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(A)\)</span> contains elements of matrix <span class="math inline">\(A\)</span> where the pair of scans <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> deviate by at least <span class="math inline">\(\tau &gt; 0\)</span>.
Surface values in <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> that are “close,” meaning within <span class="math inline">\(\tau\)</span> distance, to each other are replaced with <span class="math inline">\(NA\)</span> in this filtered matrix.</p>
<p>We compute features based on the elements of the <span class="math inline">\(cond\)</span> matrix.
First, we consider the ratio between the number of <span class="math inline">\(TRUE\)</span> and <span class="math inline">\(FALSE\)</span> elements in the matrix <span class="math inline">\(|A - B^*| \leq \tau\)</span>, where <span class="math inline">\(TRUE\)</span> values correspond to elements of <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> within <span class="math inline">\(\tau\)</span> distance of each other.
The definition of this ratio is given by <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#eq:ratio">(3.1)</a>.</p>
<p><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:similaritiesDifferencesRatio">3.12</a> in Chapter 3 shows an example of this ratio for two matching <span class="math inline">\(A = K013sA1\)</span> and <span class="math inline">\(B = K013sA2\)</span>.
The value of the “similarities vs. differences ratio” is 2.84 using a filter threshold of <span class="math inline">\(\tau = 1\)</span> micron
If we consider the <span class="math inline">\(TRUE\)</span> and <span class="math inline">\(FALSE\)</span> elements of the <span class="math inline">\(|A - B^*| \leq \tau\)</span> matrix as the elements of <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> that are most “similar” and “different” then we can interpret this value to mean that there are 2.84 times as similarities as there are differences.</p>
<p>Because we perform comparisons in both directions <span class="math inline">\(d = A,B\)</span>, we end up with two similarities vs. differences ratio values.
We compute the arithmetic mean between these two values to obtain a summary feature for the cartridge case pair as a whole:
<span class="math display">\[
r_{\text{full}} = \frac{1}{2}(r_A + r_B).
\]</span>
We assume that the <strong>full-scan similarities vs. differences ratio</strong> will be larger for matching cartridge case pairs than non-matches.</p>
<p>As another source of features, we consider the size of <span class="math inline">\(TRUE\)</span>-valued neighborhoods in the <span class="math inline">\(cond\)</span> matrix <span class="math inline">\(|A - B^*| &gt; \tau\)</span>, which is the logical complement of the <span class="math inline">\(cond\)</span> matrix used to compute <span class="math inline">\(r_{\text{full}}\)</span>.
An example of one such matrix can be seen on the left side of <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:filterLabeling">3.13</a>.
We consider each neighborhood as a set of adjacent elements, <span class="math inline">\(S\)</span>, that all have the same value.
Two elements are “adjacent” if one of their indices differs by 1 (this is a 4-neighbor or “Rook’s” scheme).
We use a connected components labeling algorithm <span class="citation">(<a href="#ref-hesselink_concurrent_2001" role="doc-biblioref">Hesselink, Meijster, and Bron 2001</a>)</span> to identify neighborhoods that all have the value <span class="math inline">\(TRUE\)</span> in <span class="math inline">\(|A - B^*| &gt; \tau\)</span>.
The middle of <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:filterLabeling">3.13</a> shows an example of neighborhood regions distinguished by fill color.
Using this procedure in both comparison directions, we obtain multiple neighborhoods that we distinguish notationally using subscripts: <span class="math inline">\(\pmb{S}_d = \{S_{d,1}, S_{d,2}, ..., S_{d, l} : l = 1,...,L_d\}\)</span> where <span class="math inline">\(L_d\)</span> is the total number of labeled <span class="math inline">\(TRUE\)</span>-valued regions in direction <span class="math inline">\(d \in \{A,B\}\)</span>.</p>
<p>Considering the size of each labeled region on the right side <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:filterLabeling">3.13</a>, we see that the distribution is heavily right-skewed with the majority of comparisons being relatively small - less than 1,000 square microns.
If we associate the <span class="math inline">\(TRUE\)</span> values of <span class="math inline">\(|A - B^*| &gt; \tau\)</span> with elements where <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> are the most “different,” then it is reasonable to assume that the size of individual <span class="math inline">\(TRUE\)</span>-valued regions would be small for a matching pair.
In other words, that there wouldn’t be large regions where <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span> are different if they truly match.</p>
<p>To compute numerical features based on this assumption, we consider the average and standard deviation of the blob sizes across the two comparison directions:</p>
<p><span class="math display">\[\begin{align*}
\overline{|S|}_{\text{full}} &amp;= \frac{1}{L_A + L_B} \sum_{d \in \{A,B\}} \sum_{l=1}^{L_d} |S_{d,l}| \\
s_{\text{full},|S|} &amp;= \sqrt{\frac{1}{L_A + L_B - 1} \sum_{d \in \{A,B\}} \sum_{l=1}^{L_d} (|S_{d,l}| - \overline{|S|}_{\text{full}})^2}
\end{align*}\]</span></p>
<p>where <span class="math inline">\(|S_{d,l}|\)</span> is the number of elements (cardinality) in set <span class="math inline">\(S_{d,l}\)</span>, <span class="math inline">\(l = 1,...,L_d\)</span> and <span class="math inline">\(d = A,B\)</span>.
We assume that the <strong>average</strong> and <strong>standard deviation of the full-scan neighborhood sizes</strong> will be small for matching pairs compared to non-matching pairs.</p>
<p>As a final source of features, we consider the correlation <span class="math inline">\(cor_{d,\text{full},\text{diff}}\)</span> between the elements of the matrices <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(A)\)</span> and <span class="math inline">\(\mathcal{F}_{|A - B^*| &gt; \tau}(B^*)\)</span> for <span class="math inline">\(d = A\)</span> and <span class="math inline">\(\mathcal{F}_{|A^* - B| &gt; \tau}(A^*)\)</span> and <span class="math inline">\(\mathcal{F}_{|A^* - B| &gt; \tau}(B)\)</span> for <span class="math inline">\(d = B\)</span>.
If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are truly matching, then we might assume that differences in the height values occur merely because of variability in the amount of contact between the cartridge case and breech face across multiple fires of a single firearm.
Figure <a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#fig:matchingCellDifferenceCorrelation">3.14</a> shows an example of the “filtered differences” between two cells <span class="math inline">\(\mathcal{F}_{|A_t - B_t^*| &gt; \tau}(A_t)\)</span> and <span class="math inline">\(\mathcal{F}_{|A_t - B_t^*| &gt; \tau}(B_t^*)\)</span>.
Although the surface values in the cell on the right appear to be shifted up relative to the values from the cell on the left, as evidenced by the generally darker shades of orange, the trends in the surface values are similar.
This is reflected in the relatively high correlation value of 0.84 between these two filtered cells.</p>
<p>Similar to the other features, we use the average “differences correlation” value <span class="math inline">\(cor_{d,\text{full},\text{diff}}\)</span> across the two comparison directions as a feature:</p>
<p><span class="math display">\[
cor_{\text{full},\text{diff}} = \frac{1}{2}\left(cor_{A,\text{full},\text{diff}} + cor_{B,\text{full},\text{diff}}\right).
\]</span></p>
<p>We assume that the <strong>full-scan differences correlation</strong> will be large for matches compared to non-matches.</p>
<p>So far, we’ve discussed visual diagnostic features for full scan comparisons.
We also compute summary features based on the visual diagnostics applied to the cell-based comparisons.
Letting <span class="math inline">\(A_t\)</span> and <span class="math inline">\(B_t^*\)</span> denote a pair of aligned cells from scans <span class="math inline">\(A\)</span> and <span class="math inline">\(B^*\)</span>, respectively, we extend the full scan notation introduced above by adding <span class="math inline">\(t\)</span> subscripts.</p>
<p>We compute the <strong>average</strong> and <strong>standard deviation of the cell-based similarities vs. differences ratio</strong> values:
<span class="math display">\[\begin{align*}
\bar{r}_{\text{cell}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t = 1}^{T_d} r_{d,t} \\
s_{\text{cell}, r} &amp;= \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t = 1}^{T_d} (r_{d,t} - \bar{r}_{\text{cell}})^2},
\end{align*}\]</span></p>
<p>We also calculate the per-cell average and standard deviation labeled neighborhood size:
<span class="math display">\[\begin{align*}
\overline{|S|}_{d,t} &amp;= \frac{1}{L_{d,t}} \sum_{l=1}^L |S_{d,t,l}| \\
s_{d,t,|S|} &amp;= \sqrt{\frac{1}{L_{d,t} - 1} \sum_{l=1}^{L_{d,t}} (|S_{d,t,l}| - \overline{|S|}_{\text{cell},d,t})^2},
\end{align*}\]</span>
and average these values to features for the pair as a whole:
<span class="math display">\[\begin{align*}
\overline{|S|}_{\text{cell}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} \overline{|S|}_{d,t} \\
\bar{s}_{\text{cell},|S|} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} s_{d,t,|S|}.
\end{align*}\]</span>
We assume that the <strong>average cell-wise neighborhood size</strong> and the <strong>average standard deviation of the cell-wise neighborhood sizes</strong> will be small for matching pairs relative to non-match pairs.</p>
<p>Finally, we compute the <strong>average cell-based differences correlation</strong> across all cells in both comparison directions:
<span class="math display">\[\begin{align*}
\overline{cor}_{\text{cell},\text{diff}} &amp;= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} cor_{d,t,\text{diff}}.
\end{align*}\]</span></p>
<p>Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:visualDiagnosticFeatures-html">4.3</a> summarizes the nine features based on visual diagnostics.
A deeper exploration of these features can be found in Chapter 3.
This concludes our explanation of the ACES feature set.
Next, we use the 19 ACES features to train and test classifier models.</p>
<table>
<caption>
<span id="tab:visualDiagnosticFeatures-html">Table 4.3: </span>Nine similarity features calculated based on visual diagnostics.
</caption>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(cor_{\text{full},\text{diff}}\)</span>
</td>
<td style="text-align:left;">
Full-scan differences correlation
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\overline{cor}_{\text{cell},\text{diff}}\)</span>
</td>
<td style="text-align:left;">
Average cell-wise differences correlation
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(r_{\text{full}}\)</span>
</td>
<td style="text-align:left;">
Full-scan similarities vs. differences ratio
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\bar{r}_{\text{cell}}\)</span>
</td>
<td style="text-align:left;">
Average cell-based similarities vs. differences ratio
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(s_{\text{cell}, r}\)</span>
</td>
<td style="text-align:left;">
Standard deviation of the cell-based similarities vs. differences ratio
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\overline{|S|}_{\text{full}}\)</span>
</td>
<td style="text-align:left;">
Average full-scan neighborhood size
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(s_{\text{full},|S|}\)</span>
</td>
<td style="text-align:left;">
Standard deviation of the full-scan neighborhood sizes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\overline{|S|}_{\text{cell}}\)</span>
</td>
<td style="text-align:left;">
Average cell-wise neighborhood sizes
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\bar{s}_{\text{cell},|S|}\)</span>
</td>
<td style="text-align:left;">
Average standard deviation of the cell-wise neighborhood sizes
</td>
</tr>
</tbody>
</table>
<!-- We use the average \textbf{full-scan differences correlation} as a feature: -->
<!-- \begin{align*} -->
<!-- cor_{\text{full},\text{diff}} = \frac{1}{2}\left(cor_{A,\text{full},\text{diff}} + cor_{B,\text{full},\text{diff}}\right). -->
<!-- \end{align*} -->
<!-- We assume that the "average" and "standard deviation of the full-scan neighborhood sizes" -->
<!-- We assume that $cor_{\text{full},\text{diff}}$ will be large for matching cartridge case pairs relative to non-matching pairs. -->
<!-- Said another way, we assume that regions of matching cartridge cases that are different will still follow similar trends. -->
<!-- This can occur due to variability in the amount of contact between a cartridge case and breech face across multiple fires of a single firearm. -->
<!-- We calculate the correlation by vectorizing the two filtered surface matrices and treating missing values by case-wise deletion. -->
<!-- As before, we extend our notation to accommodate cell comparisons $t = 1,...,T_d$ for $d = A,B$ using subscripts: $cor_{d,t,\text{diff}}$. -->
<!-- For example, $cor_{A,t,\text{diff}}$ is the correlation between cell filtered surface matrices $\mathcal{F}_{|A_t - B_{t,\theta_t^*}^*| > \tau}(A_t)$ and $\mathcal{F}_{|A_t - B_{t,\theta_t^*}^*| > \tau}(B_{t,\theta_t^*}^*)$ where $B_{t,\theta_t^*}^*$ is the matrix extracted from $B^*$ that maximizes the CCF with $A_t$. -->
<!-- We calculate the **average cell-based differences correlation** across all cells and both directions: -->
<!-- $$ -->
<!-- \overline{cor}_{\text{cell},\text{diff}} = \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} cor_{d,t,\text{diff}} -->
<!-- $$ -->
<!-- Next, we consider features based on the elements of the Boolean $cond$ matrix. -->
<!-- Consider Figure \@ref(fig:filterLabeling) that shows the filtered element-wise average $\mathcal{F}_{|A - B^*| < \tau}\left(\frac{1}{2}(A + B^*)\right)$ on the left and the associated $cond$ matrix $|A - B^*| > \tau$ visualized in black-and-white in the middle with filtered elements, whose $cond$ value is $TRUE$, shown in white. -->
<!-- We first calculate the ratio between such a $cond$ matrix and its complement. -->
<!-- For $d = A$, we consider the $cond$ matrices $|A - B^*| \leq \tau$ and $|A - B^*| > \tau$. -->
<!-- The ratio is given by -->
<!-- $$ -->
<!-- r_{d} = \frac{\pmb{1}^T I(|A - B^*| \leq \tau) \pmb{1}}{\pmb{1}^T I(|A - B^*| > \tau) \pmb{1}} -->
<!-- $$ -->
<!-- where $\pmb{1} \in \mathbb{R}^k$ is a column vector of ones and $I(\cdot)$ is the element-wise, matrix-valued indicator function. -->
<!-- We consider the average **full-scan similarities vs. differences ratio** across the two comparison directions: -->
<!-- $$ -->
<!-- r_{\text{full}} = \frac{1}{2}(r_A + r_B). -->
<!-- $$ -->
<!-- We expect $r_{\text{full}}$ to be large for matching pairs compared to non-matching pairs. -->
<!-- That is, truly matching pairs will have more similarities than differences. -->
<!-- We also calculate features based on the ratio for cell comparisons $t = 1,...,T_d$, $d = A,B$. -->
<!-- Let $r_{d,t}$ denote the ratio for cell comparison $t$ in direction $d$. -->
<!-- We consider the **average** and **standard deviation of the cell-based similarities vs. differences ratio**: -->
<!-- \begin{align*} -->
<!-- \bar{r}_{\text{cell}} &= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t = 1}^{T_d} r_{d,t} \\ -->
<!-- s_{\text{cell}, r} &= \sqrt{\frac{1}{T_A + T_B - 1} \sum_{d \in \{A,B\}} \sum_{t = 1}^{T_d} (r_{d,t} - \bar{r}_{\text{cell}})^2}. -->
<!-- \end{align*} -->
<!-- We expect $\bar{r}_{\text{cell}}$ and $s_{\text{cell}, r}$ to be large for matching cartridge case pairs relative to non-match pairs. -->
<!-- Another aspect of the $cond$ matrix we consider is the size of the individual filtered regions. -->
<!-- For two matching cartridge cases, we expect that there are few differences compared to similarities *and* that the different regions are relatively small. -->
<!-- We use a connected components labeling algorithm detailed in @hesselink_concurrent_2001 to identify individual "neighborhoods" of filtered elements [@imager]. -->
<!-- More precisely, the algorithm returns a set of sets $\pmb{S}_d = \{S_{d,1},S_{d,2},...,S_{d,L_d}\}$ where each $S_{d,l}$ is a set of indices of the $cond$ matrix that have a value of $TRUE$ and are connected by a chained-together sequence of 4 (Rook's) neighborhoods. -->
<!-- The right side of Figure \@ref(fig:filterLabeling) shows each $S_{d,l}$ distinguished by different fill colors, $l = 1,...,L_d$. -->
<!-- ```{r compDataFullScan,cache=TRUE,include=FALSE} -->
<!-- if(!file.exists("data/compData_fullScans.RData")){ -->
<!--   compData_fullScans <-  -->
<!--     bind_rows(map_dfr(seq(-30,30,by = 3), -->
<!--                       ~ cmcR::comparison_allTogether(reference = reference_eroded, -->
<!--                                                      target = target_eroded, -->
<!--                                                      theta = ., -->
<!--                                                      numCells = c(1,1), -->
<!--                                                      maxMissingProp = .99, -->
<!--                                                      sideLengthMultiplier = 1.1, -->
<!--                                                      returnX3Ps = TRUE)) %>% -->
<!--                 mutate(direction = "comparison_refToTarget") %>% -->
<!--                 filter(pairwiseCompCor == max(pairwiseCompCor)), -->
<!--               map_dfr(seq(-30,30,by = 3), -->
<!--                       ~ cmcR::comparison_allTogether(reference = target_eroded, -->
<!--                                                      target = reference_eroded, -->
<!--                                                      theta = ., -->
<!--                                                      numCells = c(1,1), -->
<!--                                                      maxMissingProp = .99, -->
<!--                                                      sideLengthMultiplier = 1.1, -->
<!--                                                      returnX3Ps = TRUE)) %>% -->
<!--                 mutate(direction = "comparison_targetToRef") %>% -->
<!--                 filter(pairwiseCompCor == max(pairwiseCompCor))) -->
<!--   save(compData_fullScans,file = "data/compData_fullScans.RData") -->
<!-- } -->
<!-- ``` -->
<!-- ```{r scanFilterBinarized,include=FALSE,cache = TRUE} -->
<!-- load("data/compData_fullScans.RData") -->
<!-- scanA <- compData_fullScans %>% -->
<!--   filter(direction == "comparison_refToTarget") %>% -->
<!--   pull(cellHeightValues) %>% -->
<!--   .[[1]] -->
<!-- scanBStar <- compData_fullScans %>% -->
<!--   filter(direction == "comparison_refToTarget") %>% -->
<!--   pull(alignedTargetCell) %>% -->
<!--   .[[1]] -->
<!-- scanA$surface.matrix <- (scanA$surface.matrix*scanA$cmcR.info$scaleByVal + scanA$cmcR.info$centerByVal) -->
<!-- scanBStar$surface.matrix <- (scanBStar$surface.matrix*scanBStar$cmcR.info$scaleByVal + scanBStar$cmcR.info$centerByVal) -->
<!-- compPlt <- impressions::x3p_comparisonPlot( -->
<!--   x3p1 = scanA, -->
<!--   x3p2 = scanBStar, -->
<!--   plotLabels = c("Scan A","Scan B*","Filtered Element-wise\nAverage", -->
<!--                  "Scan A\nFiltered\nDifferences","Scan B*\nFiltered\nDifferences"), -->
<!--   labelSize = 2.5, -->
<!--   legendLength = grid::unit(4,"in"), -->
<!--   legendHoriz = -1.3, -->
<!--   legendQuantiles = c(0,.01,.05,.5,.95,.99,1)) -->
<!-- plt1 <- ggplot_build(compPlt$patches$plots[[3]]) -->
<!-- plt1 <- plt1$data[[2]] %>% -->
<!--   ggplot(aes(x=x,y=y,fill=fill)) + -->
<!--   geom_raster() + -->
<!--   scale_fill_identity() + -->
<!--   coord_fixed(expand = FALSE) + -->
<!--   theme_void() + -->
<!--   geom_path(data = plt1$data[[4]], -->
<!--             aes(x=x,y=y,group=group), -->
<!--             size = .2, -->
<!--             colour = "gray40", -->
<!--             inherit.aes = FALSE) -->
<!-- scanFilterBinarized <- scanA %>%  -->
<!--   impressions::x3p_elemAverage(scanBStar) %>% -->
<!--   impressions::x3p_to_dataFrame() %>% -->
<!--   mutate(value = (abs(c({scanA$surface.matrix - scanBStar$surface.matrix})) > impressions::x3p_sd(scanA,scanBStar))) -->
<!-- plt2 <- scanFilterBinarized %>% -->
<!--   ggplot(aes(x=x,y=y,fill=value)) + -->
<!--   geom_raster() + -->
<!--   scale_fill_manual(values = c("black","white"),na.value = "gray65") + -->
<!--   coord_fixed(expand = FALSE) + -->
<!--   theme_void() + -->
<!--   theme(legend.position = "none") -->
<!-- scanFilterLabeled <- scanFilterBinarized %>% -->
<!--   mutate(x = x + 1,y = y + 1, -->
<!--          value = ifelse(!value | is.na(value),0,1)) %>% -->
<!--   imager::as.cimg() %>% -->
<!--   imager::label() %>% -->
<!--   as.data.frame() %>% -->
<!--   mutate(value = factor(value)) -->
<!-- plt3 <- scanFilterLabeled %>% -->
<!--   ggplot(aes(x=x,y=y,fill=value)) + -->
<!--   geom_raster() + -->
<!--   coord_fixed(expand = FALSE) + -->
<!--   theme_void() + -->
<!--   theme(legend.position = "none") + -->
<!--   scale_fill_manual(values = c("gray65",sample(RColorBrewer::brewer.pal(12,"Paired"),size = length(unique(scanFilterLabeled$value)) - 1,replace = TRUE))) -->
<!-- plt <- (plt1 | plt2 | plt3) & theme(panel.border = element_rect(colour = "black",fill = NA)) -->
<!-- ggsave(plot = plt,filename = "figures/filterLabeling.png") -->
<!-- knitr::plot_crop("figures/filterLabeling.png") -->
<!-- ``` -->
<!-- ```{r filterLabeling,out.width="\\textwidth",echo=FALSE,fig.pos="htbp",fig.cap='(Left) After aligning two scans, we filter regions that are "different" from each other, meaning the absolute difference between surface values is larger than some threshold. (Middle) We binarize the scan into "filtered" or "non-filtered" regions - shown in white and black, respectively. (Right) Using a connected components labeling algorithm, we identify connected "neighborhoods" of filtered elements. We assume that these neighborhoods will be small, on average, if comparing truly matching cartridge cases.'} -->
<!-- knitr::include_graphics("figures/filterLabeling.png") -->
<!-- ``` -->
<!-- We calculate the following features using the full-scan labeled neighborhoods: -->
<!-- \begin{align*} -->
<!-- \overline{|S|}_{\text{full}} &= \frac{1}{L_A + L_B} \sum_{d \in \{A,B\}} \sum_{l=1}^{L_d} |S_{d,l}| \\ -->
<!-- s_{\text{full},|S|} &= \sqrt{\frac{1}{L_A + L_B - 1} \sum_{d \in \{A,B\}} \sum_{l=1}^{L_d} (|S_{d,l}| - \overline{|S|}_{\text{full}})^2} -->
<!-- \end{align*} -->
<!-- where $|S_{d,l}|$ is the cardinality of $S_{d,l}$. -->
<!-- We assume that the **average** and **standard deviation of the full-scan neighborhood sizes** will be small for matching cartridge case  pairs relative to non-matching pairs. -->
<!-- That is to say, we assume that the the regions of $A$ and $B$ that are different will all be small, on average, and vary little in size. -->
<!-- This assumption is appropriate assuming that the breech face leaves consistent markings on fired cartridge cases. -->
<!-- Again, we extend our notation to accommodate individual cells. -->
<!-- Let $\pmb{S}_{d,t} = \{S_{d,t,1},...,S_{d,t,L_{d,t}}\}$ denote the set of labeled neighborhoods for a cell $t = 1,...,T_d$, $d = A,B$. -->
<!-- We calculate the per-cell average and standard deviation of the labeled neighborhood cell size: -->
<!-- \begin{align*} -->
<!-- \overline{|S|}_{d,t} &= \frac{1}{L_{d,t}} \sum_{l=1}^L |S_{d,t,l}| \\ -->
<!-- s_{d,t,|S|} &= \sqrt{\frac{1}{L_{d,t} - 1} \sum_{l=1}^{L_{d,t}} (|S_{d,t,l}| - \overline{|S|}_{\text{cell},d,t})^2}. -->
<!-- \end{align*} -->
<!-- We assume that the cell-based $\overline{|S|}_{d,t}$ and $s_{d,t,|S|}$ will be small, on average, for truly matching cartridge cases. -->
<!-- Consequently, we use the sample average of these as features: -->
<!-- \begin{align*} -->
<!-- \overline{|S|}_{\text{cell}} &= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} \overline{|S|}_{d,t} \\ -->
<!-- \bar{s}_{\text{cell},|S|} &= \frac{1}{T_A + T_B} \sum_{d \in \{A,B\}} \sum_{t=1}^{T_d} s_{d,t,|S|} -->
<!-- \end{align*} -->
<!-- We assume that the **average cell-wise neighborhood size** and the **average standard deviation of the cell-wise neighborhood sizes** will be small for matching cartridge case pairs relative to non-match pairs. -->
</div>
</div>
<div id="scoring" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Scoring<a href="automatic-matching-of-cartridge-case-impressions.html#scoring" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use a data set of 510 cartridge cases fired from 25 firearms.
We randomly split the data into 10 firearms for training and 15 firearms for testing.
This resulted in a training data set of 210 cartridge cases, <span class="math inline">\(\binom{210}{2} = 21,945\)</span> pairwise comparisons, and a testing set of 300 cartridge cases, <span class="math inline">\(\binom{300}{2} = 44,850\)</span> pairwise comparisons.
Because we consider every pairwise comparison between these scans, there is a relatively large class imbalance between matches and non-matches in these data sets.
Specifically, non-matching comparisons make up 19,756 of the 21,945 (90.0%) training comparisons and 41,769 of the 44,850 (93.1%) testing comparisons.</p>
<p>We use 10-fold cross-validation repeated thrice <span class="citation">(<a href="#ref-caret" role="doc-biblioref">Kuhn 2022</a>)</span> to train two binary classifiers based on a logistic regression and a random forest <span class="citation">(<a href="#ref-breiman" role="doc-biblioref">Breiman 2001</a>; <a href="#ref-randomForest" role="doc-biblioref">Liaw and Wiener 2002</a>)</span>.
These models predict the probability that a pair of cartridge cases match.
Then, the model classifies the pair as a match or non-match depending on whether the match probability exceeds a set threshold.
On top of the tunable parameters of each model (e.g., the DBSCAN parameters <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span>), we treat this threshold as a parameter to be optimized.</p>
<p>Models trained to maximize accuracy on imbalanced data often exhibit a “preference” for classifying new observations as the majority class <span class="citation">(<a href="#ref-Fernndez2018" role="doc-biblioref">Fernández et al. 2018</a>)</span>, which in our case are non-matches.
An optimization criterion commonly used for imbalanced data is to select the model that maximizes the area under the Receiver Operating Characteristic (ROC) curve, which measures the performance of a model under different threshold values <span class="citation">(<a href="#ref-James2013" role="doc-biblioref">James et al. 2013</a>)</span>.
The model that maximizes this area, commonly abbreviated AUC, is one that performs best under a variety of threshold values relative to the other models - this consistency is a desired trait.
Using the ROC curve, we choose the match probability threshold that balances the true negative and true positive (equivalently, the false positive and false negative) rates on the training data.</p>
<p>Once we have a trained model, we use it to predict the match probability and classify a new cartridge case pair.
However, rather than referring to the number returned by the trained model as a “probability,” which implicitly assumes a homogeneous source population between the training and test cartridge cases, we simply call the number a “score” where larger values correspond with more similar cartridge cases.
We compute this score for the pairwise comparisons in the test data as a means of comparing the generalizability of the various models.
The following section details the results of this cross-validation training/testing procedure.</p>
</div>
</div>
<div id="results" class="section level2 hasAnchor" number="4.4">
<h2><span class="header-section-number">4.4</span> Results<a href="automatic-matching-of-cartridge-case-impressions.html#results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="roc-curves" class="section level3 hasAnchor" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> ROC Curves<a href="automatic-matching-of-cartridge-case-impressions.html#roc-curves" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First, we consider results from the training procedure.
<a href="automatic-matching-of-cartridge-case-impressions.html#fig:rocPlot">4.6</a> shows the resulting ROC curves for four classifier models trained on the training data set.
We consider training the logistic regression (LR) and random forest (RF) models under two feature sets: a subset of the full ACES feature set consisting of the Cluster Indicator feature <span class="math inline">\(C_0\)</span> and the six registration-based features summarized in Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:registrationFeatures-html">4.1</a> vs. all 19 ACES features.
We consider the “<span class="math inline">\(C_0\)</span> + Registration” subset of features to represent the features used in Congruent Matching Cells methods [<span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span>; zhang_convergence_2021].</p>
<p>The ROC curves allow us to visually compare the behavior of these four classifiers under various score thresholds where
curves closer to the top-left corner are preferred.
Broadly speaking, the four models perform comparably as evidenced by the similar curves on the right side of <a href="automatic-matching-of-cartridge-case-impressions.html#fig:rocPlot">4.6</a>.
The left side shows a zoomed-in version of the top left corner of plot, which makes it easier to compare the different curves.
Visually, we see that the choice of feature group has a larger impact on the outcome classification behavior than the choice between the logistic regression or random forest models.</p>
<p>To numerically compare the four models, we compute the area under the ROC curve (AUC) as well as the score threshold (Thresh.) that balances the false negative and false positive rates (the equal error rate or EER).
The AUC for the All ACES logistic regression and random forest classifiers are higher than the AUC of the two classifiers trained on the <span class="math inline">\(C_0\)</span> + Registration feature set.
Each model has a different score threshold that yields the equal error rate, which we visualize as points along the four ROC curves in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:rocPlot">4.6</a>.
We use these thresholds to compute both the training and test classification results summarized below.
We see that the All ACES, logistic regression model has the lowest equal error rate out of the four models with the All ACES, random forest model a close second.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocPlot"></span>
<img src="images/chapter4-images/rocPlot.png" alt="ROC curves for logistic regression (LR) and random forest (RF) models trained using two feature sets - all 19 ACES features vs. a subset of seven ACES features. On the left, we zoom into the top-left corner of the ROC curve plot to better distinguish between the four curves. We see that the models trained on the full ACES feature set have higher area under the curve (AUC) and lower equal error rate (EER) values than on the subset. We also show the score classification cutoffs (Thresh.) used for each of the four models to achieve the equal error rate values." width="\textwidth" />
<p class="caption">
Figure 4.6: ROC curves for logistic regression (LR) and random forest (RF) models trained using two feature sets - all 19 ACES features vs. a subset of seven ACES features. On the left, we zoom into the top-left corner of the ROC curve plot to better distinguish between the four curves. We see that the models trained on the full ACES feature set have higher area under the curve (AUC) and lower equal error rate (EER) values than on the subset. We also show the score classification cutoffs (Thresh.) used for each of the four models to achieve the equal error rate values.
</p>
</div>
</div>
<div id="optimized-model-comparison" class="section level3 hasAnchor" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Optimized Model Comparison<a href="automatic-matching-of-cartridge-case-impressions.html#optimized-model-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a> summarizes the training and testing accuracy, true negative and true positive rates for five binary classifiers.
We distinguish between the training and testing results using gray and black points/line segments, respectively, which allows us to assess the generalizability of the various models.
The conclusions drawn from Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a> are intended to primarily be qualitative and comparative across models.
Table <a href="appendix.html#tab:trainDataResults-html">4.6</a> and Table <a href="appendix.html#tab:testDataResults-html">4.7</a> in the Appendix provide a numerical summary of these results.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:trainTestAccuracy"></span>
<img src="images/chapter4-images/classifResultsPlt_trainTest.png" alt="We summarize classification accuracy, true negative, and true positive rates for both the training and testing results, represented as gray and black points/lines respectively, for five binary classifier models. Our primary interest is the test data results, but visualizing the training data results allows us to assess the generalizability of the models after training. In the first row, we consider a classifier based on a single feature, the Cluster Indicator feature $C_0$, as a baseline. The remaining rows show results from training/testing classifiers based on a random forest (RF) and logistic regression (LR) under various feature sets and optimization critieria. The second row shows results based on a subset of seven features from the ACES feature set while the third row shows results using all 19 ACES features." width="\textwidth" />
<p class="caption">
Figure 4.7: We summarize classification accuracy, true negative, and true positive rates for both the training and testing results, represented as gray and black points/lines respectively, for five binary classifier models. Our primary interest is the test data results, but visualizing the training data results allows us to assess the generalizability of the models after training. In the first row, we consider a classifier based on a single feature, the Cluster Indicator feature <span class="math inline">\(C_0\)</span>, as a baseline. The remaining rows show results from training/testing classifiers based on a random forest (RF) and logistic regression (LR) under various feature sets and optimization critieria. The second row shows results based on a subset of seven features from the ACES feature set while the third row shows results using all 19 ACES features.
</p>
</div>
<p>We first compare the training and testing results across the five models and three columns in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a>.
In general, the true negative rates based on the test data are slightly lower than those of the training data indicating that the models’ ability to distinguish between non-matching comparisons generalizes well to the testing data.
In contrast, the true positive rates tend to be lower for the test data compared to the training data across the various models, which indicates a potential difference between the training and testing data.
As we discuss below, there is a single firearm among the 15 test firearms that contributes the majority of false negative (misclassified match) test classifications.
Despite lower true positive rates, the overall accuracy between the training and testing sets are comparable due to the large class imbalance between matching and non-matching comparisons in both.</p>
<p>In the first row, we consider a baseline classifier based solely on the Cluster Indicator feature <span class="math inline">\(C_0\)</span>.
Namely, if the DBSCAN algorithm finds clusters in the cell-based translations from both directions of a cartridge case comparison, then that pair is classified as a match.
This is analogous to the classification rule used in <span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span>.
We optimized this <span class="math inline">\(C_0\)</span>-based classifier by choosing the DBSCAN parameters <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span> that resulted in the most balanced training true negative and true positive rates, resulting in <span class="math inline">\(\epsilon = 15\)</span> and <span class="math inline">\(minPts = 8\)</span>.
The optimized <span class="math inline">\(C_0\)</span>-based classifier performs considerably worse across the three measures compared to the other models with test accuracy 89.44%, true negative rate 89.64%, and true positive rate 86.82%.</p>
<p>The second row of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a> summarizes results from training the two classifier models on a subset of the full ACES feature set consisting of the Cluster Indicator feature <span class="math inline">\(C_0\)</span> and the six registration-based features summarized in Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:registrationFeatures-html">4.1</a>.
We consider this subset of features to represent the features used in Congruent Matching Cells methods <span class="citation">(<a href="#ref-song_proposed_2013" role="doc-biblioref">Song 2013</a>; <a href="#ref-zhang_convergence_2021" role="doc-biblioref">Zhang et al. 2021</a>)</span>.
In general, we see that the logistic regression (LR) and random forest (RF) models perform comparable to each other in accuracy, true negative, and true positive rates.
Despite the fact that the models in the second and third rows were selected based on balancing the training true negative and true positive rates, we note that these rates for the test data are not as well-balanced; namely, the true negative rates still tend to be larger than the true positive rates.
Below, we explore this discrepancy by analyzing the contribution of various test firearms towards the true positive rates.</p>
<p>The third row of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a> summarizes the classification results based on using all 19 ACES features.
If we compare the “<span class="math inline">\(C_0\)</span> + Registration”-trained models in the second vs. the “All ACES”-trained models in the third row, we see that the addition of the other ACES features leads to improved test true negative and true positive rates (and consequently overall accuracy) with the most noticeable gains observed in the true positive rate.
Across all five models, the All ACES-trained logistic regression model has the largest overall test accuracy and true positive rates of 97.68% and 95.94%, respectively.
The All ACES-trained random forest model has the largest overall true negative rate of 97.87%, although the All ACES, logistic regression model is a close second at 97.81% (see Table <a href="appendix.html#tab:testDataResults-html">4.7</a> for more details).</p>
</div>
<div id="similarity-score-investigation" class="section level3 hasAnchor" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Similarity Score Investigation<a href="automatic-matching-of-cartridge-case-impressions.html#similarity-score-investigation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While it’s useful to consider the accuracy, true negative, and true positive rates to compare various models, forensic examiners would likely not use the binary classification returned by a model in casework.
Instead, they would consider the match probability predicted by the model as a similarity score and incorporate it into their decision-making process.
As such, we also consider the distribution of the predicted similarity scores for matching and non-matching comparisons.
Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:testProbs">4.8</a> shows a dot plot of the predicted similarity scores for the 41,769 non-match and 3,181 match comparisons in the test set.
Specifically, these probabilities are predicted by the logistic regression model selected to maximize the AUC based on the full ACES feature set.
As we expect, few non-match comparisons have large similarity scores, which justifies the low false positive rate observed in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a>.
However, there is a surprising number of matching comparisons that also have a low match probability.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:testProbs"></span>
<img src="images/chapter4-images/testProbs_plt.png" alt="A dot plot of the predicted similarity scores for the non-match and match comparisons in the test set based on a logistic regression model. As we expect, the non-match comparisons tend to have a low match probability. However, we see that there are many matching comparisons that also have a low match probability." width="\textwidth" />
<p class="caption">
Figure 4.8: A dot plot of the predicted similarity scores for the non-match and match comparisons in the test set based on a logistic regression model. As we expect, the non-match comparisons tend to have a low match probability. However, we see that there are many matching comparisons that also have a low match probability.
</p>
</div>
<p>To explain the matching comparisons with low similarity scores, we visualize in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:testProbs-byFirearm">4.9</a> the predicted similarity scores for matching test comparisons distinguished by the 15 test firearm ID.
We see that the firearm T has far more matching comparisons with low similarity scores compared to the other 14 test firearms.
This is further underscored by the right side of the Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:testProbs-byFirearm">4.9</a>, which shows the ratio of misclassifications to total comparisons for every pair of test firearms based on the same logistic regression model used in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:testProbs">4.8</a>.
The main diagonal shows the false negative misclassifications while the off-diagonal shows the false positives.
We use blank tiles for comparisons where 0 misclassifications occurred.
We see that the false negative rate for firearm T of 27.1% is far greater than that of other firearm pairs.
The 95 false negative firearm T comparisons comprise 76% of all 125 false negative test comparisons and about 3% of all 3,181 matching test comparisons.
In sum, the model performs distinctly worse at identifying matching comparisons from firearm T compared to the other firearms, which partially explains the lower test true positive rates noted in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a>.
Upon visual inspection of the scans from firearm T, we noted a lack of consistent markings on their surfaces, which isn’t the case for scans from other test firearms.</p>
<!-- (Left) A dot plot of the predicted similarity scores for the match comparisons in the tet set based on a logistic regression model, separated by firearm. We see that firearm T has more matching comparisons with low similarity scores than other test fierarms. (Right) Misclassifications divided by total number of pairwise comparisons for each pair of test firearms based on the same logistic regression model, We do not show comparisons with 0 misclassifications. We note that the proportion of misclassified matching comparisons of firearm T of 21.7\\% is much higher than that of other comparisons. -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:testProbs-byFirearm"></span>
<img src="images/chapter4-images/misclassifPlt.png" alt="(Left) A dot plot of the predicted similarity scores for the match comparisons in the test set based on a logistic regression model, separated by firearm. We see that firearm T has more matching comparisons with low similarity scores than the other test firearms. (Right) Misclassifications divided by total number of pairwise comparisons for each pair of test firearms based on the same logistic regression model. We do not show comparisons with 0 misclassifications. We note that the proportion of misclassified matching comparisons from firearm T of 27.1 percent is much higher than that of other comparisons." width="\textwidth" />
<p class="caption">
Figure 4.9: (Left) A dot plot of the predicted similarity scores for the match comparisons in the test set based on a logistic regression model, separated by firearm. We see that firearm T has more matching comparisons with low similarity scores than the other test firearms. (Right) Misclassifications divided by total number of pairwise comparisons for each pair of test firearms based on the same logistic regression model. We do not show comparisons with 0 misclassifications. We note that the proportion of misclassified matching comparisons from firearm T of 27.1 percent is much higher than that of other comparisons.
</p>
</div>
</div>
<div id="feature-importance" class="section level3 hasAnchor" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Feature Importance<a href="automatic-matching-of-cartridge-case-impressions.html#feature-importance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, we consider the relative importance of the 19 ACES features by fitting 10 replicate random forests using the full ACES feature set with fixed random seeds.
For each replicate, we measure a variable’s importance using the Gini Index, which measures the probability of making a misclassification for a given model <span class="citation">(<a href="#ref-hastie_elements_2008" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>.
A larger decrease in the Gini Index corresponds with higher importance.
Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:rfVarImpPlt">4.10</a> shows the distribution of the mean Gini Index decrease for the 19 ACES features.
Noting the log scale on which these points are plotted, we see that the most important features consist of a combination of density-based features <span class="math inline">\(C_0\)</span>, <span class="math inline">\(C\)</span>, and <span class="math inline">\(\Delta_{\text{trans}}\)</span> and registration-based correlation features <span class="math inline">\(\overline{\text{cor}}_{\text{cell}}\)</span> and <span class="math inline">\(\text{cor}_{\text{full}}\)</span>.
In general, the visual diagnostic features tend to have lower importance scores compared the registration and density-based features.
We discuss the sensitivity of these importance scores to various algorithm parameter choices in the next section.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rfVarImpPlt"></span>
<img src="images/chapter4-images/varImpPlt.png" alt="Variable importance measures from fitting a random forest to the training data set, repeated 10 times under various random seeds. The top features consist of  density-based features $C$ and $C_0$ and registration-based features $\overline{\text{cor}}_{\text{cell}}$ and $\text{cor}_{\text{full}}$. We plot points on a log scale and vertically jitter them for visibility." width="\textwidth" />
<p class="caption">
Figure 4.10: Variable importance measures from fitting a random forest to the training data set, repeated 10 times under various random seeds. The top features consist of density-based features <span class="math inline">\(C\)</span> and <span class="math inline">\(C_0\)</span> and registration-based features <span class="math inline">\(\overline{\text{cor}}_{\text{cell}}\)</span> and <span class="math inline">\(\text{cor}_{\text{full}}\)</span>. We plot points on a log scale and vertically jitter them for visibility.
</p>
</div>
</div>
</div>
<div id="discussion-2" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Discussion<a href="automatic-matching-of-cartridge-case-impressions.html#discussion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="comparison-to-cmc-methodology" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Comparison to CMC Methodology<a href="automatic-matching-of-cartridge-case-impressions.html#comparison-to-cmc-methodology" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use a <span class="math inline">\(C_0\)</span>-based classifier as a baseline because it is analogous to the classification rule proposed in <span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span>.
Similarly, the cell-based registration features are based on the the same cell-based comparison procedure used in <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> and summarized in cell-based comparison.
Together, we consider <span class="math inline">\(C_0\)</span> and the registration-based features a fusion of previously proposed cartridge case similarity scoring algorithms.
This is why we fit separate classifiers based on these features for the training and testing results shown in Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:testProbs-byFirearm">4.9</a>.
Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:previousWorkComparison-html">4.4</a> summarizes the similarities between the ACES algorithm and the algorithms proposed in <span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span> and <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span>.
Another key difference between ACES and both of the previous algorithms is the training/testing procedure used to optimize and validate model parameters.</p>
<table>
<caption>
<span id="tab:previousWorkComparison-html">Table 4.4: </span>Comparison of the ACES algorithm to previous work. Although ACES shares similarities to previously-proposed algorithms, it includes additional nuance by computing features across both comparison directions and using these features in a classifier model.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Paper
</th>
<th style="text-align:left;">
Translation <span class="math inline">\(T_x, T_y\)</span> (in pixels)
</th>
<th style="text-align:left;">
Rotation <span class="math inline">\(\theta\)</span> (in degrees)
</th>
<th style="text-align:left;">
<span class="math inline">\(CCF_{\max}\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Song (2013)
</td>
<td style="text-align:left;">
Use the cell-based comparison algorithm to estimate cell-based registrations
</td>
<td style="text-align:left;">
Call cells Congruent Matching Cells if their registrations are close to a reference value. Classify a cartridge case pair as a match if the CMC count is at least 6.
</td>
<td style="text-align:left;">
Compute six summative features based on full-scan and cell registrations. Use features in a classifier model.
</td>
</tr>
<tr>
<td style="text-align:left;">
Zhang et al. (2021)
</td>
<td style="text-align:left;">
Use DBSCAN algorithm to identify cells that reach a consensus registration.
</td>
<td style="text-align:left;">
Classify a cartridge case pair as a match if a DBSCAN cluster is identified.
</td>
<td style="text-align:left;">
Compute four numerical features based on DBSCAN clusters across both comparison directions. Use features in a classifier model.
</td>
</tr>
</tbody>
</table>
<p>Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:classifMethodComparison-html">4.5</a> shows the test classification error rates of the Congruent Matching Cells (CMC) algorithm proposed in <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span>, the <span class="math inline">\(C_0\)</span>-based classifier like the one proposed in <span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span>, and the two ACES logistic regression models selected to balance the true negative and true positive rates and maximize the classification accuracy.
We obtained the CMC results by applying the implementation available in the cmcR R package <span class="citation">(<a href="#ref-cmcR" role="doc-biblioref">Joe Zemmels, Hofmann, and VanderPlas 2022</a>)</span> on the same test data set used in the Results section.
We used the optimization procedure described in <span class="citation">Joseph Zemmels, VanderPlas, and Hofmann (<a href="#ref-Zemmels2023" role="doc-biblioref">2023</a>)</span> to select CMC parameters.
The <span class="math inline">\(C_0\)</span>-based error rates are the same as those shown in the first row of Figure <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a>.
The ACES logistic regression models perform better than the other classifiers on this test data set, most notably when compared to the CMC method in identifying matching cartridge case pairs.
Interestingly, the <span class="math inline">\(C_0\)</span>-based classifier has a lower false negative error rate compared to the All ACES-trained, maximum accuracy logistic regression model, although it has a much higher false positive rate.</p>
<table>
<caption>
<span id="tab:classifMethodComparison-html">Table 4.5: </span>Testing classification error, false positive, and false negative rates for four types of classifier models. The CMC method results are derived from the implementation available in cmcR R package. The “Only <span class="math inline">\(C_0\)</span> feature” classifier is analogous to the classification rule used in Zhang et al. (2021). The last row shows results from the Logistic Regression classifier trained on the all 19 ACES features.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Original Paper
</th>
<th style="text-align:right;">
Error
</th>
<th style="text-align:right;">
False Positive (%)
</th>
<th style="text-align:right;">
False Negative (%)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
CMC Method
</td>
<td style="text-align:right;">
3.9
</td>
<td style="text-align:right;">
2.3
</td>
<td style="text-align:right;">
25.8
</td>
</tr>
<tr>
<td style="text-align:left;">
Only <span class="math inline">\(C_0\)</span> feature
</td>
<td style="text-align:right;">
9.5
</td>
<td style="text-align:right;">
9.6
</td>
<td style="text-align:right;">
9.2
</td>
</tr>
<tr>
<td style="text-align:left;">
ACES LR
</td>
<td style="text-align:right;">
2.3
</td>
<td style="text-align:right;">
2.2
</td>
<td style="text-align:right;">
3.8
</td>
</tr>
</tbody>
</table>
<p>Both the registration and density-based features aim to measure similarities between two cartridge case surfaces.
These features embody the notion assumed in the CMC methodology that matching cartridge cases should have similar markings, so their cell-based correlations should be large and estimated registrations should agree.
However, <a href="automatic-matching-of-cartridge-case-impressions.html#fig:cellGridExample">4.4</a> demonstrates that even non-matching cartridge case pairs may share similar markings.
We are bound to find similarities if that is all we look for, so it is worthwhile to also consider dissimilarities.
The visual diagnostic features accomplish this by partitioning scans into similar and different regions.
The similarities vs. differences ratio and labeled neighborhood size features measure how extreme the differences are between two scans while the differences correlation features determine whether there are similarities among the different regions.</p>
<p>This direct comparison of the surface values aligns with the Theory of Identification which says that an examination should involve the comparison of the “relative height or depth, width, curvature and spatial relationship” of cartridge case impressions <span class="citation">(<a href="#ref-AFTE1992" role="doc-biblioref">AFTE Criteria for Identification Committee 1992</a>)</span>.
Comparison algorithms like ACES will inevitably be used to augment the opinion of a forensic examiner, who may need to present algorithmic results to judges or juries as part of their expert testimony.
As such, it is important that forensic examiners are able to interpret and explain the results of a comparison algorithm.
The visual diagnostic features are useful for explaining the behavior of the algorithm in a manner that aligns with more traditional identification theory.</p>
</div>
<div id="sensitivity-to-parameter-choice" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Sensitivity to Parameter Choice<a href="automatic-matching-of-cartridge-case-impressions.html#sensitivity-to-parameter-choice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When selecting the optimal models presented in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a>, we performed a good deal of searching across various parameter choices.
For example, given the relative importance of the density-based features illustrated in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:rfVarImpPlt">4.10</a>, we were interested in assessing the sensitivity of the various classifier models to DBSCAN parameter choice.
<a href="automatic-matching-of-cartridge-case-impressions.html#fig:aucSensitivity">4.11</a> shows a heat map of AUC values for the four combinations of feature group and classifier model shown in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a> across a grid of DBSCAN parameter values <span class="math inline">\(\epsilon \in \{3,...,15\}\)</span> and <span class="math inline">\(minPts \in \{3,...,10\}\)</span>.
Darker tiles correspond with higher AUC values, which in turn are associated with more preferred models.
We draw a black square around the specific <span class="math inline">\((\epsilon, minPts)\)</span> values resulting in the maximum AUC for each of the models (which we also show in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:rocPlot">4.6</a>.
Interestingly, we see that all four models achieve optimum AUC for smaller values of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span>.
Larger values of <span class="math inline">\(\epsilon\)</span> will naturally lead to larger clusters as the <span class="math inline">\(\epsilon\)</span>-neighborhood around each point grows.
It makes sense then for <span class="math inline">\(\epsilon\)</span> to remain small so as to avoid the formation of false positive clusters.
Conversely, larger values of <span class="math inline">\(minPts\)</span> will naturally lead to fewer clusters, albeit of larger size.
The fact that the optimal <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span> are both relatively small suggests that matching comparisons may not have many cells that “agree” on a registration, but the cells that do agree form a strong consensus (i.e., form tight clusters).</p>
<p>We also note the variability in the AUC values across the DBSCAN parameter space.
Specifically, we see that the “<span class="math inline">\(C_0\)</span> + Registration” models achieve the highest AUCs along a set of values in the bottom-left corner - where <span class="math inline">\(\epsilon \approx minPts\)</span> for <span class="math inline">\(\epsilon,minPts &lt; 10\)</span>.
In our experimentation, we noticed that these <span class="math inline">\(\epsilon, minPts\)</span> values are also where the cluster indicator <span class="math inline">\(C_0\)</span> feature has highly-ranked importance (as is the case in ).
Both the AUC values and the variable importance of <span class="math inline">\(C_0\)</span> decrease as either <span class="math inline">\(\epsilon\)</span> or <span class="math inline">\(minPts\)</span> increase, which indicates that the <span class="math inline">\(C_0\)</span> + Registration models rely heavily on <span class="math inline">\(C_0\)</span> to distinguish between matching and non-matching comparisons.
Comparatively, we see that the AUC values for the “All ACES” models are more consistently high across the parameter space, indicating a robustness to parameter choice.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:aucSensitivity"></span>
<img src="images/chapter4-images/dbscanAUC_sensitivity.png" alt="A heat map of AUC values associated with four classifier models across a grid of values for the two DBSCAN parameter $\epsilon$ and $minPts$. Darker tiles correspond with higher AUC. The four models are a combination of two feature groups ($C_0$ + Registration vs. All ACES) and two models (Random Forest and Logistic Regression). The &quot;All ACES&quot;-trained models have higher and more consistent AUCs compared to the &quot;$C_0$ + Registration&quot;-trained models." width="\textwidth" />
<p class="caption">
Figure 4.11: A heat map of AUC values associated with four classifier models across a grid of values for the two DBSCAN parameter <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span>. Darker tiles correspond with higher AUC. The four models are a combination of two feature groups (<span class="math inline">\(C_0\)</span> + Registration vs. All ACES) and two models (Random Forest and Logistic Regression). The “All ACES”-trained models have higher and more consistent AUCs compared to the “<span class="math inline">\(C_0\)</span> + Registration”-trained models.
</p>
</div>
<p>To better understand the behavior of the AUC values in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:aucSensitivity">4.11</a>, consider <a href="automatic-matching-of-cartridge-case-impressions.html#fig:varImp-dbscan">4.12</a> showing the Mean Gini Decrease for each of the 19 ACES features across the grid of <span class="math inline">\(\epsilon \in \{3,...,15\}\)</span> and <span class="math inline">\(minPts \in \{3,...,10\}\)</span> values.
We see that the density Cluster Indicator <span class="math inline">\(C_0\)</span>, Cluster Size <span class="math inline">\(C\)</span>, and Translation Difference <span class="math inline">\(\Delta_{\text{trans}}\)</span> have high variable importance for some combinations of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span>, but are generally less consistent than the registration cell-based and full scan correlations <span class="math inline">\(\overline{cor}_{\text{cell}}\)</span> and <span class="math inline">\(cor_{\text{full}}\)</span>.
This implies that the density features can be highly informative under optimal conditions, yet quickly lose importance under sub-optimal conditions.
In our experimentation, we noticed that other ACES features “take up the mantle” when <span class="math inline">\(C_0\)</span> or <span class="math inline">\(C\)</span> have low importance, which explains the relative stability in AUC values observed in the “All ACES”-trained models shown in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:aucSensitivity">4.11</a>.</p>
<p>The relationship between <span class="math inline">\(C_0\)</span> and <span class="math inline">\(C\)</span> (first two plots in the first row) is noteworthy by the sharp boundary defined by the line <span class="math inline">\(minPts = \epsilon\)</span>.
Above this line, when <span class="math inline">\(minPts &gt; \epsilon\)</span>, we see that <span class="math inline">\(C_0\)</span> is considered more important than <span class="math inline">\(C\)</span>.
In other words, as the minimum size required to be classified as a cluster (<span class="math inline">\(minPts\)</span>) and the neighborhood radius (<span class="math inline">\(\epsilon\)</span>) both become stricter, there will naturally be fewer clusters formed.
In these instances, knowing whether a cluster is formed is more informative than the size of that cluster.
However, the importance of <span class="math inline">\(C_0\)</span> below the line, when <span class="math inline">\(minPts &lt; \epsilon\)</span>, is seemingly replaced by <span class="math inline">\(C\)</span> rising in importance.
That is, when <span class="math inline">\(minPts\)</span> and <span class="math inline">\(\epsilon\)</span> are less strict, then more clusters will form and the actual size of the cluster becomes more informative than the fact that it exists.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varImp-dbscan"></span>
<img src="images/chapter4-images/varImp-dbscan.png" alt="A heat map of variable importance measures for the 19 features in the ACES data set across a grid of values for the two DBSCAN parameter $\epsilon$ and $minPts$. Darker tiles correspond with higher importance. As in \autoref{fig:rfVarImpPlt}, we visualze the importance measures on a log-transformed color scale to more clearly see variability among smaller values. We see that features like the Cell-Based Cluster Indicator $C_0$ and Cell-Based Cluster Size $C$ have inconsistent importance measures across the difference values of $\epsilon$ and $minPts$ while the cell-based and full scan correlation features $\overline{cor}_{\text{cell}}$ and $cor_{\text{full}}$ have more consistent importance." width="\textwidth" />
<p class="caption">
Figure 4.12: A heat map of variable importance measures for the 19 features in the ACES data set across a grid of values for the two DBSCAN parameter <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span>. Darker tiles correspond with higher importance. As in , we visualze the importance measures on a log-transformed color scale to more clearly see variability among smaller values. We see that features like the Cell-Based Cluster Indicator <span class="math inline">\(C_0\)</span> and Cell-Based Cluster Size <span class="math inline">\(C\)</span> have inconsistent importance measures across the difference values of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span> while the cell-based and full scan correlation features <span class="math inline">\(\overline{cor}_{\text{cell}}\)</span> and <span class="math inline">\(cor_{\text{full}}\)</span> have more consistent importance.
</p>
</div>
<p>The <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(minPts\)</span> values are not the only parameters that can be tuned.
We also computed the ACES features using two different cell grids: <span class="math inline">\(4 \times 4\)</span> and <span class="math inline">\(8 \times 8\)</span>.
Each cell in the <span class="math inline">\(4 \times 4\)</span> grid captures a larger portion of the cartridge case’s surface compared to the <span class="math inline">\(8 \times 8\)</span> grid, which would presumably be useful for the visual diagnostic-based features.
However, the <span class="math inline">\(8 \times 8\)</span> grid has the benefit of having more cells with which to measure consensus using the registration and density-based features.
Our experimentation showed that the <span class="math inline">\(4 \times 4\)</span> cell grid features resulted in categorically better classification results compared to the <span class="math inline">\(8 \times 8\)</span> features.
Throughout this paper, we present the <span class="math inline">\(4 \times 4\)</span> results.</p>
<p>One would expect that having more cells would make it easier to measure the consensus.
However, even the density-based features, such as the cluster size feature <span class="math inline">\(C\)</span>, had better separation between matching and non-matching comparisons when the <span class="math inline">\(4 \times 4\)</span> cell grid was used compared to the <span class="math inline">\(8 \times 8\)</span> grid.
Further, performing classification using a combination of the <span class="math inline">\(4 \times 4\)</span> and <span class="math inline">\(8 \times 8\)</span> features actually led to <em>lower</em> overall test accuracy compared to just using the <span class="math inline">\(4 \times 4\)</span> grid.
We chalk this outcome up to the specific cell-based registration procedure we used.
Recall from image registration algorithm that we first perform a pre-registration using the full scans and a rotation grid <span class="math inline">\(\pmb{\Theta}\)</span>.
Using the full scan-estimated rotation <span class="math inline">\(\theta_d^*\)</span>, we then perform the cell-based comparison procedure using a limited rotation grid <span class="math inline">\(\pmb{\Theta}_d&#39; = \{\theta_d^* - 2^\circ, \theta_d^* - 1^\circ,...,\theta_d^* + 2^\circ\}\)</span>.</p>
<p>To save on computational time, rather than comparing each source cell to the full target scan, we compare it to a slightly larger region that is located in the same position in the target scan (<span class="math inline">\(1.1 \times\)</span> the side length of the source cell).
Assuming the full scan registration was successful, a source cell isn’t allowed to “move” very far in this region to register.
This in contrast to, for example, the original CMC methodology proposed in <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> where each source cell is compared to a region in the target scan that is much larger, 2 or 3 times the side length.
The consequence of our implementation is that cells, even for non-matching comparisons, tend to have registration values close to the origin (i.e., no movement), and are therefore more likely to form DBSCAN clusters compared to if we used larger target regions.
In this case, a higher number of cells actually leads to a higher chance of forming “false positive” clusters for a non-match comparison, which is exactly what we observe from the <span class="math inline">\(8 \times 8\)</span> comparisons.
The formation of false positive clusters is far rarer in the <span class="math inline">\(4 \times 4\)</span> cell grid case.
We hypothesize that the <span class="math inline">\(8 \times 8\)</span> cell grid results could improve if a different target region size were used.
However, our implementation makes sense pragmatically, since the CCF computation grows exponentially with the size of the region cell or target region, and conceptually, since the full scan registration should result in a rough alignment of two matching scans before applying the cell-based comparison with a finer rotation grid.</p>
</div>
<div id="model-selection-considerations" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Model Selection Considerations<a href="automatic-matching-of-cartridge-case-impressions.html#model-selection-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our intention in fitting the logistic regression and random forest classification models using different feature sets is to explore each model’s strengths and weaknesses.
A critical step in putting the ACES algorithm into practice will be to settle on a single model.
Pragmatically, it seems reasonable to choose the model with the highest estimated accuracy on available test data.
However, we noted that models trained by this optimization criterion on imbalanced data tend to over-classify the majority class.
This is the case for the CMC method results we summarized in Table <a href="automatic-matching-of-cartridge-case-impressions.html#tab:classifMethodComparison-html">4.5</a>, but is also true for ACES statistical models trained to maximize overall accuracy.
For example, if we were to shift the similarity score classification threshold for the All ACES logistic regression model to maximize the overall accuracy on the training data, the resulting score threshold is 0.54 with test accuracy, true negative, and true positive rates of 99.4%, 99.9%, and 92.4%.
Given the large true negative rate, we might favor this model from an ethical perspective since misclassifying a truly non-matching cartridge case pair may incriminate an innocent individual.
However, the true positive rate is considerably lower than the “balanced” results summarized in <a href="automatic-matching-of-cartridge-case-impressions.html#fig:trainTestAccuracy">4.7</a>.
Further exploration of different optimization criteria is warranted.</p>
<p>Another aspect to consider when choosing a model is interpretability and explainability.
If an algorithm is applied in forensic casework, then evidentiary conclusions derived from the algorithm’s output will inevitably be presented to a non-expert judge or jury.
More interpretable models are easier to understand, and therefore should be preferred.
The classification behavior of the logistic regression and classification tree models are arguably easier to explain than the random forest model.
For example, the logistic regression model parameters can be understood in terms of the estimated increase in odds of a match.
Paired with its comparable performance to the random forest, we propose the logistic regression model with all 19 ACES features as the preferred model that balances interpretability with accuracy.</p>
</div>
</div>
<div id="conclusion-2" class="section level2 hasAnchor" number="4.6">
<h2><span class="header-section-number">4.6</span> Conclusion<a href="automatic-matching-of-cartridge-case-impressions.html#conclusion-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this paper, we introduced the Automatic Cartridge Evidence Scoring (ACES) algorithm to measure the similarity between two fired cartridge cases based on their breech face impressions.
In particular, we defined a set of 19 similarity features and used these features to train and test classifier models.
We validated our algorithm on a set of 510 cartridge cases - the largest validation study of a cartridge case similarity scoring algorithm to-date.
Compared to predominant algorithms like the CMC algorithm described in <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span>, the ACES logistic regression model achieves higher test accuracy rates while having more balanced true positive and true negative rates.
We propose a logistic regression classifier trained on the ACES feature set as a new benchmark to which future scoring methods are compared.</p>
<p>Before the ACES algorithm can be put into practice, we must devise new stress-tests, using new ammunition and firearm combinations, to assess its robustness.
There is also an opportunity to optimize additional parameters, such as the number of cells used in cell-based comparison or parameters used in pre-processing, to measure their effects on final results.
A variety of factors, such as make/model and wear of the evidence or the algorithm parameters used, affect the discriminative power of the 19 features defined in this paper.
We view the current version of the ACES algorithm as more a foundation for future improvements than a final solution.
We expect the ACES feature set to evolve over time; for discriminatory features to replace less informative features.
Given the gravity of the application, we stress interpretability as a guiding principle for future feature engineering and model selection.
A misunderstood feature or result may lead a lay judge or juror to an incorrect conclusion.
Additionally, we urge future researchers to use a train/test procedure similar to the one outlined in this paper to validate proposed methods.</p>
<p>We developed the <a href="https://jzemmels.github.io/scored/">scored</a> R package as an open-source companion to this paper.
The code and data used in this paper are available at <a href="https://github.com/jzemmels/jdssvSubmission" class="uri">https://github.com/jzemmels/jdssvSubmission</a>.</p>
<div style="page-break-after: always;"></div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-AFTE1992" class="csl-entry">
AFTE Criteria for Identification Committee. 1992. <span>“Theory of Identification, Range Striae Comparison Reports and Modified Glossary Definitions.”</span> <em>AFTE Journal</em> 24 (3): 336–40.
</div>
<div id="ref-Baldwin2014" class="csl-entry">
Baldwin, David P., Stanley J. Bajic, Max Morris, and Daniel Zamzow. 2014. <span>“A Study of False-Positive and False-Negative Error Rates in Cartridge Case Comparisons.”</span> Defense Technical Information Center. <a href="https://doi.org/10.21236/ada611807">https://doi.org/10.21236/ada611807</a>.
</div>
<div id="ref-breiman" class="csl-entry">
Breiman, Leo. 2001. <span>“<span>Random Forests</span>.”</span> <em>Machine Learning</em> 45 (1): 5–32. <a href="https://doi.org/10.1023/a:1010933404324">https://doi.org/10.1023/a:1010933404324</a>.
</div>
<div id="ref-Brown1992" class="csl-entry">
Brown, Lisa Gottesfeld. 1992. <span>“A Survey of Image Registration Techniques.”</span> <em><span>ACM</span> Computing Surveys</em> 24 (4): 325–76. <a href="https://doi.org/10.1145/146370.146374">https://doi.org/10.1145/146370.146374</a>.
</div>
<div id="ref-chen_convergence_2017" class="csl-entry">
Chen, Zhe, John Song, Wei Chu, Johannes A. Soons, and Xuezeng Zhao. 2017. <span>“A Convergence Algorithm for Correlation of Breech Face Images Based on the Congruent Matching Cells (<span>CMC</span>) Method.”</span> <em>Forensic Science International</em> 280 (November): 213–23. <a href="https://doi.org/10.1016/j.forsciint.2017.08.033">https://doi.org/10.1016/j.forsciint.2017.08.033</a>.
</div>
<div id="ref-Ester1996" class="csl-entry">
Ester, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. <span>“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.”</span> In <em>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</em>, 226–31. KDD’96. Portland, Oregon: AAAI Press.
</div>
<div id="ref-Fernndez2018" class="csl-entry">
Fernández, Alberto, Salvador Garcı́a, Mikel Galar, Ronaldo C. Prati, Bartosz Krawczyk, and Francisco Herrera. 2018. <em>Learning from Imbalanced Data Sets</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-98074-4">https://doi.org/10.1007/978-3-319-98074-4</a>.
</div>
<div id="ref-ISO25178-72" class="csl-entry">
<span>“<span class="nocase">Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p</span>.”</span> 2017. Standard. Vol. 2014. Geneva, CH: International Organization for Standardization. <a href="https://www.iso.org/standard/62310.html">https://www.iso.org/standard/62310.html</a>.
</div>
<div id="ref-Haralick1987" class="csl-entry">
Haralick, Robert M., Stanley R. Sternberg, and Xinhua Zhuang. 1987. <span>“Image Analysis Using Mathematical Morphology.”</span> <em><span>IEEE</span> Transactions on Pattern Analysis and Machine Intelligence</em> <span>PAMI</span>-9 (4): 532–50. <a href="https://doi.org/10.1109/tpami.1987.4767941">https://doi.org/10.1109/tpami.1987.4767941</a>.
</div>
<div id="ref-hare_automatic_2016" class="csl-entry">
Hare, Eric, Heike Hofmann, and Alicia Carriquiry. 2017. <span>“Automatic <span>Matching</span> of <span>Bullet</span> <span>Land</span> <span>Impressions</span>.”</span> <em>The Annals of Applied Statistics</em> 11 (4): 2332–56. <a href="http://arxiv.org/abs/1601.05788">http://arxiv.org/abs/1601.05788</a>.
</div>
<div id="ref-hastie_elements_2008" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer Series in Statistics. New York, NY, USA: Springer New York Inc.
</div>
<div id="ref-hesselink_concurrent_2001" class="csl-entry">
Hesselink, Wim H., Arnold Meijster, and Coenraad Bron. 2001. <span>“Concurrent Determination of Connected Components.”</span> <em>Science of Computer Programming</em> 41 (2): 173–94. <a href="https://doi.org/10.1016/S0167-6423(01)00007-7">https://doi.org/10.1016/S0167-6423(01)00007-7</a>.
</div>
<div id="ref-hofmann_inconclusives_2021" class="csl-entry">
Hofmann, Heike, Alicia Carriquiry, and Susan Vanderplas. 2021. <span>“<span class="nocase">Treatment of inconclusives in the AFTE range of conclusions</span>.”</span> <em>Law, Probability and Risk</em> 19 (3-4): 317–64. <a href="https://doi.org/10.1093/lpr/mgab002">https://doi.org/10.1093/lpr/mgab002</a>.
</div>
<div id="ref-x3ptools" class="csl-entry">
Hofmann, Heike, Susan Vanderplas, Ganesh Krishnan, and Eric Hare. 2020. <em><span class="nocase">x3ptools: Tools for Working with 3D Surface Measurements</span></em>. <a href="https://cran.r-project.org/web/packages/x3ptools/index.html">https://cran.r-project.org/web/packages/x3ptools/index.html</a>.
</div>
<div id="ref-James2013" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in r</em>. Springer. <a href="https://faculty.marshall.usc.edu/gareth-james/ISL/">https://faculty.marshall.usc.edu/gareth-james/ISL/</a>.
</div>
<div id="ref-caret" class="csl-entry">
Kuhn, Max. 2022. <em>Caret: Classification and Regression Training</em>. <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.
</div>
<div id="ref-randomForest" class="csl-entry">
Liaw, Andy, and Matthew Wiener. 2002. <span>“Classification and Regression by randomForest.”</span> <em>R News</em> 2 (3): 18–22. <a href="https://CRAN.R-project.org/doc/Rnews/">https://CRAN.R-project.org/doc/Rnews/</a>.
</div>
<div id="ref-MacQueen1967" class="csl-entry">
MacQueen, J. B. 1967. <span>“Some Methods for Classification and Analysis of MultiVariate Observations.”</span> In <em>Proc. Of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, edited by L. M. Le Cam and J. Neyman, 1:281–97. University of California Press.
</div>
<div id="ref-council_strengthening_2009" class="csl-entry">
National Research Council. 2009. <em>Strengthening <span>Forensic</span> <span>Science</span> in the <span>United</span> <span>States</span>: <span>A</span> <span>Path</span> <span>Forward</span></em>. Washington, DC: The National Academies Press. <a href="https://doi.org/10.17226/12589">https://doi.org/10.17226/12589</a>.
</div>
<div id="ref-pcast2016" class="csl-entry">
President’s Council of Advisors on Sci. &amp; Tech. 2016. <span>“Forensic Science in Criminal Courts: Ensuring Scientific Validity of Feature-Comparison Methods.”</span> <a href="https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf">https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf</a>.
</div>
<div id="ref-Rlanguage" class="csl-entry">
R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-song_proposed_2013" class="csl-entry">
Song, John. 2013. <span>“Proposed <span>‘<span>NIST</span> <span>Ballistics</span> <span>Identification</span> <span>System</span> (<span>NBIS</span>)’</span> <span>Based</span> on <span>3d</span> <span>Topography</span> <span>Measurements</span> on <span>Correlation</span> <span>Cells</span>.”</span> <em>American Firearm and Tool Mark Examiners Journal</em> 45 (2): 11. <a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868">https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868</a>.
</div>
<div id="ref-Swofford2021" class="csl-entry">
Swofford, H., and C. Champod. 2021. <span>“Implementation of Algorithms in Pattern <span>&amp;</span> Impression Evidence: A Responsible and Practical Roadmap.”</span> <em>Forensic Science International: Synergy</em> 3: 100142. <a href="https://doi.org/10.1016/j.fsisyn.2021.100142">https://doi.org/10.1016/j.fsisyn.2021.100142</a>.
</div>
<div id="ref-tai_fully_2018" class="csl-entry">
Tai, Xiao Hui, and William F. Eddy. 2018. <span>“A <span>Fully</span> <span>Automatic</span> <span>Method</span> for <span>Comparing</span> <span>Cartridge</span> <span>Case</span> <span>Images</span>,”</span> <em>Journal of Forensic Sciences</em> 63 (2): 440–48. <a href="http://doi.wiley.com/10.1111/1556-4029.13577">http://doi.wiley.com/10.1111/1556-4029.13577</a>.
</div>
<div id="ref-tong_improved_2015" class="csl-entry">
Tong, Mingsi, John Song, and Wei Chu. 2015. <span>“An <span>Improved</span> <span>Algorithm</span> of <span>Congruent</span> <span>Matching</span> <span>Cells</span> (<span>CMC</span>) <span>Method</span> for <span>Firearm</span> <span>Evidence</span> <span>Identifications</span>.”</span> <em>Journal of Research of the National Institute of Standards and Technology</em> 120 (April): 102. <a href="https://doi.org/10.6028/jres.120.008">https://doi.org/10.6028/jres.120.008</a>.
</div>
<div id="ref-MASS" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics with s</em>. Fourth. New York: Springer. <a href="http://www.stats.ox.ac.uk/pub/MASS4">http://www.stats.ox.ac.uk/pub/MASS4</a>.
</div>
<div id="ref-Vorburger2015" class="csl-entry">
Vorburger, T V, J Song, and N Petraco. 2015. <span>“Topography Measurements and Applications in Ballistics and Tool Mark Identifications.”</span> <em>Surface Topography: Metrology and Properties</em> 4 (1): 013002. <a href="https://doi.org/10.1088/2051-672x/4/1/013002">https://doi.org/10.1088/2051-672x/4/1/013002</a>.
</div>
<div id="ref-vorburger_surface_2007" class="csl-entry">
Vorburger, T V, J H Yen, B Bachrach, T B Renegar, J J Filliben, L Ma, H G Rhee, et al. 2007. <span>“Surface Topography Analysis for a Feasibility Assessment of a National Ballistics Imaging Database.”</span> NIST IR 7362. Gaithersburg, MD: National Institute of Standards; Technology. <a href="https://doi.org/10.6028/NIST.IR.7362">https://doi.org/10.6028/NIST.IR.7362</a>.
</div>
<div id="ref-xiaoHui_seminar" class="csl-entry">
Xiao Hui Tai. 2018. <span>“Comparing Cartridge Breechface Marks: 2d Versus 3d.”</span> Center for Statistics; Applications in Forensic Evidence. January 31, 2018. <a href="https://forensicstats.org/blog/portfolio/comparing-cartridge-breechface-marks-2d-versus-3d/">https://forensicstats.org/blog/portfolio/comparing-cartridge-breechface-marks-2d-versus-3d/</a>.
</div>
<div id="ref-cmcR" class="csl-entry">
Zemmels, Joe, Heike Hofmann, and Susan VanderPlas. 2022. <em>cmcR: An Implementation of the ’Congruent Matching Cells’ Method</em>.
</div>
<div id="ref-Zemmels2023" class="csl-entry">
Zemmels, Joseph, Susan VanderPlas, and Heike Hofmann. 2023. <span>“A Study in Reproducibility: The Congruent Matching Cells Algorithm and <span class="nocase">cmcR</span> Package.”</span> <em>The R Journal</em> 14 (4): 79–102. <a href="https://doi.org/10.32614/rj-2023-014">https://doi.org/10.32614/rj-2023-014</a>.
</div>
<div id="ref-zhang_convergence_2021" class="csl-entry">
Zhang, Hao, Jialing Zhu, Rongjing Hong, Hua Wang, Fuzhong Sun, and Anup Malik. 2021. <span>“Convergence-Improved Congruent Matching Cells (<span>CMC</span>) Method for Firing Pin Impression Comparison.”</span> <em>Journal of Forensic Sciences</em> 66 (2): 571–82. <a href="https://doi.org/10.1111/1556-4029.14634">https://doi.org/10.1111/1556-4029.14634</a>.
</div>
<div id="ref-Zheng2014" class="csl-entry">
Zheng, X, J Soons, T V Vorburger, J Song, T Renegar, and R Thompson. 2014. <span>“Applications of Surface Metrology in Firearm Identification.”</span> <em>Surface Topography: Metrology and Properties</em> 2 (1): 014012. <a href="https://doi.org/10.1088/2051-672x/2/1/014012">https://doi.org/10.1088/2051-672x/2/1/014012</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>If more than one cluster is identified, we binarize the points based on whether they were assigned to any cluster or if they are a noise point and proceed as if there is only one cluster. We assume that two or more clusters form only because of the course rotation grid considered. Were a finer grid used, the points would coalesce into a single cluster around the true translation value. This assumption has empirical support through our experimentation.<a href="automatic-matching-of-cartridge-case-impressions.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="computational-details-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
