<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Cartridge Case Comparison Pipeline</title>
  <meta name="description" content="A Cartridge Case Comparison Pipeline" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="A Cartridge Case Comparison Pipeline" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="jzemmels/cartridgeCaseLitReview" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Cartridge Case Comparison Pipeline" />
  
  
  

<meta name="author" content="Joseph Zemmels" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Literature Review</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preliminaries-forensic-examinations"><i class="fa fa-check"></i><b>1.1</b> Preliminaries: Forensic Examinations</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#firearm-and-toolmark-identification"><i class="fa fa-check"></i><b>1.1.1</b> Firearm and Toolmark Identification</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#why-should-firearm-and-toolmark-identification-change"><i class="fa fa-check"></i><b>1.1.2</b> Why Should Firearm and Toolmark Identification Change?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#forensic-comparison-pipelines"><i class="fa fa-check"></i><b>1.2</b> Forensic Comparison Pipelines</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#digital-representations-of-evidence"><i class="fa fa-check"></i><b>1.2.1</b> Digital Representations of Evidence</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#pre-processing-procedures-for-forensic-data"><i class="fa fa-check"></i><b>1.2.2</b> Pre-processing Procedures for Forensic Data</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#forensic-data-feature-extraction"><i class="fa fa-check"></i><b>1.2.3</b> Forensic Data Feature Extraction</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#similarity-scores-classification-rules-for-forensic-data"><i class="fa fa-check"></i><b>1.2.4</b> Similarity Scores &amp; Classification Rules for Forensic Data</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#reproducibility-of-comparison-pipelines"><i class="fa fa-check"></i><b>1.2.5</b> Reproducibility of Comparison Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#diagnostic-tools"><i class="fa fa-check"></i><b>1.3</b> Diagnostic Tools</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#visual-diagnostics"><i class="fa fa-check"></i><b>1.3.1</b> Visual Diagnostics</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#interactive-diagnostics"><i class="fa fa-check"></i><b>1.3.2</b> Interactive Diagnostics</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#automating-and-improving-the-cartridge-case-comparison-pipeline"><i class="fa fa-check"></i><b>1.4</b> Automating and Improving the Cartridge Case Comparison Pipeline</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#image-processing-techniques"><i class="fa fa-check"></i><b>1.4.1</b> Image Processing Techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#density-based-spatial-clustering-of-applications-with-noise"><i class="fa fa-check"></i><b>1.4.2</b> Density-Based Spatial Clustering of Applications with Noise</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#implementation-considerations"><i class="fa fa-check"></i><b>1.4.3</b> Implementation Considerations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><i class="fa fa-check"></i><b>2</b> A Study in Reproducibility: The Congruent Matching Cells Algorithm and cmcR package</a>
<ul>
<li class="chapter" data-level="" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="2.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#intro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#repeatability-and-reproducibility"><i class="fa fa-check"></i><b>2.1.1</b> Repeatability and reproducibility</a></li>
<li class="chapter" data-level="2.1.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#the-congruent-matching-cells-algorithm"><i class="fa fa-check"></i><b>2.1.2</b> The Congruent Matching Cells algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#cmcMethod"><i class="fa fa-check"></i><b>2.2</b> The CMC pipeline</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#initialData"><i class="fa fa-check"></i><b>2.2.1</b> Initial data</a></li>
<li class="chapter" data-level="2.2.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#preProcessing"><i class="fa fa-check"></i><b>2.2.2</b> Pre-processing procedures</a></li>
<li class="chapter" data-level="2.2.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#comparisonProcedure"><i class="fa fa-check"></i><b>2.2.3</b> “Correlation cell” comparison procedure</a></li>
<li class="chapter" data-level="2.2.4" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#decision-rule"><i class="fa fa-check"></i><b>2.2.4</b> Decision rule</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#discussion"><i class="fa fa-check"></i><b>2.3</b> Discussion</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#ambiguities"><i class="fa fa-check"></i><b>2.3.1</b> Ambiguity in algorithmic descriptions</a></li>
<li class="chapter" data-level="2.3.2" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#investigation"><i class="fa fa-check"></i><b>2.3.2</b> CMC pattern matching pipeline</a></li>
<li class="chapter" data-level="2.3.3" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#processing-condition-sensitivity"><i class="fa fa-check"></i><b>2.3.3</b> Processing condition sensitivity</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#conclusion"><i class="fa fa-check"></i><b>2.4</b> Conclusion</a></li>
<li class="chapter" data-level="2.5" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#acknowledgement"><i class="fa fa-check"></i><b>2.5</b> Acknowledgement</a></li>
<li class="chapter" data-level="2.6" data-path="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html"><a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html#computational-details"><i class="fa fa-check"></i><b>2.6</b> Computational details</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><i class="fa fa-check"></i><b>3</b> Diagnostic Tools for Cartridge Case Comparison Algorithms</a>
<ul>
<li class="chapter" data-level="" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#abstract-1"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="3.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#background-and-introduction"><i class="fa fa-check"></i><b>3.2</b> Background and Introduction</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#notational-conventions"><i class="fa fa-check"></i><b>3.2.1</b> Notational Conventions</a></li>
<li class="chapter" data-level="3.2.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#registration-procedure"><i class="fa fa-check"></i><b>3.2.2</b> Registration Procedure</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostics-1"><i class="fa fa-check"></i><b>3.3</b> Visual Diagnostics</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-x3p-plot"><i class="fa fa-check"></i><b>3.3.1</b> The X3P Plot</a></li>
<li class="chapter" data-level="3.3.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#the-comparison-plot"><i class="fa fa-check"></i><b>3.3.2</b> The Comparison Plot</a></li>
<li class="chapter" data-level="3.3.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics"><i class="fa fa-check"></i><b>3.3.3</b> Visual Diagnostic Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#statistical-learning-from-visual-diagnostics"><i class="fa fa-check"></i><b>3.4</b> Statistical Learning from Visual Diagnostics</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#visual-diagnostic-statistics-as-features"><i class="fa fa-check"></i><b>3.4.1</b> Visual Diagnostic Statistics as Features</a></li>
<li class="chapter" data-level="3.4.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#binary-classification-results"><i class="fa fa-check"></i><b>3.4.2</b> Binary Classification Results</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#discussion-1"><i class="fa fa-check"></i><b>3.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#case-studies"><i class="fa fa-check"></i><b>3.5.1</b> Case Studies</a></li>
<li class="chapter" data-level="3.5.2" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#sensitivity-to-filter-threshold"><i class="fa fa-check"></i><b>3.5.2</b> Sensitivity to Filter Threshold</a></li>
<li class="chapter" data-level="3.5.3" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#interactive-cartridgeinvestigatr-application"><i class="fa fa-check"></i><b>3.5.3</b> Interactive cartridgeInvestigatR application</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="diagnostic-tools-for-cartridge-case-comparison-algorithms.html"><a href="diagnostic-tools-for-cartridge-case-comparison-algorithms.html#conclusion-1"><i class="fa fa-check"></i><b>3.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html"><i class="fa fa-check"></i><b>4</b> Automatic Matching of Cartridge Case Impressions</a>
<ul>
<li class="chapter" data-level="" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#abstract-2"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="4.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#introduction-1"><i class="fa fa-check"></i><b>4.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#previous-work"><i class="fa fa-check"></i><b>4.1.1</b> Previous Work</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#cartridge-case-data"><i class="fa fa-check"></i><b>4.2</b> Cartridge Case Data</a></li>
<li class="chapter" data-level="4.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#methods"><i class="fa fa-check"></i><b>4.3</b> Methods</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#pre-processing"><i class="fa fa-check"></i><b>4.3.1</b> Pre-processing</a></li>
<li class="chapter" data-level="4.3.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#comparing"><i class="fa fa-check"></i><b>4.3.2</b> Comparing</a></li>
<li class="chapter" data-level="4.3.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#scoring"><i class="fa fa-check"></i><b>4.3.3</b> Scoring</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#results"><i class="fa fa-check"></i><b>4.4</b> Results</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#roc-curves"><i class="fa fa-check"></i><b>4.4.1</b> ROC Curves</a></li>
<li class="chapter" data-level="4.4.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#optimized-model-comparison"><i class="fa fa-check"></i><b>4.4.2</b> Optimized Model Comparison</a></li>
<li class="chapter" data-level="4.4.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#similarity-score-investigation"><i class="fa fa-check"></i><b>4.4.3</b> Similarity Score Investigation</a></li>
<li class="chapter" data-level="4.4.4" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#feature-importance"><i class="fa fa-check"></i><b>4.4.4</b> Feature Importance</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#discussion-2"><i class="fa fa-check"></i><b>4.5</b> Discussion</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#comparison-to-cmc-methodology"><i class="fa fa-check"></i><b>4.5.1</b> Comparison to CMC Methodology</a></li>
<li class="chapter" data-level="4.5.2" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#sensitivity-to-parameter-choice"><i class="fa fa-check"></i><b>4.5.2</b> Sensitivity to Parameter Choice</a></li>
<li class="chapter" data-level="4.5.3" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#model-selection-considerations"><i class="fa fa-check"></i><b>4.5.3</b> Model Selection Considerations</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="automatic-matching-of-cartridge-case-impressions.html"><a href="automatic-matching-of-cartridge-case-impressions.html#conclusion-2"><i class="fa fa-check"></i><b>4.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="computational-details-1.html"><a href="computational-details-1.html"><i class="fa fa-check"></i>Computational Details</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="4.7" data-path="appendix.html"><a href="appendix.html#registration-procedure-details"><i class="fa fa-check"></i><b>4.7</b> Registration Procedure Details</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="appendix.html"><a href="appendix.html#cell-based-registration-details"><i class="fa fa-check"></i><b>4.7.1</b> Cell-Based Registration Details</a></li>
<li class="chapter" data-level="4.7.2" data-path="appendix.html"><a href="appendix.html#registration-based-feature-distributions"><i class="fa fa-check"></i><b>4.7.2</b> Registration-Based Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="appendix.html"><a href="appendix.html#dbscan-algorithm-details"><i class="fa fa-check"></i><b>4.8</b> DBSCAN Algorithm Details</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="appendix.html"><a href="appendix.html#density-based-feature-distributions"><i class="fa fa-check"></i><b>4.8.1</b> Density-Based Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="appendix.html"><a href="appendix.html#visual-diagnostic-details"><i class="fa fa-check"></i><b>4.9</b> Visual Diagnostic Details</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="appendix.html"><a href="appendix.html#visual-diagnostic-feature-distributions"><i class="fa fa-check"></i><b>4.9.1</b> Visual Diagnostic Feature Distributions</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="appendix.html"><a href="appendix.html#model-specific-results"><i class="fa fa-check"></i><b>4.10</b> Model-Specific Results</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Cartridge Case Comparison Pipeline</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">A Cartridge Case Comparison Pipeline</h1>
<p class="author"><em>Joseph Zemmels</em></p>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>Forensic examinations attempt to solve the binary classification problem of whether two pieces of evidence originated from the same source. Algorithms to compare evidence are increasingly used in forensic examinations to supplement an examiner’s opinion with an objective measure of similarity. Given the gravity of the application, such algorithms must first be thoroughly tested under various conditions to identify strengths and weaknesses, which requires a good deal of experimentation. This experimentation is expedited when algorithms are intentionally developed to be accessible to the wider scientific community including fellow researchers and practitioners. In this work, we discuss an algorithm to objectively measure the similarity between impressions left on cartridge cases during the firing process. We have designed this algorithm to be approachable for researchers and practitioners alike. Chapter 2 discusses a modularization of the algorithm into a “pipeline” that enables reproducibility, experimentation, and comprehension. Our goal in this modularization is to lay a foundation adherent to the “tidy” principles of design upon which improvements can be easily developed. Chapter 3 details a suite of diagnostic tools that illuminate the inner-workings of the algorithm and aid in identifying when and why the algorithm “works” correctly. These diagnostics will be useful for both researchers interested in correcting the algorithm’s behavior and for practitioners concerned with interpreting the algorithm in case work. Chapter 4 introduces novel elements of the pipeline that we demonstrate are improvements to predominant methods. We introduce the Automatic Cartridge Evidence Scoring (ACES) algorithm that measures the similarity between two cartridge cases using a novel set of numeric features and a statistical classification model. We implement the various components of ACES in three R packages: <code>cmcR</code>, <code>impressions</code>, and <code>scored</code>, and provide a user-friendly interface for the algorithm in an interactive web application called <code>cartridgeInvestigatR</code>.</p>
</div>
</div>
<div id="literature-review" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> Literature Review<a href="index.html#literature-review" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="preliminaries-forensic-examinations" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Preliminaries: Forensic Examinations<a href="index.html#preliminaries-forensic-examinations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A bullet casing is found at the scene of a murder.
The bullet is recovered from the victim during autopsy.
A handwritten letter threatening the victim is found in their pocket.
The assailant’s shoeprints are discovered fleeing the area.
Who left this evidence?
Investigators obtain the gun, shoes, and handwriting samples of a suspect.
This evidence, along with the crime scene evidence, is sent to a forensic laboratory for analysis.
Forensic examiners compare the evidence to establish whether they share a common source.
The suspect is charged after the examiners conclude that there is sufficient agreement between the crime scene and suspect’s samples.</p>
<p>The procedure described above, in which evidence is analyzed to determine its origin, is called the <em>source identification</em> problem <span class="citation">(<a href="#ref-Ommen2018" role="doc-biblioref">Ommen and Saunders 2018</a>)</span>.
Historically, forensic examiners have relied on tools (e.g., microscopes), case facts, and experience to develop an opinion on the similarity of two pieces of evidence.
More recently, algorithms to automatically compare evidence and provide an objective measure of similarity have been introduced.
These algorithms can be used in a forensic examination to supplement and inform the examiner’s conclusion.
We propose an automatic, objective solution to the source identification problem; specifically in the context of comparing fired <em>cartridge cases.</em>
Cartridge case comparison is a sub-discipline of Firearm and Toolmark Identification, which is reviewed in the next section.</p>
<div id="firearm-and-toolmark-identification" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Firearm and Toolmark Identification<a href="index.html#firearm-and-toolmark-identification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Firearm and toolmark identification involves studying markings or impressions left by a hard surface such as a firearm or screwdriver on a softer surface <span class="citation">(<a href="#ref-Thompson2017" role="doc-biblioref">Thompson 2017</a>)</span>.
For example, a barrel’s rifling leaves toolmarks on a bullet as it travels out of the gun.</p>
<div id="the-firing-process" class="section level4 hasAnchor" number="1.1.1.1">
<h4><span class="header-section-number">1.1.1.1</span> The Firing Process<a href="index.html#the-firing-process" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this section, we describe the basic process of firing a cartridge out of a handgun or rifle.
A <em>cartridge</em> consists of a metal casing containing primer, gunpowder, and a bullet.
Figure <a href="index.html#fig:cartridgeDiagram">1.1</a> shows a cross-section of a cartridge featuring these components <span class="citation">(<a href="#ref-calibersExplained" role="doc-biblioref">30 Magazine Clip 2017</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cartridgeDiagram"></span>
<img src="images/bulletdiagram1.png" alt="A cartridge containing primer, powder, and a bullet. The firing process is initiated by loading a cartridge into the barrel of a firearm." width=".5\textwidth" />
<p class="caption">
Figure 1.1: A cartridge containing primer, powder, and a bullet. The firing process is initiated by loading a cartridge into the barrel of a firearm.
</p>
</div>
<p>First, a cartridge is loaded into the back of the barrel in an area called the <em>chamber</em>.
Figure <a href="index.html#fig:pistolParts">1.2</a> shows an example of a cartridge loaded into the chamber of a pistol <span class="citation">(<a href="#ref-rattenbury" role="doc-biblioref">Rattenbury 2015</a>)</span>.
Note that the hammer of the pistol in Figure <a href="index.html#fig:pistolParts">1.2</a> is pulled to hold the firing pin under spring tension.
Upon squeezing the trigger, the firing pin releases and travels forwards at a high velocity.
The firing pin strikes the primer of the cartridge case, causing it to explode.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pistolParts"></span>
<img src="images/Parts-pistol.png" alt="Cross-section of a pistol with a chambered cartridge and drawn-back hammer. Pulling the trigger releases the firing pin which strikes the cartridge case primer." width=".5\textwidth" />
<p class="caption">
Figure 1.2: Cross-section of a pistol with a chambered cartridge and drawn-back hammer. Pulling the trigger releases the firing pin which strikes the cartridge case primer.
</p>
</div>
<p>The explosion of the primer ignites the powder in the cartridge <span class="citation">(<a href="#ref-hampton" role="doc-biblioref">Hampton 2016</a>)</span>.
As shown in <a href="index.html#fig:firingCartridge">1.3</a>, gas rapidly expands in the cartridge that pushes the bullet down the barrel.
Simultaneously, the rest of the cartridge travels towards the back of the barrel and strikes the back wall of the barrel, known as the <em>breech face</em>, with considerable force.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:firingCartridge"></span>
<img src="images/firingCartridgeDiagram.jpg" alt="A cartridge after a firing pin has struck the primer. The explosion of the primer ignites the powder within the cartridge, causing gas to rapidly expand and force the bullet down the barrel." width=".5\textwidth" />
<p class="caption">
Figure 1.3: A cartridge after a firing pin has struck the primer. The explosion of the primer ignites the powder within the cartridge, causing gas to rapidly expand and force the bullet down the barrel.
</p>
</div>
<p>Any markings on the breech face are imprinted onto the cartridge case, creating the so-called <em>breech face impressions</em>.
These impressions are analogous to a barrel’s “fingerprint” left on the cartridge case.
Figure <a href="index.html#fig:impressionDiagram">1.4</a> shows cartoon examples of breech face markings that appear on cartridge cases <span class="citation">(<a href="#ref-hampton" role="doc-biblioref">Hampton 2016</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:impressionDiagram"></span>
<img src="images/breechFaceImpressionDiagram.jpg" alt="Examples of common breech face impression patterns. These are considered analogous to a breech face fingerprint left on the cartridge surface." width=".5\textwidth" />
<p class="caption">
Figure 1.4: Examples of common breech face impression patterns. These are considered analogous to a breech face fingerprint left on the cartridge surface.
</p>
</div>
<p>Figure <a href="index.html#fig:realCartridgeCase">1.5</a> shows the base of a fired cartridge <span class="citation">(<a href="#ref-hampton" role="doc-biblioref">Hampton 2016</a>)</span>.
The hole to the south-east of the center of the primer is the impression left by the firing pin.
Note the horizontal striated breech face markings on the primer to the left of the firing pin impression.
We focus on the comparison of such markings.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:realCartridgeCase"></span>
<img src="images/realCartridgeCaseImage.jpg" alt="A fired 9mm Luger cartridge case with visible firing pin and breech face impressions." width=".7\textwidth" />
<p class="caption">
Figure 1.5: A fired 9mm Luger cartridge case with visible firing pin and breech face impressions.
</p>
</div>
<!-- The extractor pin and ejector pushes the cartridge case out of the chamber. -->
<!-- As shown in Figure \@ref(fig:extractorMarkings), this may leave additional markings on the cartridge surface \citep{hampton}. -->
<!-- Firing pin, breech face, and extractor pin and ejector markings are all used in a forensic examination to determine whether two cartridge cases were fired from the same firearm. -->
<!-- We focus on the comparison of breech face impressions specifically. -->
<!-- ```{r,echo=FALSE,fig.cap="\\label{fig:extractorMarkings} Examples of common extractor pin and ejector markings. Forensic examiners study impressions on the cartridge to identify the source of the fired cartridge.",out.width=".5\\textwidth"} -->
<!-- knitr::include_graphics("images/extractorPinDiagram.png") -->
<!-- ``` -->
</div>
<div id="an-overview-of-firearm-and-toolmark-examinations" class="section level4 hasAnchor" number="1.1.1.2">
<h4><span class="header-section-number">1.1.1.2</span> An Overview of Firearm and Toolmark Examinations<a href="index.html#an-overview-of-firearm-and-toolmark-examinations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Trained firearm and toolmark examiners use a <em>comparison microscope</em> like the one in Figure <a href="index.html#fig:comparisonMicroscope">1.6</a> to examine two pieces of evidence <span class="citation">(<a href="#ref-Zheng2014" role="doc-biblioref">X. Zheng et al. 2014</a>)</span>.
A comparison microscope combines the view of two compound microscopes into a single view via an <em>optical bridge</em>.
This allows an examiner to view two microscope stages simultaneously under the same eyepiece.
The right Figure <a href="index.html#fig:comparisonMicroscope">1.6</a> shows the view of two bullets under a comparison microscope.
The white dotted line represents the split in the two fields of view.
The goal of using a comparison microscope is to assess the “agreement” of the features on two pieces of evidence.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:comparisonMicroscope"></span>
<img src="images/comparisonMicroscope.png" alt="A comparison microscope consists of two stages upon which evidence is placed. These stages are placed under two compound microscopes that are joined together via an optical bridge and allow for viewing of both stages simultaneously under a single eyepiece. The image on the right shows an example of a bullet viewed under a comparison microscope." width=".6\textwidth" />
<p class="caption">
Figure 1.6: A comparison microscope consists of two stages upon which evidence is placed. These stages are placed under two compound microscopes that are joined together via an optical bridge and allow for viewing of both stages simultaneously under a single eyepiece. The image on the right shows an example of a bullet viewed under a comparison microscope.
</p>
</div>
<p>Firearm examiners distinguish between three broad categories when characterizing the features of a fired bullet or cartridge case: class, subclass, and individual characteristics.
<em>Class characteristics</em> are features associated with the manufacturing of the firearm such as the size of ammunition chambered by the firearm, the orientation of the extractor and ejector, or the width and twist direction of the barrel rifling.
An early step in a forensic examination is to determine the class characteristics of the firearm of origin as they can narrow the relevant population of potential firearm sources <span class="citation">(<a href="#ref-Thompson2017" role="doc-biblioref">Thompson 2017</a>)</span>.
For example, a 9mm cartridge case must have been fired by a firearm that can chamber 9mm ammunition.</p>
<p>If the discernible class characteristics match between two pieces of evidence, then the examiner uses a comparison microscope to compare the <em>individual characteristics</em> of the evidence.
Individual characteristics are markings attributed to imperfections on the firearm surface due to the manufacturing process, use, and wear of the tool.
For example, markings on the breech face of a barrel often form after repeated fires of the firearm.
Individual characteristics are assumed to occur randomly across different firearms and therefore can be used to distinguish between two firearms.
The examiner independently rotates and translates the stages of a comparison microscope to find the position where the markings on the two pieces of evidence match <span class="citation">(<a href="#ref-Zheng2014" role="doc-biblioref">X. Zheng et al. 2014</a>)</span>.
An examiner concludes that the evidence originated from the same firearm if the individual characteristics are in “sufficient agreement” <span class="citation">(<a href="#ref-AFTE1992" role="doc-biblioref">AFTE Criteria for Identification Committee 1992</a>)</span>.</p>
<p><em>Subclass characteristics</em> exist between the macro-level class and micro-level individual characteristics.
These characteristics relate to markings reproduced across a subgroup of firearms.
For example, breech faces manufactured by the same milling machine may share similar markings <span class="citation">(<a href="#ref-firearmManufacturing" role="doc-biblioref">Werner et al. 2021</a>)</span>.
It can be difficult to distinguish between individual and subclass characteristics during an examination.
An examiner’s decision process may be affected if the existence of subclass characteristics is suspected.</p>
<p>Many firearm and toolmark examiners in the United States adhere to the Association of Firearm and Toolmark Examiners (AFTE) Range of Conclusions when making their evidentiary conclusions <span class="citation">(<a href="#ref-AFTE1992" role="doc-biblioref">AFTE Criteria for Identification Committee 1992</a>)</span>.
According to these guidelines, six possible conclusions can be made in a firearm and toolmark examination:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Identification</strong>: Agreement of a combination of individual characteristics and all discernible class characteristics where the extent of agreement exceeds that which can occur in the comparison of toolmarks made by different tools and is consistent with the agreement demonstrated by toolmarks known to have been produced by the same tool.</p></li>
<li><p><strong>Inconclusive</strong>: there are three possible inconclusive decisions</p></li>
</ol>
<blockquote>
<p>2.1 Some agreement of individual characteristics and all discernible class characteristics, but insufficient for an identification.</p>
</blockquote>
<blockquote>
<p>2.2 Agreement of all discernible class characteristics without agreement or disagreement of individual characteristics due to an absence, insufficiency, or lack of reproducibility.</p>
</blockquote>
<blockquote>
<p>2.3 Agreement of all discernible class characteristics and disagreement of individual characteristics, but insufficient for an elimination.</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>Elimination</strong>: Significant disagreement of discernible class characteristics and/or individual characteristics.</p></li>
<li><p><strong>Unsuitable</strong>: Unsuitable for examination.</p></li>
</ol>
<p>Forensic examinations first involve an examination of a “questioned” bullet or cartridge case for identifiable toolmarks <span class="citation">(<a href="#ref-Thompson2017" role="doc-biblioref">Thompson 2017</a>)</span>.
The examiner classifies markings by their class, individual, and subclass characteristics.
The examiner compares these characteristics to “known source” fires obtained from a suspect’s firearm if one is available.
Otherwise, class characteristics from the questioned bullet can be used to narrow the relevant population and provide potential leads.
An examiner’s decision may be used as part of an ongoing investigation or presented at trial as expert testimony.</p>
<p>Standard operating procedures for assessing and comparing evidence differ between forensic laboratories.
For example, some labs collapse the three possible inconclusive decisions into a single decision <span class="citation">(<a href="#ref-Neuman2022" role="doc-biblioref">Neuman et al. 2022</a>)</span> or prohibit examiners from making an elimination based on differences in individual characteristics <span class="citation">(<a href="#ref-Duez2017" role="doc-biblioref">Duez et al. 2017</a>)</span>.</p>
</div>
</div>
<div id="why-should-firearm-and-toolmark-identification-change" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Why Should Firearm and Toolmark Identification Change?<a href="index.html#why-should-firearm-and-toolmark-identification-change" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In 2009, the National Research Council released a report assessing a number of forensic disciplines including Firearm and Toolmark analysis.
The report pointed out that firearm and toolmark analysis lacked a precisely defined process and that little research had been done to determine the reliability or repeatability of the methods.
<em>Reliability</em> refers to the ability to correctly classify evidence as originating from the same source or not.
<em>Repeatability</em> refers to the consistency of conclusions; for example, whether an examiner makes the same conclusion if presented with the same evidence on different occasions.
Two of the recommendations from this study were to establish rigorously-validated laboratory procedures and “develop automated techniques capable of enhancing forensic technologies <span class="citation">(<a href="#ref-council_strengthening_2009" role="doc-biblioref">National Research Council 2009</a>)</span>.”</p>
<p>A number of studies assess the reliability and repeatability of a firearm and toolmark examination (non-exhaustively: <span class="citation">DeFrance and Arsdale (<a href="#ref-DeFrance2003" role="doc-biblioref">2003</a>)</span>, <span class="citation">Hamby, Brundage, and Thorpe (<a href="#ref-Hamby2009" role="doc-biblioref">2009</a>)</span>, <span class="citation">Fadul et al. (<a href="#ref-fadulempirical2011" role="doc-biblioref">2011a</a>)</span>, <span class="citation">Stroman (<a href="#ref-Stroman2014" role="doc-biblioref">2014</a>)</span>, <span class="citation">Baldwin et al. (<a href="#ref-Baldwin2014" role="doc-biblioref">2014</a>)</span>, <span class="citation">Smith, Smith, and Snipes (<a href="#ref-Smith2016" role="doc-biblioref">2016</a>)</span>, <span class="citation">Mattijssen et al. (<a href="#ref-MATTIJSSEN2020" role="doc-biblioref">2020</a>)</span>).
These studies indicate that examiners have a low error rate when comparing evidence obtained under controlled conditions (i.e., for which ground-truth is known).
However, as pointed out in a 2016 report from the President’s Council of Advisors on Science and Technology, many of these studies, save <span class="citation">Baldwin et al. (<a href="#ref-Baldwin2014" role="doc-biblioref">2014</a>)</span>, were not “appropriately designed to test the foundational validity and estimate reliability <span class="citation">(<a href="#ref-pcast2016" role="doc-biblioref">President’s Council of Advisors on Sci. &amp; Tech. 2016</a>)</span>.”
The report called for more properly-designed studies to establish the scientific validity of the discipline.</p>
<p>Due to the opacity in the decision-making process, examiners are referred to as “black boxes” in a similar sense to black-box algorithms <span class="citation">(<a href="#ref-HumanFactorsCommittee2020" role="doc-biblioref">OSAC Human Factors Committee 2020</a>)</span>.
Their evidentiary conclusions are fundamentally subjective and empirical evidence suggests that conclusions may differ if examiners are presented with the same evidence on different occasions <span class="citation">(<a href="#ref-Ulery2011" role="doc-biblioref">Ulery et al. 2011</a>, <a href="#ref-Ulery2012" role="doc-biblioref">2012</a>)</span>.
Examiners rarely need to provide quantitative justification for their conclusion.
Even for qualitative justifications, it can be difficult to determine what the examiner is actually “looking at” to arrive at their conclusion <span class="citation">(<a href="#ref-Ulery2014" role="doc-biblioref">Ulery et al. 2014</a>)</span>.
This suggests the need to supplement these black box decisions with transparent, objective techniques that quantitatively measure the similarity between pieces of evidence.
As stated in <span class="citation">President’s Council of Advisors on Sci. &amp; Tech. (<a href="#ref-pcast2016" role="doc-biblioref">2016</a>)</span>, efforts should be made to “convert firearms analysis from a subjective method to an objective method” including “developing and testing image-analysis algorithms for comparing the similarity of tool marks.”
This work focuses on the development of an algorithm for comparing breech face impressions on cartridge cases.</p>
</div>
</div>
<div id="forensic-comparison-pipelines" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Forensic Comparison Pipelines<a href="index.html#forensic-comparison-pipelines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recent work in many forensic disciplines has focused on the development of algorithms to measure the similarity between pieces of evidence including glass <span class="citation">(<a href="#ref-Curran2000-hp" role="doc-biblioref">Curran, Champod, and Buckleton 2000a</a>; <a href="#ref-Park2019" role="doc-biblioref">Park and Tyner 2019</a>; <a href="#ref-openForSciR" role="doc-biblioref">Tyner et al. 2019</a>)</span>, handwriting <span class="citation">(<a href="#ref-crawford_handwriting_2020" role="doc-biblioref">Crawford 2020</a>)</span>, shoe prints <span class="citation">(<a href="#ref-park_algorithm_2020" role="doc-biblioref">Park and Carriquiry 2020</a>)</span>, ballistics <span class="citation">(<a href="#ref-hare_automatic_2016" role="doc-biblioref">Hare, Hofmann, and Carriquiry 2017</a>; <a href="#ref-tai_fully_2018" role="doc-biblioref">Tai and Eddy 2018</a>)</span>, and toolmarks <span class="citation">(<a href="#ref-Hadler2017" role="doc-biblioref">Hadler and Morris 2017</a>; <a href="#ref-Krishnan2018" role="doc-biblioref">Krishnan and Hofmann 2018</a>)</span>.
These algorithms often result in a numerical score for two pieces of evidence.
A numerical score can add more nuance to an evidentiary conclusion beyond simply stating whether the evidence originated from the same source as would be the case in binary classification.
For example, a larger similarity scores implies the evidence is more similar.
However, an examiner must ultimately reach one of two conclusions (or three, if admitting inconclusives).
Whether a conclusion should be based solely on an algorithm’s similarity score or if an examiner should incorporate the similarity score into their own decision-making process is still up for debate <span class="citation">(<a href="#ref-Swofford2021" role="doc-biblioref">Swofford and Champod 2021</a>)</span>.
In this work we view forensic comparison algorithms as a supplement to, rather than a replacement of, the traditional forensic examination.</p>
<p>We treat forensic comparison algorithms as evidence-to-classification “pipelines.”
Broadly, the steps of the pipeline include <span class="citation">Rice (<a href="#ref-Rice2020" role="doc-biblioref">2020</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>capturing a digital representation of the evidence,</p></li>
<li><p>pre-processing this representation to isolate or emphasize a region of interest of the evidence,</p></li>
<li><p>comparing regions of interest from two different pieces of evidence to obtain a (perhaps high-dimensional) set of similarity features,</p></li>
<li><p>combining these features into a low-dimensional set of similarity scores, and</p></li>
<li><p>defining a classification rule based on these similarity features.</p></li>
</ol>
<p>We add to this structure the emphasis that each step of the pipeline can be further broken-down into modularized pieces.
For example, the pre-processing step may include multiple sub-procedures to isolate a region of interest of the evidence.
Figure <a href="index.html#fig:pipelineDiagram">1.7</a> shows two possible variations of the cartridge case comparison pipeline as well as the parameters requiring manual specification and alternative modules.
The benefits of this modularization include easing the process of experimenting with different parameters/sub-procedures and improving the comprehensibility of the pipeline.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pipelineDiagram"></span>
<img src="images/pipelineDiagram_6-8-22.png" alt="Variations upon the cartridge case comparison pipeline. The first two columns detail the pipeline with different sub-procedures. The third columns shows the parameters that require manual specification at each step. The fourth column shows  alternative processing steps that could replace steps in the existing pipeline." width="\textwidth" />
<p class="caption">
Figure 1.7: Variations upon the cartridge case comparison pipeline. The first two columns detail the pipeline with different sub-procedures. The third columns shows the parameters that require manual specification at each step. The fourth column shows alternative processing steps that could replace steps in the existing pipeline.
</p>
</div>
<p>In the following sections, we detail recent advances to each of the five steps in the pipeline outlined above.
We narrow our focus to advances made in comparing firearms evidence.</p>
<div id="digital-representations-of-evidence" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Digital Representations of Evidence<a href="index.html#digital-representations-of-evidence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Digital representations of cartridge case evidence commonly come in two forms: 2D optical images or 3D topographic scans.
A common way to take 2D optical images is to take a picture of the cartridge case surface lit up under a microscope, implying a dependence on the lighting conditions under which the picture was taken.
Some recent work has focused on comparing 2D optical images <span class="citation">(<a href="#ref-tai_fully_2018" role="doc-biblioref">Tai and Eddy 2018</a>; <a href="#ref-tong_fired_2014" role="doc-biblioref">Tong et al. 2014</a>)</span>, although the use of 3D microscopes has recently become more prevalent to capture the surface of ballistics evidence.</p>
<p>Using a 3D microscope allows for the scanning of surfaces at the micron (or micrometer) level under light-agnostic conditions <span class="citation">(<a href="#ref-weller_2012" role="doc-biblioref">T. J. Weller et al. 2012</a>)</span>.
Figure <a href="index.html#fig:cartridgeCaseImages">1.8</a> shows a 2D image and 3D topography of the same cartridge case primer from <span class="citation">Fadul et al. (<a href="#ref-fadulempirical2011" role="doc-biblioref">2011a</a>)</span>.
<!-- One common 3D scanning procedure is "disc scanning confocal microscopy." -->
<!-- This procedure works by shining a focused beam of light on the cartridge case surface. -->
<!-- This light is reflected back onto a pinhole allowing a limited height range to pass through. -->
<!-- The microscope scans through different height range "slices" and compiles all these slices into a single 3D topography of the cartridge case primer surface. -->
<!-- The Microdisplay Scan Confocal Microscope from Sensofar\texttrademark Metrology is shown in Figure \@ref(fig:sensofarScanner) \citep{bermudez2017confocal}. --></p>
<!-- ```{r,echo=FALSE,fig.cap="\\label{fig:sensofarScanner} The Microdisplay Scan Confocal Microscope from Sensofar\\texttrademark\\ Metrology. The cartridge case surface is captured by scanning through a range of vertical slices and compiling these slices into a single 3D topography.",out.width=".5\\textwidth"} -->
<!-- knitr::include_graphics("images/sensofarScanner.png") -->
<!-- ``` -->
<!-- Figure \@ref(fig:cartridgeCaseImages) shows a 2D image and 3D topography of the same cartridge case primer from \citet{fadulempirical2011}. -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cartridgeCaseImages"></span>
<img src="images/fadul1-1_sidebyside.PNG" alt="A cartridge case captured using 2D confocal reflectance microscopy (left) and 3D disc scanning confocal microscopy (right)." width="\textwidth" />
<p class="caption">
Figure 1.8: A cartridge case captured using 2D confocal reflectance microscopy (left) and 3D disc scanning confocal microscopy (right).
</p>
</div>
<p>Recently, Cadre Forensicsintroduced the TopMatch-3D High-Capacity Scanner <span class="citation">(<a href="#ref-topmatch" role="doc-biblioref">T. Weller et al. 2015</a>)</span>.
Figure <a href="index.html#fig:topMatchScanner">1.9</a> shows a TopMatch scanner with a tray of 15 fired cartridge cases <span class="citation">(<a href="#ref-topmatchFlyer" role="doc-biblioref">Cadre Forensics 2019</a>)</span>.
This scanner collects images of a gel pad under various lighting conditions into which the cartridge case surface is impressed. Proprietary algorithms combine these images into a regular 2D array called a <em>surface matrix</em>.
Elements of the surface matrix represent the relative height value of the associated surface.<br />
The physical dimensions of these scans are about 5.5 <span class="math inline">\(mm^2\)</span> captured at a resolution of 1.84 microns per pixel (1000 microns equals 1 mm).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:topMatchScanner"></span>
<img src="images/TopMatchSystem7.png" alt="The TopMatch-3D High-Capacity Scanner from Cadre Forensics\texttrademark\ . The scanner captures topographic scans of a gel pad into which a cartridge case surface is impressed." width=".7\textwidth" />
<p class="caption">
Figure 1.9: The TopMatch-3D High-Capacity Scanner from Cadre Forensics . The scanner captures topographic scans of a gel pad into which a cartridge case surface is impressed.
</p>
</div>
<p>The ISO standard x3p file format is commonly used to save 3D scans <span class="citation">(<a href="#ref-ISO25178-72" role="doc-biblioref"><span>“<span class="nocase">Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p</span>”</span> 2017</a>)</span>.
An x3p is a container consisting of a single surface matrix representing the height values of the surface and metadata concerning the parameters under which the scan was taken as shown in Figure <a href="index.html#fig:x3pFlowchart">1.10</a> <span class="citation">(<a href="#ref-Zheng2020" role="doc-biblioref">Xiaoyu Zheng et al. 2020</a>)</span>.
A number of studies suggest that 3D topographic scans of cartridge case surfaces lead to more accurate classifications than 2D optical images of the same evidence <span class="citation">(<a href="#ref-Tai2019" role="doc-biblioref">Tai 2019</a>; <a href="#ref-tong_fired_2014" role="doc-biblioref">Tong et al. 2014</a>; <a href="#ref-song_3d_2014" role="doc-biblioref">Song et al. 2014</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:x3pFlowchart"></span>
<img src="images/x3pFlowchart.jpg" alt="The hierarchy of information stored in the x3p file format for both bullet and cartridge case evidence." width=".7\textwidth" />
<p class="caption">
Figure 1.10: The hierarchy of information stored in the x3p file format for both bullet and cartridge case evidence.
</p>
</div>
</div>
<div id="pre-processing-procedures-for-forensic-data" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Pre-processing Procedures for Forensic Data<a href="index.html#pre-processing-procedures-for-forensic-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After obtaining a surface’s digital representation, we next want to isolate regions of the surface containing distinguishable markings.
Figure <a href="index.html#fig:cartridgeCaseImages">1.8</a> shows an example of a 2D image and 3D scan of the same cartridge case.
In both representations, the corners of these arrays include regions of the cartridge case surface outside of the primer.
The center of the cartridge case primer contains an impression left by the firing pin during the firing process.
<!-- In most applications, impressions left by the firing pin are compared separately from the breech face impressions \citep{Zhang2016}. -->
<!-- Because we are interested in the comparison of breech face impressions between two cartridge cases, only the annular region surrounding the firing pin impression is of interest. -->
We wish to isolate the annular breech face region around the firing pin impression from the rest of the captured surface.</p>
<p>Both the 2D optical and 3D topographic representations of cartridge case surfaces are fundamentally pictorial in nature.
As such, breech face impression isolation commonly relies on image processing and computer vision techniques.
<span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span> uses a combination of histogram equalization, Canny edge detection, and morphological operations to isolate breech face impressions in 2D images.
A Gaussian filter is another common tool to emphasize breech face impressions.
<span class="citation">Tong et al. (<a href="#ref-tong_fired_2014" role="doc-biblioref">2014</a>)</span> apply a low-pass Gaussian filter to remove noise via a Gaussian-weighted moving average operation.
<span class="citation">Chu, Tong, and Song (<a href="#ref-chu_validation_2013" role="doc-biblioref">2013</a>)</span> and <span class="citation">Song et al. (<a href="#ref-song_estimating_2018" role="doc-biblioref">2018</a>)</span> use a bandpass Gaussian filter to simultaneously remove noise and unwanted global structure from the scan.
<span class="citation">Song et al. (<a href="#ref-song_3d_2014" role="doc-biblioref">2014</a>)</span> and <span class="citation">Chen et al. (<a href="#ref-chen_convergence_2017" role="doc-biblioref">2017</a>)</span> use a “robust” variant of the Gaussian filter to omit outliers from the scan <span class="citation">(<a href="#ref-ISO16610-71" role="doc-biblioref"><span>“<span class="nocase">Geometrical product specifications (GPS) - Filtration - Part 71: Robust areal filters: Gaussian regression filters</span>”</span> 2014</a>)</span>.</p>
<p>Instead of automatic procedures, others have used subjective human intervention to isolate the breech face impressions.
For example, <span class="citation">Song et al. (<a href="#ref-song_estimating_2018" role="doc-biblioref">2018</a>)</span> performed “manually trimming to extract the breech face impression of interest” on a set of cartridge case scans.
In <span class="citation">Roth et al. (<a href="#ref-Roth2015" role="doc-biblioref">2015</a>)</span>, examiners manually identify the borders of the breech face impression region by placing points around an image of the cartridge case primer.</p>
</div>
<div id="forensic-data-feature-extraction" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Forensic Data Feature Extraction<a href="index.html#forensic-data-feature-extraction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After isolating the breech face impressions, we compare two pre-processed cartridge case scans and compute a set of similarity features.
Because the cartridge cases at this point are represented as high-dimensional matrices, this process can be thought of as a dimensionality reduction of the high-dimensional surface arrays to a set of similarity statistics.</p>
<p>A variety of features have been proposed to quantify the similarity between two cartridge case surface arrays.
<span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span> propose calculating the cross-correlation function (CCF) value between two cartridge cases across a grid of rotations.
The cross-correlation function measures the similarity between two matrices for every translation of one matrix against the other.
For two matching cartridge cases, we assume that the CCF will be largest after aligning the cartridge cases surfaces by their shared breech face impressions.
Conversely, we expect the CCF to be relatively small for two non-matching cartridge cases no matter the alignment.
<span class="citation">Riva and Champod (<a href="#ref-Riva2014" role="doc-biblioref">2014</a>)</span> propose combining the CCF between two aligned scans with the element-wise median Euclidean distance and median difference between the normal vectors at each point of the surface.
<span class="citation">Riva et al. (<a href="#ref-Riva2016" role="doc-biblioref">2016</a>)</span> and <span class="citation">Riva et al. (<a href="#ref-Riva2020" role="doc-biblioref">2020</a>)</span> applied Principal Component Analysis to reduce these three features down to two principal components for the sake of fitting a 2D kernel density estimator.</p>
<p>Pertinent to this work is the cell-based comparison procedure originally outlined in <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span>.
The underlying assumption of <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> is similar to that of <span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span>: that two matching cartridge cases will exhibit higher similarity when they are close to being correctly aligned.
While <span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span> measured similarity using the CCF between the two full scans, <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> proposes partitioning the scans into a grid of “correlation cells” and counting the number of similar cells between the two scans.
The rationale behind this procedure is that many cartridge case scans have only a few regions with discriminatory markings.
As such, comparing full scans may result in a lower correlation than if one were to focus on the highly-discriminatory regions.
In theory, dividing the scans into cells allows for the identification of these regions.
After breaking a scan into a grid of cells, each cell is compared to the other scan to identify the rotation and translation, known together as the <em>registration</em>, at which the cross-correlation is maximized.
<span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> assumes that the cells from a truly matching pair of cartridge cases will “agree” on their registration in the other scan.
<span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> referred to the procedure of counting the number of similar cells the “Congruent Matching Cells” method.
Chapter 2 contains more details of this procedure.</p>
</div>
<div id="similarity-scores-classification-rules-for-forensic-data" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Similarity Scores &amp; Classification Rules for Forensic Data<a href="index.html#similarity-scores-classification-rules-for-forensic-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Following feature extraction, the dimensionality of these features is further reduced to a low-dimensional, usually univariate, similarity score.
We can define a decision boundary based on the value of the similarity score to classify cartridge case pairs as matching or non-matching.</p>
<p>After calculating the CCF across various possible registrations, <span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span> propose using the maximum observed CCF value as the univariate similarity score.
They perform binary classifications by setting a CCF threshold above which pairs are classified as “matches” and below which as “non-matches.”
<span class="citation">Tai (<a href="#ref-Tai2019" role="doc-biblioref">2019</a>)</span> proposes setting a CCF cut-off that maximizes the precision and recall in a training set of pairwise comparisons.</p>
<p><span class="citation">Riva et al. (<a href="#ref-Riva2016" role="doc-biblioref">2016</a>)</span> and <span class="citation">Riva et al. (<a href="#ref-Riva2020" role="doc-biblioref">2020</a>)</span> use a training set to fit two 2D kernel density estimates to a set of features from matching and non-matching comparisons.
Using these estimates, they compute a score-based likelihood ratio (SLR), which can be interpreted as a similarity score <span class="citation">(<a href="#ref-Garton2021" role="doc-biblioref">Garton et al. 2020</a>)</span>.</p>
<p>For the Congruent Matching cells method, <span class="citation">Song (<a href="#ref-song_proposed_2013" role="doc-biblioref">2013</a>)</span> proposes using the number of cells that agree on a registration, the “congruent matching” cells, as a similarity score.
The criteria used to define “congruent matching” cells has changed across papers <span class="citation">(<a href="#ref-song_3d_2014" role="doc-biblioref">Song et al. 2014</a>; <a href="#ref-tong_fired_2014" role="doc-biblioref">Tong et al. 2014</a>; <a href="#ref-tong_improved_2015" role="doc-biblioref">Tong, Song, and Chu 2015</a>; <a href="#ref-chen_convergence_2017" role="doc-biblioref">Chen et al. 2017</a>)</span> and will be discussed in greater detail in Chapter 2.
The authors of these papers have consistently used six congruent matching cells as a decision boundary to distinguish matching and non-matching cartridge case pairs.</p>
<p><span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span> applies the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm <span class="citation">(<a href="#ref-Ester1996" role="doc-biblioref">Ester et al. 1996</a>)</span> to the features from the cell-based comparison procedure to determine if any clusters form amongst the per-cell estimated registration values.
This is based on the assumption that any cells that come to a consensus on their registration should form a cluster in translation <span class="math inline">\((x,y)\)</span> and rotation <span class="math inline">\(\theta\)</span> space.
<span class="citation">Zhang et al. (<a href="#ref-zhang_convergence_2021" role="doc-biblioref">2021</a>)</span> proposes a binary classifier based on whether any clusters are identified by the DBSCAN algorithm <span class="citation">(<a href="#ref-Ester1996" role="doc-biblioref">Ester et al. 1996</a>)</span>.
If a cluster is found for a particular pairwise comparison, then that pair is classified as a “match” and otherwise as a “non-match.”</p>
<p>Apart from the algorithms described in <span class="citation">Tai and Eddy (<a href="#ref-tai_fully_2018" role="doc-biblioref">2018</a>)</span> and <span class="citation">Tai (<a href="#ref-Tai2019" role="doc-biblioref">2019</a>)</span>, the authors of these comparison algorithms have not provided publicly available code or data.
As such, although the results reported in associated papers are promising, it is difficult or impossible for other researchers to verify or reproduce the findings.
Results must be reproducible to be accepted by others in any scientific domain.
In the next section, we discuss recent challenges and opportunities in computationally reproducible research.</p>
</div>
<div id="reproducibility-of-comparison-pipelines" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Reproducibility of Comparison Pipelines<a href="index.html#reproducibility-of-comparison-pipelines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="citation">National Academy of Sciences, Engineering, and Medicine (<a href="#ref-nasem_2019" role="doc-biblioref">2019</a>)</span> defines <em>reproducibility</em> as “obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis.”
While not exact in their definition of “consistent,” the authors assert that, barring a few exceptions, it is reasonable to expect that the results obtained by a second researcher, after applying the exact same processing steps to the exact same data, be the exact same as the original results.
<!-- Among the exceptions given is if the original researcher had made a mistake in writing the original source code. -->
In either case, they assert that “a study’s data and code have to be available in order for others to reproduce and confirm results.”
Given data and code, researchers are able to verify the results, incorporate the materials into their own research, and improve or accelerate discovery <span class="citation">(<a href="#ref-Stodden2018b" role="doc-biblioref">Stodden, Krafczyk, and Bhaskar 2018</a>)</span>.</p>
<p>A number of studies indicate that computationally reproducible research is sparse across various disciplines.
<span class="citation">Stodden, Krafczyk, and Bhaskar (<a href="#ref-Stodden2018b" role="doc-biblioref">2018</a>)</span> and <span class="citation">Stodden, Seiler, and Ma (<a href="#ref-Stodden2018a" role="doc-biblioref">2018</a>)</span> studied the reproducibility of articles sampled from the <em>Journal of Computational Physics</em> and the journal <em>Science</em>, respectively.
In the former, <span class="citation">Stodden, Krafczyk, and Bhaskar (<a href="#ref-Stodden2018b" role="doc-biblioref">2018</a>)</span> found that zero of 306 randomly selected articles from the <em>Journal of Computaional Physics</em> were “straightforward to reproduce with minimal effort” and, at best, that five articles were “reproducible after some tweaking.”
In the latter, <span class="citation">Stodden, Seiler, and Ma (<a href="#ref-Stodden2018a" role="doc-biblioref">2018</a>)</span> found that only 3 of 204 randomly selected articles from <em>Science</em> were “straightforward to reproduce with minimal effort;” despite a journal policy requiring that all code and data used in the paper be made available to any reader.
Similar findings were found in <span class="citation">A. C. Chang and Li (<a href="#ref-CChang2022" role="doc-biblioref">2022</a>)</span> (29 of 59 economic papers reproducible), <span class="citation">Iqbal et al. (<a href="#ref-Iqbal2016" role="doc-biblioref">2016</a>)</span> (zero of 268 biomedical papers provided raw data and 1 in 268 linked to a full study protocol), <span class="citation">Duvendack, Palmer-Jones, and Reed (<a href="#ref-Duvendack2015" role="doc-biblioref">2015</a>)</span> (50% or more published articles include data or code in only 27 of 333 economics journals), and <span class="citation">Gundersen, Gil, and Aha (<a href="#ref-Gundersen2018" role="doc-biblioref">2018</a>)</span> (24 of 400 AI conference papers included code).
A common recommendation amongst these authors is to establish of rigorous tools and standards to promote reproducibility.
This includes making code and data used in a paper easily-accessible to readers.</p>
<p>Infrastructure already exists to ease the process of developing, maintaining, and sharing open-source code and data.
Data repositories such as the NIST Ballistics Toolmark Research Database <span class="citation">(<a href="#ref-Zheng2020" role="doc-biblioref">Xiaoyu Zheng et al. 2020</a>)</span> provide open access to raw data.
<span class="citation">Grüning et al. (<a href="#ref-Grning2018" role="doc-biblioref">2018</a>)</span> discuss the use of package managers such as Conda (<a href="https://anaconda.org/anaconda/conda">https://anaconda.org/anaconda/conda</a>), container software such as Docker (<a href="https://www.docker.com/">https://www.docker.com/</a>), and virtual machine software to preserve the entire data analysis environment in-perpetuity.
For situations in which VMs or containers aren’t available, software such as the <code>manager</code> R package allows users to “compare package inventories across machines, users, and time to identify changes in functions and objects <span class="citation">(<a href="#ref-Rice2020" role="doc-biblioref">Rice 2020</a>)</span>.”
<span class="citation">Piccolo and Frampton (<a href="#ref-Piccolo2016" role="doc-biblioref">2016</a>)</span> reference repositories like Bioconductor <span class="citation">(<a href="#ref-Huber2015" role="doc-biblioref">Huber et al. 2015</a>)</span> that make it easy to document and distribute code.
Further, software such as the <code>knitr</code> R package <span class="citation">(<a href="#ref-Xie2014" role="doc-biblioref">Xie 2014a</a>)</span> enable “literate programming” in which prose and executed code can be interwoven to make it easier to understand the code’s function.
These tools make data, code, and derivative research findings more accessible, in terms of both acquisition and comprehensibility, to consumers and fellow researchers.</p>
</div>
</div>
<div id="diagnostic-tools" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Diagnostic Tools<a href="index.html#diagnostic-tools" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Forensic examiners often provide expert testimony in court cases.
As part of this testimony, an examiner is allowed to provide facts about the outcome of a forensic examination and their opinion about what the results mean.
A party to a court may challenge the examiner on the validity of the underlying scientific method or whether they interpreted the results correctly <span class="citation">(<a href="#ref-aafsArticle" role="doc-biblioref">American Academy of Forensic Sciences 2021</a>)</span>.
In these situations, examiners need to explain the process by which they reached an evidentiary conclusion to the fact finders of the case; namely, the judge or jury.
As algorithms are more often used in forensic examinations, the technical knowledge required to understand and explain an algorithm to lay-people has increased.
Indeed, even the most effective algorithms may be moot if an examiner can’t explain the algorithm in their testimony.
While in some cases the authors of the algorithm have been willing to provide testimony to establish the validity of the algorithm <span class="citation">(<a href="#ref-trueAlleleTestimony" role="doc-biblioref">Indiana County Court of Common Pleas 2009</a>)</span>, this will become less viable as algorithms become more prevalent.</p>
<p>The resources required to educate examiners on the theory and implementation of highly technical algorithms makes additional training seem currently implausible.
An alternative is to develop algorithms from the ground-up to be intuitive for examiners to understand and explain to others.
<em>Explainability</em> refers to the ability to identify the factors that contributed to the results of an algorithm <span class="citation">(<a href="#ref-Belle2021PrinciplesAP" role="doc-biblioref">Belle and Papantonis 2021</a>)</span>.
For example, understanding why a classifier predicted one class over another.
<em>Diagnostics</em> are tools to explain or justify the behavior of a model or algorithm in specific instances.
Myriad diagnostic tools exist to explain the results of an algorithm.
These range from identifying instances of the training set that illuminate how the model operates <span class="citation">(<a href="#ref-Deng2018" role="doc-biblioref">Deng 2018</a>)</span> to fitting more transparent models that accurately approximate the complex model <span class="citation">(<a href="#ref-Puiutta2020" role="doc-biblioref">Puiutta and Veith 2020</a>)</span> to explaining the behavior of the algorithm in a small region of interest of the prediction space <span class="citation">(<a href="#ref-LIME" role="doc-biblioref">Ribeiro, Singh, and Guestrin 2016</a>; <a href="#ref-Goode2021" role="doc-biblioref">Goode and Hofmann 2021</a>)</span>.
Many of these methods require additional technical knowledge to interpret these explanations.</p>
<div id="visual-diagnostics" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Visual Diagnostics<a href="index.html#visual-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A less technical approach is to use visualizations that facilitate understanding of model behavior.
Properly constructed visuals enable both exploratory data analysis and diagnostics <span class="citation">(<a href="#ref-Buja2009" role="doc-biblioref">Buja et al. 2009</a>)</span>, which are critical steps in the data analysis process for anticipating and assessing model fit.
Given that many of the procedures by which cartridge case evidence is captured, processed, and compared are based on image processing techniques, a visual diagnostic is an intuitive mode of explanation for researchers and lay-people alike.
As stated in <span class="citation">Cleveland (<a href="#ref-cleveland1994elements" role="doc-biblioref">1994</a>)</span>, “graphical methods tend to show data sets as a whole, allowing us to summarize the behavior and to study detail. This leads to much more thorough data analyses.”</p>
<p>Numerical statistics summarize the behavior of data, but miss the detail referenced in Cleveland’s quote <span class="citation">(<a href="#ref-telea2014data" role="doc-biblioref">Telea 2014</a>)</span>.
To illustrate this, consider the famous data sets from <span class="citation">(<a href="#ref-Anscombe1973" role="doc-biblioref">Anscombe 1973</a>)</span> known as Anscombe’s quartet.
The two variables in each data set are plotted against one another in Figure <a href="index.html#fig:anscombeQuartet">1.11</a>.
There are clear differences in the relationship between <code>x</code> and <code>y</code> across these four data sets.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:anscombeQuartet"></span>
<img src="thesis_files/figure-html/anscombeQuartet-1.png" alt="A visualization of Anscombe's quartet. Despite there being obvious differences between these four data sets, their summary statistics are nearly identical" width="480" />
<p class="caption">
Figure 1.11: A visualization of Anscombe’s quartet. Despite there being obvious differences between these four data sets, their summary statistics are nearly identical
</p>
</div>
<p>Despite these differences, Table <a href="index.html#tab:anscombeStats-html">1.1</a> shows that summary statistics, namely the first two moments, are identical.
This demonstrates that visual diagnostics can be more effective at uncovering data behavior than summary statistics (at least low-order moments).</p>
<table>
<caption>
<span id="tab:anscombeStats-html">Table 1.1: </span>Moments of the two variables in Anscombe’s quartet.
</caption>
<thead>
<tr>
<th style="text-align:right;">
Data Set
</th>
<th style="text-align:right;">
<span class="math inline">\(\bar{x}\)</span>
</th>
<th style="text-align:right;">
$S.D. <span class="math inline">\(x\)</span>
</th>
<th style="text-align:right;">
<span class="math inline">\(\bar{y}\)</span>
</th>
<th style="text-align:right;">
S.D. <span class="math inline">\(y\)</span>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
3.32
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
2.03
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
3.32
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
2.03
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
3.32
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
2.03
</td>
</tr>
<tr>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
3.32
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
2.03
</td>
</tr>
</tbody>
</table>
<p>Given the pivotal role that visual diagnostics play in the data analysis pipeline, we now consider best practices in creating data visualizations.
Human brains are wired for seeing patterns and differences, and for understanding spatial relationships from this <span class="citation">(<a href="#ref-telea2014data" role="doc-biblioref">Telea 2014</a>)</span>.
As such, an effective visual diagnostic or data visualization is one that conveys patterns quickly and easily, and with minimal scope for understanding.
Arising originally from a psychological theory of perception, the Gestalt Laws of Perceptual Organization <span class="citation">(<a href="#ref-Goldstein2016-un" role="doc-biblioref">Goldstein and Brockmole 2016</a>)</span> summarize important considerations when constructing statistical graphics.
The Gestalt laws are as follows:</p>
<ul>
<li><p><strong>Pragnanz - the law of simplicity:</strong> Every stimulus pattern is seen in such a away that the resulting structure is as simple as possible.</p></li>
<li><p><strong>Proximity:</strong> Things that are near each other appear to be grouped together.</p></li>
<li><p><strong>Good Continuation:</strong> Points that, when connected, result in straight or smoothly curving lines are seen as belonging together, and the lines tend to be seen in such a way as to follow the smoothest path.</p></li>
<li><p><strong>Similarity:</strong> Similar things appear to be grouped together.</p></li>
<li><p><strong>Common Region:</strong> Elements that are within the same region of space appear to be grouped together.</p></li>
<li><p><strong>Uniform Connectedness:</strong> A connected region of visual properties, such as the lightness, color, texture, or motion, is perceived as a single unit.</p></li>
<li><p><strong>Synchrony:</strong> Visual events that occur at the same time are perceived as belonging together.</p></li>
<li><p><strong>Common Fate:</strong> Things that are moving in the same direction appear to be grouped together.</p></li>
<li><p><strong>Familiarity:</strong> Things that form patterns that are familiar or meaningful are likely to become grouped together.</p></li>
</ul>
<p>These laws provide guidance on how to construct a visual that concisely conveys a pattern or difference in data.
For data visualization, additional laws include <span class="citation">(<a href="#ref-Midway2020" role="doc-biblioref">Midway 2020</a>)</span>:</p>
<ul>
<li><p><strong>Use and Effective Geometry:</strong> Choose a geometry (shape and features of a statistical graphic) that is appropriate to the data.</p></li>
<li><p><strong>Colors Always Mean Something:</strong> Colors in visuals can convey groupings or a range of values.</p></li>
</ul>
<p>Figure <a href="index.html#fig:chickweightExample">1.12</a> depicts a case study of the Gestalt principles in practice.
The plot shows the weight over time of chicks fed one of two experimental diets <span class="citation">(<a href="#ref-crowder1990analysis" role="doc-biblioref">Crowder and Hand 1990</a>)</span>.
Individual points represent the weight of a single chick on a particular day.
Connected points represent the weight for a single chick over time.
This is an example of using an effective geometry (point &amp; line graph to represent time series) along with the Gestalt law of Good Continuation.
We further apply the Gestalt law of Common Region to facet the data set into plots based on diet.
This implicitly communicates to the audience that the weights of two diet groups of chicks is expected to differ.
Indeed, appealing to the Gestalt law of Uniform Connectedness, the “motion” of the grouped time series suggests that chicks given Diet 2 tend to gain weight more rapidly than those given Diet 1.
This may suggest a particular modeling structure for these time series (e.g., diet fixed effect) or the need to assess the experimental design to ensure that the assumption that the chicks were randomly sampled from the same population is appropriate.
We see how such a plot can be used for both exploratory data analysis or as a post-hoc diagnostic tool.
Alternative to faceting, the time series from these two diet groups could be combined into a single plot and distinguished by color.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chickweightExample"></span>
<img src="thesis_files/figure-html/chickweightExample-1.png" alt="An example of a statistical graphic that uses the Gestalt Laws of Perceptual Organization to communicate data findings." width="480" />
<p class="caption">
Figure 1.12: An example of a statistical graphic that uses the Gestalt Laws of Perceptual Organization to communicate data findings.
</p>
</div>
<p>The R programming language <span class="citation">(<a href="#ref-Rlanguage" role="doc-biblioref">R Core Team 2017</a>)</span> provides a variety of tools to create visual diagnostics.
Among the most robust of these tools is the ggplot2 package <span class="citation">(<a href="#ref-ggplot2" role="doc-biblioref">Wickham 2009</a>)</span>.
This package extends the “Grammar of Graphics” introduced in <span class="citation">Wilkinson (<a href="#ref-Wilkinson2005" role="doc-biblioref">2005</a>)</span> to provide a user-friendly structure to create statistical graphics.
We use the <code>+</code> operator to “layer” features of a statistical graphic (e.g., elements, transformations, guides, labels) on a blank canvas.
Figure <a href="index.html#fig:ggplot2Example">1.13</a> along with the accompanying code chunk demonstrates how to create a residual plot from a simple linear regression using the ggplot2 package.
This visual diagnostic allows the analyst or audience to determine whether the homoscedasticity or linear form assumptions underlying simple linear regression are met.
For those willing to learn the “grammar,” the code used to create these statistical graphics can easily be re-used and tweaked to fit a specific application.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="index.html#cb1-1" aria-hidden="true" tabindex="-1"></a>lmFit <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="at">formula =</span> rating <span class="sc">~</span> complaints,<span class="at">data =</span> datasets<span class="sc">::</span>attitude)</span>
<span id="cb1-2"><a href="index.html#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="index.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-4"><a href="index.html#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="index.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">Complaints =</span> datasets<span class="sc">::</span>attitude<span class="sc">$</span>complaints,</span>
<span id="cb1-6"><a href="index.html#cb1-6" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Residuals =</span> lmFit<span class="sc">$</span>residuals)) <span class="sc">+</span></span>
<span id="cb1-7"><a href="index.html#cb1-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> Complaints,<span class="at">y =</span> Residuals)) <span class="sc">+</span></span>
<span id="cb1-8"><a href="index.html#cb1-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>,<span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb1-9"><a href="index.html#cb1-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;% in-favor of handling of employee complaints&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ggplot2Example"></span>
<img src="thesis_files/figure-html/ggplot2Example-1.png" alt="An example of using the ggplot2 package to construct a residual plot from a simple linear regression. The features of the statistical graphic are combined layer-by-layer using the + operator as we see in the accompanying code chunk." width="480" />
<p class="caption">
Figure 1.13: An example of using the ggplot2 package to construct a residual plot from a simple linear regression. The features of the statistical graphic are combined layer-by-layer using the + operator as we see in the accompanying code chunk.
</p>
</div>
<p>Properly constructed visual diagnostics provide the audience with a nuanced yet intuitive explanation of the behavior of a model or algorithm that summary diagnostic statistics may not convey.
Tools like the ggplot2 package provide a coherent, thorough infrastructure for creating such visual diagnostics.
However, the tools discussed thus far are useful for creating <em>static</em> visualizations.
In the next section, we discuss the benefits of making a visual diagnostic interactive to user input.</p>
</div>
<div id="interactive-diagnostics" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Interactive Diagnostics<a href="index.html#interactive-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Interactive diagnostic tools encourage both expert and lay users to engage with an analysis pipeline that otherwise may be technically or conceptually inaccessible.
Rather than answering a question posed by the author of a plot as a static plot does, such interactive diagnostic tools enable the audience to formulate and answer their own questions.
This leads to deeper engagement with the data <span class="citation">(<a href="#ref-telea2014data" role="doc-biblioref">Telea 2014</a>)</span>.
While the ggplot2 package eases the process of constructing visual diagnostics, software such as the shiny R package <span class="citation">(<a href="#ref-shiny" role="doc-biblioref">W. Chang et al. 2021</a>)</span> enables the consumer of the diagnostic to interact with the visualizations and underlying data.
The shiny package provides tools for using R to build web applications run on HTML, CSS, and JavaScript.
Among other functionality, these applications allow users to upload or create their own data, set parameters for an analysis, interact with visualizations or data sets (e.g., by hovering to display a tooltip), and export their analyses in various file formats <span class="citation">(<a href="#ref-Beeley2018-ci" role="doc-biblioref">Beeley and Sukhdeve 2018</a>)</span>.</p>
<!-- Figure \@ref(fig:IPDmada) \citep{wang2021ipdmada} shows a screenshot of the IPDmada shiny application that enables users to perform a meta-analysis of diagnostic test accuracy studies at the individual patient level (an individual patient data meta-analysis or IPD-MA) using a variety of statistical techniques. -->
<!-- As seen in \@ref(fig:IPDmada), the user can upload their own data csv file and select parameters that will enable the importing of the data. -->
<!-- The other tabs at the top of the application provide statistical tools to analyze the uploaded data. -->
<!-- This application is useful for researchers who are interested in analyzing diagnostic test accuracy data, yet do not necessarily have the coding skills to perform such an analysis in R themselves. -->
<!-- ```{r,fig.cap="\\label{fig:IPDmada} The IPDmada shiny application allows users to analyze individual patient data from a diagnostic test accuracy study using a variety of statistical techniques.",fig.height = 3,out.width=".7\\textwidth"} -->
<!-- knitr::include_graphics("images/IPDmadaShinyExample.png") -->
<!-- ``` -->
<p>Several recently-released software provide interactive diagnostic applications for firarms and toolmarks evidence.
Most notable of these software is the Virtual Comparison Microscopy application from Cadre Forensics.
In contrast to traditional Light Comparison Microscopy (LCM) that uses a comparison microscope, this software displays digital representations of the cartridge case surface on a computer screen.
Figure <a href="index.html#fig:topMatchAnnotationExample">1.14</a> shows a screenshot of comparing two cartridge case surfaces <span class="citation">(<a href="#ref-Chapnick2020" role="doc-biblioref">Chapnick et al. 2020</a>)</span>.
The functionality shown allows the user to manually annotate the surfaces of the two cartridge cases to identify similar and different markings.
For example, the user has selected a shade of blue to represent similarities between the two surfaces.
Conversely, shades of yellow and red represent differences between the two surfaces.
This sort of interactivity allows the user to customize their analysis more effectively than they could with a static visualization.
Further, we can save a history of the annotations for further analysis.
These annotations are a visual diagnostic tool that allows others to understand the specific patterns that the examiner looks at during an examination.
Another major benefit of using VCM over LCM is the ability to share scans over the internet rather than sending the physical specimen to another lab, which takes time and may damage the specimen.
<span class="citation">Duez et al. (<a href="#ref-Duez2017" role="doc-biblioref">2017</a>)</span>, <span class="citation">Chapnick et al. (<a href="#ref-Chapnick2020" role="doc-biblioref">2020</a>)</span>, and <span class="citation">Knowles, Hockey, and Marshall (<a href="#ref-Knowles2021" role="doc-biblioref">2021</a>)</span> all demonstrate that performing forensic examinations using such VCM technology yields equally, if not more, accurate conclusions compared to traditional LCM methods.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:topMatchAnnotationExample"></span>
<img src="images/topMatchSoftwareAnnotation.jpg" alt="A screenshot of the TopMatch-3D\texttrademark\ Virtual Comparison Microscopy software. In this example, similar and different markings on the cartridge case scans are manually annotated by the user using shades of blue and yellow/red, respectively." width="\textwidth" />
<p class="caption">
Figure 1.14: A screenshot of the TopMatch-3D Virtual Comparison Microscopy software. In this example, similar and different markings on the cartridge case scans are manually annotated by the user using shades of blue and yellow/red, respectively.
</p>
</div>
<p>In Chapter 3, we introduce a suite of static and interactive visual diagnostic tools.
We discuss how these visual diagnostic tools can be used by both researchers and practitioners to understand the behavior of automatic cartridge case comparison algorithms.</p>
</div>
</div>
<div id="automating-and-improving-the-cartridge-case-comparison-pipeline" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Automating and Improving the Cartridge Case Comparison Pipeline<a href="index.html#automating-and-improving-the-cartridge-case-comparison-pipeline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we review preliminaries needed to understand various sub-routines of the cartridge case comparison pipeline.</p>
<div id="image-processing-techniques" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Image Processing Techniques<a href="index.html#image-processing-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We first review image processing and computer vision algorithms used in cartridge case comparison algorithms.
Throughout this section, let <span class="math inline">\(A, B \in \mathbb{R}^{k \times k}\)</span> denote two images for <span class="math inline">\(k &gt; 0\)</span>.
We use lowercase letters and subscripts to denote a particular value of a matrix: <span class="math inline">\(a_{ij}\)</span> is the value in the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column, starting in the top-left corner, of matrix <span class="math inline">\(A\)</span>.
In our application, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> represent the surface matrices of two cartridge cases.</p>
<div id="image-registration" class="section level4 hasAnchor" number="1.4.1.1">
<h4><span class="header-section-number">1.4.1.1</span> Image Registration<a href="index.html#image-registration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Image registration</em> involves transforming image <span class="math inline">\(B\)</span> to align best with image <span class="math inline">\(A\)</span> (or vice versa) <span class="citation">(<a href="#ref-Brown1992" role="doc-biblioref">Brown 1992</a>)</span>.
In our application, this transformation is composed of a discrete translation <span class="math inline">\((m^*,n^*) \in \mathbb{Z}^2\)</span> and rotation by <span class="math inline">\(\theta^* \in [-180^\circ, 180^\circ]\)</span>.
Together, we refer to <span class="math inline">\((m^*,n^*,\theta^*)\)</span> as the “registration” of image <span class="math inline">\(B\)</span> to <span class="math inline">\(A\)</span>.
To determine the optimal registration, we calculate the <em>cross-correlation function</em> between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, denoted <span class="math inline">\((A \star B)\)</span>, which measures the similarity between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for every possible translation of <span class="math inline">\(B\)</span>.
The CCF between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is a 2D array of dimension <span class="math inline">\(2k - 1 \times 2k - 1\)</span> where the value of the <span class="math inline">\(m,n\)</span>-th element is given by:</p>
<p><span class="math display">\[
(a \star b)_{mn} = \sum_{i=1}^k \sum_{j=1}^k a_{mn} \cdot b_{i + m,j + n}
\]</span></p>
<p>where <span class="math inline">\(1 \leq m,n \leq 2k -1\)</span>.
The value <span class="math inline">\((a \star b)_{mn}\)</span> quantifies the similarity between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> after <span class="math inline">\(B\)</span> is translated <span class="math inline">\(m\)</span> elements horizontally and <span class="math inline">\(n\)</span> elements vertically.</p>
<p>A natural choice for aligning <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is the translation that maximizes the CCF.
However, we must also consider that <span class="math inline">\(B\)</span> may also need to be rotated to align optimally with <span class="math inline">\(A\)</span>.
We therefore compute the maximum CCF value across a range of rotations of <span class="math inline">\(B\)</span>.
Let <span class="math inline">\(B_\theta\)</span> denote <span class="math inline">\(B\)</span> rotated by an angle <span class="math inline">\(\theta\)</span> and <span class="math inline">\(b_{\theta_{mn}}\)</span> the <span class="math inline">\(m,n\)</span>-th element of <span class="math inline">\(B_\theta\)</span>. Then the estimated registration <span class="math inline">\((m^*, n^*, \theta^*)\)</span> is:</p>
<p><span class="math display">\[
(m^*, n^*, \theta^*) = \arg \max_{m,n,\theta} (a \star b_{\theta})_{mn}.
\]</span></p>
<p>In practice, we consider a discrete range of rotations <span class="math inline">\(\Theta \subset [-180^\circ, 180^\circ]\)</span>.
The registration procedure is given by:</p>
<ol style="list-style-type: decimal">
<li>For each <span class="math inline">\(\theta \in \pmb{\Theta}\)</span>:</li>
</ol>
<blockquote>
<p>1.1 Rotate image <span class="math inline">\(B\)</span> by <span class="math inline">\(\theta\)</span> to obtain <span class="math inline">\(B_\theta\)</span>.</p>
</blockquote>
<blockquote>
<p>1.2 Calculate the CCF between <span class="math inline">\(A\)</span> and <span class="math inline">\(B_\theta\)</span>.</p>
</blockquote>
<blockquote>
<p>1.3 Determine the translation <span class="math inline">\([m_{\theta}^*,n_{\theta}^*]\)</span> at which the CCF is maximized. Also, record the CCF value associated with this translation.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li><p>Across all <span class="math inline">\(\theta \in \Theta\)</span>, determine the rotation <span class="math inline">\(\theta^*\)</span> at which the largest CCF value is achieved.</p></li>
<li><p>The estimated registration consists of rotation <span class="math inline">\(\theta^*\)</span> and translation <span class="math inline">\([m^*,n^*] \equiv [m_{\theta^*}^*,n_{\theta^*}^*]\)</span>.</p></li>
</ol>
<p>In this instance, we refer to image <span class="math inline">\(A\)</span> as the “reference” and <span class="math inline">\(B\)</span>, the image aligned to the reference, as the “target.”
We represent the transformation to register <span class="math inline">\(B\)</span> to <span class="math inline">\(A\)</span> element-wise where the index <span class="math inline">\(i,j\)</span> maps to <span class="math inline">\(i^*,j^*\)</span> by:</p>
<p><span class="math display">\[
\begin{pmatrix} j^* \\ i^* \end{pmatrix} = \begin{pmatrix} n^* \\ m^* \end{pmatrix} + \begin{pmatrix} \cos(\theta^*) &amp; -\sin(\theta^*) \\ \sin(\theta^*) &amp; \cos(\theta^*) \end{pmatrix} \begin{pmatrix} j \\ i \end{pmatrix}.
\]</span></p>
<p>Under this transformation, the value <span class="math inline">\(b_{ij}\)</span> now occupies the the <span class="math inline">\(i^*,j^*\)</span>-th element.
In practice, we use <em>nearest-neighbor interpolation</em> meaning <span class="math inline">\(i^*\)</span> and <span class="math inline">\(j^*\)</span> are rounded to the nearest integer.</p>
<p>Based on the definition given above, the CCF is computationally taxing.
In image processing, it is common to use an implementation based on the Fast Fourier Transform <span class="citation">(<a href="#ref-Brown1992" role="doc-biblioref">Brown 1992</a>)</span>.
This implementation leverages the Cross-Correlation Theorem, which states that for images <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> the CCF can be expressed in terms of a frequency-domain pointwise product:</p>
<p><span class="math display">\[
(A \star B)[m,n] = \mathcal{F}^{-1}\left(\overline{\mathcal{F}(A)} \odot \mathcal{F}(B)\right)[m,n]
\]</span></p>
<p>where <span class="math inline">\(\mathcal{F}\)</span> and <span class="math inline">\(\mathcal{F}^{-1}\)</span> denote the discrete Fourier and inverse discrete Fourier transforms, respectively, and <span class="math inline">\(\overline{\mathcal{F}(A)}\)</span> denotes the complex conjugate <span class="citation">(<a href="#ref-fft_brigham" role="doc-biblioref">Brigham 1988</a>)</span>.
Because the product on the right-hand side is calculated pointwise, this result allows us to trade the moving sum computations from the definition of the CCF for two forward Fourier transformations, a pointwise product, and an inverse Fourier transformation.
The Fast Fourier Transform (FFT) algorithm can be used to reduce the computational load considerably.</p>
<p>Figure <a href="index.html#fig:ccfTranslationExample">1.15</a> shows an example of two images <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> of dimension <span class="math inline">\(100 \times 100\)</span> and <span class="math inline">\(21 \times 21\)</span>, respectively.
The white boxes in both of the images are of dimension <span class="math inline">\(10 \times 10\)</span>.
The box in image A is centered on index [30,50] while the box in image B is centered on index [11,11].
The right image shows the result of calculating the CCF using image <span class="math inline">\(A\)</span> as reference and <span class="math inline">\(B\)</span> as template.
The CCF achieves a maximum of 1, indicating a perfect match, at the translation value of <span class="math inline">\([m^*,n^*] = [22,-2]\)</span>.
This means that if image B were overlaid onto image A such that their center indices coincided, then image B would need to be shifted 22 units “up” and 2 units “left” to match perfectly with image A.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ccfTranslationExample"></span>
<img src="figures/unnamed-chunk-13-1.png" alt="(Left) A reference image $A$ and template image $B$ both featuring a white box of dimension $10 \times 10$. (Right) The cross-correlation function (CCF) between $A$ and $B$. The index at which the CCF is maximized represents the translation at which $A$ and $B$ are most similar." width="\textwidth" />
<p class="caption">
Figure 1.15: (Left) A reference image <span class="math inline">\(A\)</span> and template image <span class="math inline">\(B\)</span> both featuring a white box of dimension <span class="math inline">\(10 \times 10\)</span>. (Right) The cross-correlation function (CCF) between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. The index at which the CCF is maximized represents the translation at which <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are most similar.
</p>
</div>
</div>
<div id="gaussian-filters" class="section level4 hasAnchor" number="1.4.1.2">
<h4><span class="header-section-number">1.4.1.2</span> Gaussian Filters<a href="index.html#gaussian-filters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In image processing, a Gaussian filter (equivalently, blur or smoother) is a mathematical operator that imputes the values in an image using a locally-weighted sum of surrounding values.
We use a <em>lowpass</em> Gaussian filter to smooth the surface values of a cartridge case scan.
The weights are dictated according to the Gaussian function of a chosen standard deviation <span class="math inline">\(\sigma\)</span> given by:</p>
<p><span class="math display">\[
f(n,m;\sigma) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{1}{2\sigma^2}(n^2 + m^2)\right).
\]</span></p>
<p>It is common to populate a 2D array with the values of the Gaussian function treating the center index as the origin.
Such an array is called a <em>kernel</em>.
An example of a <span class="math inline">\(3 \times 3\)</span> Gaussian kernel <span class="math inline">\(K\)</span> with standard deviation <span class="math inline">\(\sigma = 1\)</span> is given below.</p>
<p><span class="math display">\[
K =
\begin{pmatrix}
0.075 &amp; 0.124 &amp; 0.075 \\
0.124 &amp; 0.204 &amp; 0.124 \\
0.075 &amp; 0.124 &amp; 0.075
\end{pmatrix}.
\]</span></p>
<p>For an image <span class="math inline">\(A\)</span> and Gaussian kernel <span class="math inline">\(K\)</span> with standard deviation <span class="math inline">\(\sigma\)</span>, the lowpass filtered version of <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(A_{lp,\sigma}\)</span> is given by:
<span class="math display">\[
A_{lp,\sigma}[m,n] = \mathcal{F}^{-1}\left(\mathcal{F}(A) \odot \mathcal{F}(K)\right)[m,n].
\]</span>
This operation, known as <em>convolution</em>, is extremely similar to the calculation of the CCF defined in the Image Registration section <span class="citation">(<a href="#ref-ISO1661021" role="doc-biblioref"><span>“<span class="nocase">Geometrical product specifications (GPS) - Filtration - Part 61: Linear areal filters: Gaussian filters</span>”</span> 2011</a>)</span>.</p>
<p>From left to right, Figure <a href="index.html#fig:gaussianFilterExample">1.16</a> shows an image <span class="math inline">\(A\)</span> of a box injected with Gaussian noise (noise standard deviation <span class="math inline">\(\sigma_n = 0.3\)</span>) followed by the application of various Gaussian filters.
In the middle of Figure <a href="index.html#fig:gaussianFilterExample">1.16</a>, we see that the lowpass filter (kernel standard deviation <span class="math inline">\(\sigma_k = 2\)</span>) recovers some of the definition of the box by “smoothing” some of the Gaussian noise.</p>
<p>If a lowpass filter smooths values in an image, then a <em>highpass</em> filter performs a “sharpening” operation.
For an image <span class="math inline">\(A\)</span> and kernel standard deviation <span class="math inline">\(\sigma\)</span>, the highpass filtered version <span class="math inline">\(A_{hp}\)</span> can be defined as:</p>
<p><span class="math display">\[
A_{hp,\sigma} = A - A_{lp,\sigma}.
\]</span></p>
<p>The highpass filter therefore removes larger-scale (smooth) structure from an image and retains high-frequency structure such as noise or edges.
The fourth facet of Figure <a href="index.html#fig:gaussianFilterExample">1.16</a> shows a highpass-filtered image <span class="math inline">\(A\)</span> .
The smooth interior of the box is effectively removed from the image while the edges are preserved.</p>
<p>Finally, a <em>bandpass</em> Gaussian filter simultaneously performs highpass sharpening and lowpass smoothing operations.
Generally, the standard deviation of the highpass kernel will be considerably larger than that of the lowpass kernel.
This leads to retaining sharp edges while also reducing noise.
An example of a bandpass filtered image <span class="math inline">\(A\)</span> is shown in Figure <a href="index.html#fig:gaussianFilterExample">1.16</a>.
The edges of the box are better-preserved compared to the lowpass filter figure while the interior of the box is better-preserved compared to the highpass filter figure.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gaussianFilterExample"></span>
<img src="figures/unnamed-chunk-14-1.png" alt="An image $A$ of a box with Gaussian noise undergoing a lowpass, highpass, and bandpass filter operation." width="\textwidth" />
<p class="caption">
Figure 1.16: An image <span class="math inline">\(A\)</span> of a box with Gaussian noise undergoing a lowpass, highpass, and bandpass filter operation.
</p>
</div>
<p>Variations on the standard Gaussian filter include the “robust” Gaussian regression filter.
This filter fluctuates between a filter step, which applies a Gaussian filter, and outlier step, which identifies and omits outlier observations from the next filter step <span class="citation">(<a href="#ref-robustFilter" role="doc-biblioref">Brinkman and Bodschwinna 2003b</a>)</span>.
Another alternative, the “edge preserving” filter, adapts the kernel weights when approaching the boundary of an image to mitigate so-called <em>boundary effects</em> <span class="citation">(<a href="#ref-Aurich1995" role="doc-biblioref">Aurich and Weule 1995</a>)</span>.</p>
<p>We use Gaussian filters to change the values on the interior of a cartridge case surface to better emphasize breech face impressions.
In the next section, we discuss applying morphological operations to change the values on the edges of a cartridge case surface.</p>
</div>
<div id="morphological-operations" class="section level4 hasAnchor" number="1.4.1.3">
<h4><span class="header-section-number">1.4.1.3</span> Morphological Operations<a href="index.html#morphological-operations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mathematical morphology refers to a theory and collection of image processing techniques for geometrical structures <span class="citation">(<a href="#ref-Haralick1987" role="doc-biblioref">Haralick, Sternberg, and Zhuang 1987</a>)</span>.
In our application, these geometrical structures are cartridge case scans; specifically, binarized versions of these scans representing whether a particular pixel contains part of the cartridge case surface.
We discuss this in greater detail in Chapter 2.</p>
<p>Two fundamental operations in mathematical morphology are <em>dilation</em> and <em>erosion</em> <span class="citation">(<a href="#ref-Haralick1987" role="doc-biblioref">Haralick, Sternberg, and Zhuang 1987</a>)</span>.
For our purposes, these are both set operations on black and white, encoded as 0 and 1 respectively, images.
We call the set of black and white pixels the “background” and “foreground” of the image, respectively.
For an image <span class="math inline">\(A\)</span>, let <span class="math inline">\(W = \{(m,n) : A_{mn} = 1\}\)</span> denote the foreground of <span class="math inline">\(A\)</span>.
An example of a <span class="math inline">\(7 \times 7\)</span> binary image <span class="math inline">\(A\)</span> with <span class="math inline">\(W = \{(3,3),(3,4),(3,5),(4,3),(4,4),(4,5),(5,3),(5,4),(5,5)\}\)</span> is given below.</p>
<p><span class="math display">\[
A =
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\]</span></p>
<p>A <em>structuring element</em> is a second, typically small, array <span class="math inline">\(B\)</span> of ones that affects the amount of dilation or erosion applied to <span class="math inline">\(W\)</span> within <span class="math inline">\(A\)</span>.
For simplicity, the indexing of the structuring element uses the center element as the index origin.
For example, a <span class="math inline">\(3 \times 3\)</span> structuring element is given by <span class="math inline">\(B = \{(-1,-1),(-1,0),(-1,1),(-1,0),(0,0),(0,1),(1,-1),(1,0),(1,1)\}\)</span> or visually:</p>
<p><span class="math display">\[
B =
\begin{pmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1
\end{pmatrix}
\]</span></p>
<p>As the name suggests, a <em>dilation</em> grows the region <span class="math inline">\(W\)</span> within image <span class="math inline">\(A\)</span> by replacing 0-valued pixels that border <span class="math inline">\(W\)</span> with 1.
The structuring element <span class="math inline">\(B\)</span> dictates which pixels are replaced with 1.
We define the dilation of <span class="math inline">\(W\)</span> by <span class="math inline">\(B\)</span>, denoted <span class="math inline">\(W \oplus B\)</span>, element-wise:</p>
<p><span class="math display">\[
W \oplus B = \{[m,n] \in A : [m,n] = [i + k,j + l] \text{ for } [i,j] \in W \text{ and } [k,l] \in B\}
\]</span></p>
<p>In our example,
<span class="math display">\[W \oplus B = \{[3,2],[3,3],[3,4],[3,5],[3,6],[4,2],[4,3],[4,4],[4,5],[4,6],[5,2],[5,3],[5,4],[5,5],[5,6]\}\]</span>
or visually:</p>
<p><span class="math display">\[
W \oplus B =
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\]</span></p>
<p>The dilation operation by this <span class="math inline">\(B\)</span> has the effect of growing the region <span class="math inline">\(W\)</span> inside of <span class="math inline">\(A\)</span> by one index in each direction.</p>
<p>In contrast, <em>erosion</em> has the effect of shrinking <span class="math inline">\(W\)</span>.
The erosion of <span class="math inline">\(W\)</span> by <span class="math inline">\(B\)</span> is:</p>
<p><span class="math display">\[
A \ominus B = \{[m,n] \in A: [m,n] + [k,l] \in A \text{ for every } [k,l] \in B\}.
\]</span></p>
<p>Using the same example as above, <span class="math inline">\(W \ominus B = \{[3,3]\}\)</span> or visually:</p>
<p><span class="math display">\[
W \ominus B =
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\]</span></p>
<p>Erosion by this <span class="math inline">\(B\)</span> shrinks the region <span class="math inline">\(W\)</span> in <span class="math inline">\(A\)</span> by one index in each direction.</p>
<p>Figure <a href="index.html#fig:dilationErosionExample">1.17</a> shows our example represented using black and white pixels.
In practice, the foreground set <span class="math inline">\(W\)</span> may contain disconnected regions to which dilation or erosion can be independently applied.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dilationErosionExample"></span>
<img src="figures/unnamed-chunk-15-1.png" alt="A $7 \times 7$ image $A$ featuring a $3 \times 3$ box undergoing dilation and erosion by a $3 \times 3$ structuring element $B$." width="\textwidth" />
<p class="caption">
Figure 1.17: A <span class="math inline">\(7 \times 7\)</span> image <span class="math inline">\(A\)</span> featuring a <span class="math inline">\(3 \times 3\)</span> box undergoing dilation and erosion by a <span class="math inline">\(3 \times 3\)</span> structuring element <span class="math inline">\(B\)</span>.
</p>
</div>
<p>This concludes our review of image processing techniques we use in subsequent chapters.
Next, we discuss a clustering procedure used in Chapter 4 to calculate similarity features.</p>
</div>
</div>
<div id="density-based-spatial-clustering-of-applications-with-noise" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Density-Based Spatial Clustering of Applications with Noise<a href="index.html#density-based-spatial-clustering-of-applications-with-noise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is a clustering procedure that assigns observations to clusters if they are in a region of high observation density <span class="citation">(<a href="#ref-Ester1996" role="doc-biblioref">Ester et al. 1996</a>)</span>.
As we will see, the DBSCAN algorithm does not require the user to pre-specify the number of expected clusters as is required in common clustering algorithms like K-means.
Further, the algorithm does not require that all points be assigned to a cluster.
<!-- Points not assigned to a cluster at the end of the algorithm are called "noise points." --></p>
<p>Let <span class="math inline">\(D\)</span> represent a <span class="math inline">\(n \times p\)</span> data set (<span class="math inline">\(n\)</span> observations, each of dimension <span class="math inline">\(p\)</span>) and let <span class="math inline">\(x,y,z \in D\)</span> denote three observations.
The DBSCAN algorithm relies on the notion of <span class="math inline">\(\epsilon\)</span>-neighborhoods.
Given some neighborhood radius <span class="math inline">\(\epsilon \in \mathbb{R}\)</span> and distance metric <span class="math inline">\(d\)</span>, <span class="math inline">\(y\)</span> is in the <span class="math inline">\(\epsilon\)</span>-neighborhood of <span class="math inline">\(x\)</span> if <span class="math inline">\(d(x,y) \leq \epsilon\)</span>.
The <em><span class="math inline">\(\epsilon\)</span>-neighborhood</em> of <span class="math inline">\(x\)</span> is defined as the set <span class="math inline">\(N_{\epsilon}(x) = \{y \in D : d(x,y) \leq \epsilon\}\)</span>.
Given a minimum number of points <span class="math inline">\(Minpts \in \mathbb{N}\)</span> (notation used in <span class="citation">(<a href="#ref-Ester1996" role="doc-biblioref">Ester et al. 1996</a>)</span>), observation <span class="math inline">\(x\)</span> is called a <em>core point</em> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> if <span class="math inline">\(|N_{\epsilon}(x)| \geq Minpts\)</span>.
Core points are treated as the “seeds” of clusters in the DBSCAN algorithm.
The user must select values of <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span>.</p>
<p>Figure <a href="index.html#fig:epsNeighborhoodExample">1.18</a> shows an example of a data set <span class="math inline">\(D \in \mathbb{R}^{10 \times 2}\)</span>.
We represent the 10 observations in <span class="math inline">\(D\)</span> on the Cartesian plane.
An <span class="math inline">\(\epsilon\)</span>-neighborhood using the Euclidean distance metric and <span class="math inline">\(\epsilon = 3\)</span> is drawn around an observation <span class="math inline">\(x\)</span> located at <span class="math inline">\((3,2)\)</span>.
Points inside the circle are neighbors of <span class="math inline">\(x\)</span>.
If, for example, <span class="math inline">\(Minpts = 2\)</span>, then <span class="math inline">\(x\)</span> would be considered a core point.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:epsNeighborhoodExample"></span>
<img src="figures/dbscanExample_corePoint.png" alt="An $\epsilon$-neighborhood around a observation located at $(3,2)$ for $\epsilon = 3$. Points are colored blue if they are neighbors to this observation and red otherwise." width=".8\textwidth" />
<p class="caption">
Figure 1.18: An <span class="math inline">\(\epsilon\)</span>-neighborhood around a observation located at <span class="math inline">\((3,2)\)</span> for <span class="math inline">\(\epsilon = 3\)</span>. Points are colored blue if they are neighbors to this observation and red otherwise.
</p>
</div>
<p><span class="citation">Ester et al. (<a href="#ref-Ester1996" role="doc-biblioref">1996</a>)</span> introduces two relational notions, <em>density-reachability</em> and <em>density-connectivity</em>, to identify regions of high observation density.
A point <span class="math inline">\(y\)</span> is <em>directly density-reachable</em> to a point <span class="math inline">\(x\)</span> if <span class="math inline">\(x\)</span> is a core point and <span class="math inline">\(y \in N_{\epsilon}(x)\)</span>.
In Figure <a href="index.html#fig:epsNeighborhoodExample">1.18</a>, the observation located at <span class="math inline">\((1,0)\)</span> is directly density-reachable to the observation located at <span class="math inline">\((3,2)\)</span>.
More broadly, a point <span class="math inline">\(x_m\)</span> is <em>density-reachable</em> to a point <span class="math inline">\(x_1\)</span> if there exists a chain of observations <span class="math inline">\(x_1,x_2,...,x_{m-1},x_m\)</span> such that <span class="math inline">\(x_{i+1}\)</span> is directly density-reachable from <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i = 1,...,n\)</span>.
Density reachability captures the notion of “neighbors of neighbors” for core points.
The DBSCAN algorithm agglomerates density-reachable points into single clusters.</p>
<p>Figure <a href="index.html#fig:densityReachableExample">1.19</a> highlights three points <span class="math inline">\((1,0), (3,2)\)</span>, and <span class="math inline">\((4,4)\)</span>.
Using <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>, we see that all three of these points are core points.
Further, the points at <span class="math inline">\((1,0)\)</span> and <span class="math inline">\((4,4)\)</span> are density-reachable by way of the point <span class="math inline">\((3,2)\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densityReachableExample"></span>
<img src="figures/dbscanExample_densityReachable.png" alt="An example of three points that are density-reachable with respect to $\epsilon = 3$ and $Minpts = 2$." width=".8\textwidth" />
<p class="caption">
Figure 1.19: An example of three points that are density-reachable with respect to <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>.
</p>
</div>
<p>Finally, a point <span class="math inline">\(y\)</span> is <em>density-connected</em> to a point <span class="math inline">\(x\)</span> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> if there exists a point <span class="math inline">\(z\)</span> such that both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are density-reachable to <span class="math inline">\(z\)</span> (with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span>).
While density-reachability requires that all points in-between two points also be core points, density-connectivity extends the notion of “neighbors of neighbors” to include points that are merely within the neighborhood of density-reachable points.
Figure <a href="index.html#fig:densityConnectedExample">1.20</a> illustrates how the points located at <span class="math inline">\((4,7)\)</span> and <span class="math inline">\((0,-2)\)</span> are density-connected but not density-reachable.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:densityConnectedExample"></span>
<img src="figures/dbscanExample_densityConnected.png" alt="An example of two points that are density-connected, but not density-reachable, with respect to $\epsilon = 3$ and $Minpts = 2$." width=".8\textwidth" />
<p class="caption">
Figure 1.20: An example of two points that are density-connected, but not density-reachable, with respect to <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>.
</p>
</div>
<p>A <em>cluster</em> <span class="math inline">\(C \subset D\)</span> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> satisfies the following conditions:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\forall x,y\)</span>: if <span class="math inline">\(x \in C\)</span> and <span class="math inline">\(y\)</span> is density-reachable from <span class="math inline">\(x\)</span> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span>, then <span class="math inline">\(y \in C\)</span>.</p></li>
<li><p><span class="math inline">\(\forall x,y \in C\)</span>: <span class="math inline">\(x\)</span> is density-connected to <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span>.</p></li>
</ol>
<p>For a data set <span class="math inline">\(D\)</span>, the DBSCAN algorithm determines clusters based on the above definition.
Points not assigned to a cluster are classified as <em>noise points</em>.
The algorithm halts once all points are assigned to a cluster or classified as noise.</p>
<p>Figure <a href="index.html#fig:dbscanResultExample">1.21</a> shows the labels return by DBSCAN for the example considered above with respect to <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>.
The algorithm finds a cluster of seven points, colored blue, and classifies three points as noise, colored red.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dbscanResultExample"></span>
<img src="figures/dbscanExample_clusters.png" alt="Cluster labeling for 10 data points using the DBSCAN algorithm with parameters $\epsilon = 3$ and $Minpts = 2$. Seven points are assigned to a single cluster and the remaining three are classified as noise." width=".8\textwidth" />
<p class="caption">
Figure 1.21: Cluster labeling for 10 data points using the DBSCAN algorithm with parameters <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>. Seven points are assigned to a single cluster and the remaining three are classified as noise.
</p>
</div>
<!-- ### Features Based on Visual Diagnostics -->
<!-- Much of the "explainable" algorithms literature focuses on black-box machine learning algorithms such as Random Forests or Multi-layer Neural Networks. -->
<!-- Less focus is placed on constructing explainable features. -->
<!-- Feature selection and engineering is a critical, often time-intensive step in the data analysis process that isn't often -->
<!-- The visual diagnostic tools discussed in Chapter 4 are used to develop a set of features. -->
<!-- By definition, these features are human-interpretable unlike, for example, features that are calculated in the convolution layer of a convolutional neural network. -->
<!-- The interpretability of these features imply that they can be explained to forensic examiners or lay-people. -->
<!-- This will make it easier to introduce such methods into forensic labs and court rooms. -->
</div>
<div id="implementation-considerations" class="section level3 hasAnchor" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Implementation Considerations<a href="index.html#implementation-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the computational sciences, it is one thing to publish code along with research findings.
Publicly-available code and data make results accessible in terms of acquisition.
It is much more challenging to make code <em>conceptually</em> accessible to others.
The former allows others to obtain the same results under the same programming conditions while the latter empowers others to actually engage with and potentially improve upon individual pieces of the algorithm.
In any data analysis pipeline, the procedural details may be obscured as the goals of the analysis become more sophisticated.
<!-- This is helpful neither for the individual performing the analysis nor for any consumer of the results. -->
It is therefore worthwhile to design tools that make the data analysis process both easier to implement and understand <span class="citation">(<a href="#ref-tidy-data" role="doc-biblioref">Wickham 2014</a>)</span>.</p>
<p>Our implementation of the cartridge case comparison pipeline adheres to the “tidy” principles of design <span class="citation">(<a href="#ref-tidyverse" role="doc-biblioref">Wickham et al. 2019</a>)</span>.
The “tidyverse” is a collection of R packages that share an underlying design philosophy and structure.
Knowledge and skills learned while using one tidy package can be applied to others.
The four principles of a tidy API are:</p>
<ol style="list-style-type: decimal">
<li><em>Reuse existing data structures.</em></li>
</ol>
<blockquote>
<p>For example, users do not need to learn new data attributes or compatible functions if a package reuses existing data structures.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li><em>Compose simple functions with the pipe.</em></li>
</ol>
<blockquote>
<p>The pipe operator allows the output of one function to be passed as input to another without assigning a new variable. We incrementally transform data as they move from one function to another rather than drastically transforming the data in a single call.</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li><em>Embrace functional programming.</em></li>
</ol>
<blockquote>
<p>The functional programming paradigm encourages immutability of objects, meaning data passed as input to a function are not changed.
Rather, the function makes a copy of the input data, manipulates the copy, and returns the transformed copy as output.
This differs from an “object-oriented” paradigm where functions have the ability to implicitly rewrite or change the state of the original data.
It is easier to reason about a function that actually returns an object as output than one that changes the input object as a “side effect.”</p>
</blockquote>
<ol start="4" style="list-style-type: decimal">
<li><em>Design for humans.</em></li>
</ol>
<blockquote>
<p>Designing a package for humans largely comes down to using consistent, explicit, and descriptive naming schemes for objects and functions.</p>
</blockquote>
<p>Conceptualizing the cartridge case comparison procedure as a pipeline makes it easier to understand.
We go one step further by actually implementing the procedure as a sequence of algorithms that are programatically connected together in the R statistical programming language <span class="citation">(<a href="#ref-Rlanguage" role="doc-biblioref">R Core Team 2017</a>)</span>.
In particular, we utilize the pipe operator available from the magrittr R package <span class="citation">(<a href="#ref-magrittr" role="doc-biblioref">Bache and Wickham 2022</a>)</span>.
The pipe operator allows the user to think intuitively in terms of verbs applied to the data.
Table <a href="index.html#tab:pipelineTable-html">1.2</a> illustrates two pipelines that utilize the pipe operator.
The left-hand example shows how an R data frame is manipulated by piping it between functions from the dplyr package.
Functions like <code>group_by</code>, <code>summarize</code>, and <code>filter</code> are simple building blocks strung together to create complicated workflows.
The right-hand example similarly illustrates a cartridge case object passing through a comparison pipeline.
While the full comparison procedure is complex, the modularization to the <code>preProcess_</code>, <code>comparison_</code>, and <code>decision_</code> steps, which can further be broken-down into simpler functions, renders the process more understandable and flexible for the user.</p>
<table>
<caption>
<span id="tab:pipelineTable-html">Table 1.2: </span>Two examples of data analysis workflows that utilize the pipe operator. The left side shows a data frame manipulation while the right side shows a comparison of two cartridge cases.
</caption>
<thead>
<tr>
<th style="text-align:left;">
Data Frame Manipulation Example
</th>
<th style="text-align:left;">
Cartridge Case Comparison Example
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\texttt{dataFrame %&gt;%}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\texttt{cartridgeCase1 %&gt;%}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\ \ \texttt{group_by(category) %&gt;%}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\ \ \texttt{preProcess_func(params1) %&gt;%}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\ \ \texttt{summarize(x = summary(var)) %&gt;%}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\ \ \texttt{comparison_func(cartridgeCase2,params2) %&gt;%}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;">
<span class="math inline">\(\ \ \texttt{filter(x &gt; 0) ...}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\ \ \texttt{decision_func(params3) ...}\)</span>
</td>
</tr>
</tbody>
</table>
<p>Adherence to tidy principles makes it easier to engage with and understand the overall data analysis pipeline.
In our application it also enables experimentation by making it easy to change one step of the pipeline and measure the downstream effects <span class="citation">(<a href="#ref-reproducibleScience" role="doc-biblioref">Zimmerman et al. 2019</a>)</span>.
Each step of the cartridge case comparison pipeline requires the user to define parameters.
These can range from minor to substantial, such as choosing the standard deviation used in a Gaussian filter to choosing the algorithm used to calculate a similarity score.
So far, no consensus exists for the “best” parameter settings.
A large amount of experimentation is yet required to establish these parameters.
A tidy implementation of the cartridge case comparison pipeline allows more people to engage in the validation and improvement of the procedure.</p>
<p>Figure <a href="index.html#fig:taiEddyPreprocess">1.22</a>, Figure <a href="index.html#fig:ricePreprocess">1.23</a>, Figure <a href="index.html#fig:handwriterPreprocess">1.24</a>, and Figure <a href="index.html#fig:cmcRPreprocess">1.25</a> illustrate how various forensic comparison algorithms use a modularized structure to conceptualize their pre-processing procedures.
In each figure, a sequence of modular procedures are applied to a piece of evidence.
Figure <a href="index.html#fig:taiEddyPreprocess">1.22</a> shows morphological and image processing procedures applied to a 2D image of a cartridge case to remove the firing pin region <span class="citation">(<a href="#ref-tai_fully_2018" role="doc-biblioref">Tai and Eddy 2018</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:taiEddyPreprocess"></span>
<img src="images/taiEddyPreprocess.png" alt="A pre-processing procedure applied to a 2D image of a cartridge case to identify the firing pin impression. The procedure results in a 2D image of a cartridge case without the firing pin impression region." width=".7\textwidth" />
<p class="caption">
Figure 1.22: A pre-processing procedure applied to a 2D image of a cartridge case to identify the firing pin impression. The procedure results in a 2D image of a cartridge case without the firing pin impression region.
</p>
</div>
<p>Figure <a href="index.html#fig:ricePreprocess">1.23</a> shows the procedure by which a 2D “signature” of a bullet scan is extracted from a 3D topographical scan <span class="citation">(<a href="#ref-Rice2020" role="doc-biblioref">Rice 2020</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ricePreprocess"></span>
<img src="images/riceBulletPreprocessDiagram.png" alt="A pre-processing procedure for extracting 2D bullet \`\`signatures&quot; from a 3D topographic bullet scan. The procedure results in an ordered sequence of values representing the local variations in the surface of the bullet." width=".8\textwidth" />
<p class="caption">
Figure 1.23: A pre-processing procedure for extracting 2D bullet ``signatures” from a 3D topographic bullet scan. The procedure results in an ordered sequence of values representing the local variations in the surface of the bullet.
</p>
</div>
<p>Figure <a href="index.html#fig:handwriterPreprocess">1.24</a> shows how an image of the written word “csafe” is processed using the handwriter R package to break the word into individual <em>graphemes</em> that can be further processed <span class="citation">(<a href="#ref-handwriter" role="doc-biblioref">Berry, Taylor, and Baez-Santiago 2021</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:handwriterPreprocess"></span>
<img src="images/handwriterPreprocessDiagram.png" alt="A pre-processing procedure applied to an image of the handwritten word &quot;csafe.&quot; The procedure results in a skeletonized version of the word that has been separated into graphemes as represented by orange nodes." width=".35\textwidth" />
<p class="caption">
Figure 1.24: A pre-processing procedure applied to an image of the handwritten word “csafe.” The procedure results in a skeletonized version of the word that has been separated into graphemes as represented by orange nodes.
</p>
</div>
<p>Finally, Figure <a href="index.html#fig:cmcRPreprocess">1.25</a> shows a 3D topographical cartridge case scan undergoing various procedures to isolate and highlight the breech face impressions.
These procedures are discussed in greater detail in Chapter 2.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cmcRPreprocess"></span>
<img src="figures/preProcessPlots.png" alt="A cartridge case undergoing various pre-processing steps. The procedure results in a cartridge case scan in which the breech face impressions have been segmented and highlighted." width="\textwidth" />
<p class="caption">
Figure 1.25: A cartridge case undergoing various pre-processing steps. The procedure results in a cartridge case scan in which the breech face impressions have been segmented and highlighted.
</p>
</div>
<p>By breaking the broader pre-processing step into modularized pieces, we can devise other arrangements of these pre-processing procedures that may improve the segmenting or emphasizing of the region of interest.
The modularity of the pipeline makes it easier to understand what the algorithm is doing “under the hood.”
A genuine modular implementation enables others to experiment with alternative versions of the pipeline, thus accelerating discovery and improvement.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-calibersExplained" class="csl-entry">
30 Magazine Clip. 2017. <span>“Calibers Explained.”</span> <em>30 Magazine Clip</em>.
</div>
<div id="ref-AFTE1992" class="csl-entry">
AFTE Criteria for Identification Committee. 1992. <span>“Theory of Identification, Range Striae Comparison Reports and Modified Glossary Definitions.”</span> <em>AFTE Journal</em> 24 (3): 336–40.
</div>
<div id="ref-aafsArticle" class="csl-entry">
American Academy of Forensic Sciences. 2021. <span>“What Is Forensic Science?”</span> American Academy of Forensic Sciences. <a href="https://www.aafs.org/careers-forensic-science/what-forensic-science">https://www.aafs.org/careers-forensic-science/what-forensic-science</a>.
</div>
<div id="ref-Anscombe1973" class="csl-entry">
Anscombe, F. J. 1973. <span>“Graphs in Statistical Analysis.”</span> <em>The American Statistician</em> 27 (1): 17. <a href="https://doi.org/10.2307/2682899">https://doi.org/10.2307/2682899</a>.
</div>
<div id="ref-Aurich1995" class="csl-entry">
Aurich, Volker, and Jörg Weule. 1995. <span>“Non-Linear Gaussian Filters Performing Edge Preserving Diffusion.”</span> In <em>Informatik Aktuell</em>, 538–45. Springer Berlin Heidelberg. <a href="https://doi.org/10.1007/978-3-642-79980-8_63">https://doi.org/10.1007/978-3-642-79980-8_63</a>.
</div>
<div id="ref-magrittr" class="csl-entry">
Bache, Stefan Milton, and Hadley Wickham. 2022. <em>Magrittr: A Forward-Pipe Operator for r</em>. <a href="https://CRAN.R-project.org/package=magrittr">https://CRAN.R-project.org/package=magrittr</a>.
</div>
<div id="ref-Baldwin2014" class="csl-entry">
Baldwin, David P., Stanley J. Bajic, Max Morris, and Daniel Zamzow. 2014. <span>“A Study of False-Positive and False-Negative Error Rates in Cartridge Case Comparisons.”</span> Defense Technical Information Center. <a href="https://doi.org/10.21236/ada611807">https://doi.org/10.21236/ada611807</a>.
</div>
<div id="ref-Beeley2018-ci" class="csl-entry">
Beeley, Chris, and Shitalkumar R Sukhdeve. 2018. <em>Web Application Development with <span>R</span> Using Shiny</em>. 3rd ed. Birmingham, England: Packt Publishing.
</div>
<div id="ref-Belle2021PrinciplesAP" class="csl-entry">
Belle, Vaishak, and Ioannis Papantonis. 2021. <span>“Principles and Practice of Explainable Machine Learning.”</span> <em>Frontiers in Big Data</em> 4.
</div>
<div id="ref-handwriter" class="csl-entry">
Berry, Nick, James Taylor, and Felix Baez-Santiago. 2021. <em>Handwriter: Handwriting Analysis in r</em>. <a href="https://CRAN.R-project.org/package=handwriter">https://CRAN.R-project.org/package=handwriter</a>.
</div>
<div id="ref-fft_brigham" class="csl-entry">
Brigham, E. Oran. 1988. <em>The Fast Fourier Transform and Its Applications</em>. USA: Prentice-Hall, Inc.
</div>
<div id="ref-robustFilter" class="csl-entry">
———. 2003b. <em>Advanced Techniques for Assessment Surface Topography</em>. Elsevier. <a href="https://doi.org/10.1016/b978-1-903996-11-9.x5000-2">https://doi.org/10.1016/b978-1-903996-11-9.x5000-2</a>.
</div>
<div id="ref-Brown1992" class="csl-entry">
Brown, Lisa Gottesfeld. 1992. <span>“A Survey of Image Registration Techniques.”</span> <em><span>ACM</span> Computing Surveys</em> 24 (4): 325–76. <a href="https://doi.org/10.1145/146370.146374">https://doi.org/10.1145/146370.146374</a>.
</div>
<div id="ref-Buja2009" class="csl-entry">
Buja, Andreas, Dianne Cook, Heike Hofmann, Michael Lawrence, Eun-Kyung Lee, Deborah F. Swayne, and Hadley Wickham. 2009. <span>“Statistical Inference for Exploratory Data Analysis and Model Diagnostics.”</span> <em>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> 367 (1906): 4361–83. <a href="https://doi.org/10.1098/rsta.2009.0120">https://doi.org/10.1098/rsta.2009.0120</a>.
</div>
<div id="ref-topmatchFlyer" class="csl-entry">
Cadre Forensics. 2019. <span>“Top Match-3d High Capacity: 3d Imaging and Analysis System for Firearm Forensics.”</span> Cadre Forensics.
</div>
<div id="ref-CChang2022" class="csl-entry">
Chang, Andrew C., and Phillip Li. 2022. <span>“Is Economics Research Replicable? Sixty Published Papers from Thirteen Journals Say <span>“</span>Often Not<span>”</span>.”</span> <em>Critical Finance Review</em> 11 (1): 185–206. <a href="https://doi.org/10.1561/104.00000053">https://doi.org/10.1561/104.00000053</a>.
</div>
<div id="ref-shiny" class="csl-entry">
Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2021. <em>Shiny: Web Application Framework for r</em>. <a href="https://CRAN.R-project.org/package=shiny">https://CRAN.R-project.org/package=shiny</a>.
</div>
<div id="ref-Chapnick2020" class="csl-entry">
Chapnick, Chad, Todd J. Weller, Pierre Duez, Eric Meschke, John Marshall, and Ryan Lilien. 2020. <span>“Results of the 3d Virtual Comparison Microscopy Error Rate (<span>VCMER</span>) Study for Firearm Forensics.”</span> <em>Journal of Forensic Sciences</em> 66 (2): 557–70. <a href="https://doi.org/10.1111/1556-4029.14602">https://doi.org/10.1111/1556-4029.14602</a>.
</div>
<div id="ref-chen_convergence_2017" class="csl-entry">
Chen, Zhe, John Song, Wei Chu, Johannes A. Soons, and Xuezeng Zhao. 2017. <span>“A Convergence Algorithm for Correlation of Breech Face Images Based on the Congruent Matching Cells (<span>CMC</span>) Method.”</span> <em>Forensic Science International</em> 280 (November): 213–23. <a href="https://doi.org/10.1016/j.forsciint.2017.08.033">https://doi.org/10.1016/j.forsciint.2017.08.033</a>.
</div>
<div id="ref-chu_validation_2013" class="csl-entry">
Chu, Wei, Mingsi Tong, and John Song. 2013. <span>“Validation <span>Tests</span> for the <span>Congruent</span> <span>Matching</span> <span>Cells</span> (<span>CMC</span>) <span>Method</span> <span>Using</span> <span>Cartridge</span> <span>Cases</span> <span>Fired</span> with <span>Consecutively</span> <span>Manufactured</span> <span>Pistol</span> <span>Slides</span>.”</span> <em>Journal of the Association of Firearms and Toolmarks Examiners</em> 45 (4): 6. <a href="https://www.nist.gov/publications/validation-tests-congruent-matching-cells-cmc-method-using-cartridge-cases-fired">https://www.nist.gov/publications/validation-tests-congruent-matching-cells-cmc-method-using-cartridge-cases-fired</a>.
</div>
<div id="ref-cleveland1994elements" class="csl-entry">
Cleveland, W. S. 1994. <em>The Elements of Graphing Data</em>. AT&amp;T Bell Laboratories. <a href="https://books.google.com/books?id=KMsZAQAAIAAJ">https://books.google.com/books?id=KMsZAQAAIAAJ</a>.
</div>
<div id="ref-crawford_handwriting_2020" class="csl-entry">
Crawford, Amy. 2020. <span>“Bayesian Hierarchical Modeling for the Forensic Evaluation of Handwritten Documents.”</span> {Ph.D thesis}, Iowa State University. <a href="https://doi.org/10.31274/etd-20200624-257">https://doi.org/10.31274/etd-20200624-257</a>.
</div>
<div id="ref-crowder1990analysis" class="csl-entry">
Crowder, M. J., and D. J. Hand. 1990. <em>Analysis of Repeated Measures</em>. Chapman &amp; Hall/CRC Monographs on Statistics &amp; Applied Probability. Taylor &amp; Francis. <a href="https://books.google.com/books?id=XsGX6Jgzo-IC">https://books.google.com/books?id=XsGX6Jgzo-IC</a>.
</div>
<div id="ref-Curran2000-hp" class="csl-entry">
Curran, James Michael, Tacha Natalie Hicks Champod, and John S Buckleton, eds. 2000a. <em>Forensic Interpretation of Glass Evidence</em>. Boca Raton, FL: CRC Press.
</div>
<div id="ref-DeFrance2003" class="csl-entry">
DeFrance, Charles, and MD Arsdale. 2003. <span>“Validation Study of Electrochemical Rifling.”</span> <em>Association of Firearms and Tool Marks Examiners Journal</em> 35 (January): 35–37.
</div>
<div id="ref-Deng2018" class="csl-entry">
Deng, Houtao. 2018. <span>“Interpreting Tree Ensembles with <span class="nocase">inTrees</span>.”</span> <em>International Journal of Data Science and Analytics</em> 7 (4): 277–87. <a href="https://doi.org/10.1007/s41060-018-0144-8">https://doi.org/10.1007/s41060-018-0144-8</a>.
</div>
<div id="ref-Duez2017" class="csl-entry">
Duez, Pierre, Todd Weller, Marcus Brubaker, Richard E. Hockensmith, and Ryan Lilien. 2017. <span>“Development and Validation of a Virtual Examination Tool for Firearm Forensics, ,”</span> <em>Journal of Forensic Sciences</em> 63 (4): 1069–84. <a href="https://doi.org/10.1111/1556-4029.13668">https://doi.org/10.1111/1556-4029.13668</a>.
</div>
<div id="ref-Duvendack2015" class="csl-entry">
Duvendack, Maren, Richard W. Palmer-Jones, and W. Reed. 2015. <span>“Replications in Economics: A Progress Report.”</span> <em>Econ Journal Watch</em> 12 (2).
</div>
<div id="ref-Ester1996" class="csl-entry">
Ester, Martin, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. <span>“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.”</span> In <em>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</em>, 226–31. KDD’96. Portland, Oregon: AAAI Press.
</div>
<div id="ref-fadulempirical2011" class="csl-entry">
Fadul, T., G. Hernandez, S. Stoiloff, and Gulati Sneh. 2011a. <span>“An <span>Empirical</span> <span>Study</span> to <span>Improve</span> the <span>Scientific</span> <span>Foundation</span> of <span>Forensic</span> <span>Firearm</span> and <span>Tool</span> <span>Mark</span> <span>Identification</span> <span>Utilizing</span> 10 <span>Consecutively</span> <span>Manufactured</span> <span>Slides</span>.”</span> <a href="https://www.ojp.gov/ncjrs/virtual-library/abstracts/empirical-study-improve-scientific-foundation-forensic-firearm-and">https://www.ojp.gov/ncjrs/virtual-library/abstracts/empirical-study-improve-scientific-foundation-forensic-firearm-and</a>.
</div>
<div id="ref-Garton2021" class="csl-entry">
Garton, Nathaniel, Danica Ommen, Jarad Niemi, and Alicia Carriquiry. 2020. <span>“Score-Based Likelihood Ratios to Evaluate Forensic Pattern Evidence.”</span> <a href="https://arxiv.org/abs/2002.09470">https://arxiv.org/abs/2002.09470</a>.
</div>
<div id="ref-ISO1661021" class="csl-entry">
<span>“<span class="nocase">Geometrical product specifications (GPS) - Filtration - Part 61: Linear areal filters: Gaussian filters</span>.”</span> 2011. Standard. Vol. 2011. Geneva, CH: International Organization for Standardization.
</div>
<div id="ref-ISO16610-71" class="csl-entry">
<span>“<span class="nocase">Geometrical product specifications (GPS) - Filtration - Part 71: Robust areal filters: Gaussian regression filters</span>.”</span> 2014. Standard. Vol. 2014. Geneva, CH: International Organization for Standardization. <a href="https://www.iso.org/standard/60159.html">https://www.iso.org/standard/60159.html</a>.
</div>
<div id="ref-ISO25178-72" class="csl-entry">
<span>“<span class="nocase">Geometrical product specifications (GPS) — Surface texture: Areal — Part 72: XML file format x3p</span>.”</span> 2017. Standard. Vol. 2014. Geneva, CH: International Organization for Standardization. <a href="https://www.iso.org/standard/62310.html">https://www.iso.org/standard/62310.html</a>.
</div>
<div id="ref-Goldstein2016-un" class="csl-entry">
Goldstein, E, and James Brockmole. 2016. <em>Sensation and Perception</em>. 10th ed. Mason, OH: CENGAGE Learning Custom Publishing.
</div>
<div id="ref-Goode2021" class="csl-entry">
Goode, Katherine, and Heike Hofmann. 2021. <span>“Visual Diagnostics of an Explainer Model: Tools for the Assessment of <span>LIME</span> Explanations.”</span> <em>Statistical Analysis and Data Mining: The <span>ASA</span> Data Science Journal</em> 14 (2): 185–200. <a href="https://doi.org/10.1002/sam.11500">https://doi.org/10.1002/sam.11500</a>.
</div>
<div id="ref-Grning2018" class="csl-entry">
Grüning, Björn, John Chilton, Johannes Köster, Ryan Dale, Nicola Soranzo, Marius van den Beek, Jeremy Goecks, Rolf Backofen, Anton Nekrutenko, and James Taylor. 2018. <span>“Practical Computational Reproducibility in the Life Sciences.”</span> <em>Cell Systems</em> 6 (6): 631–35. <a href="https://doi.org/10.1016/j.cels.2018.03.014">https://doi.org/10.1016/j.cels.2018.03.014</a>.
</div>
<div id="ref-Gundersen2018" class="csl-entry">
Gundersen, Odd Erik, Yolanda Gil, and David W. Aha. 2018. <span>“On Reproducible <span>AI</span>: Towards Reproducible Research, Open Science, and Digital Scholarship in <span>AI</span> Publications.”</span> <em><span>AI</span> Magazine</em> 39 (3): 56–68. <a href="https://doi.org/10.1609/aimag.v39i3.2816">https://doi.org/10.1609/aimag.v39i3.2816</a>.
</div>
<div id="ref-Hadler2017" class="csl-entry">
Hadler, Jeremy R., and Max D. Morris. 2017. <span>“An Improved Version of a Tool Mark Comparison Algorithm.”</span> <em>Journal of Forensic Sciences</em> 63 (3): 849–55. <a href="https://doi.org/10.1111/1556-4029.13640">https://doi.org/10.1111/1556-4029.13640</a>.
</div>
<div id="ref-Hamby2009" class="csl-entry">
Hamby, James E., David J. Brundage, and James W. Thorpe. 2009. <span>“The Identification of Bullets Fired from 10 Consecutively Rifled 9mm Ruger Pistol Barrels: A Research Project Involving 507 Participants from 20 Countries.”</span> In <em>AFTE Journal</em>, 41:99–110.
</div>
<div id="ref-hampton" class="csl-entry">
Hampton, Della. 2016. <span>“Firearms Identification. A Discipline Mainly Concerned with Determining Whether a Bullet or Cartridge Was Fired by a Particular Weapon. - Ppt Download.”</span> <em>SlidePlayer</em>.
</div>
<div id="ref-Haralick1987" class="csl-entry">
Haralick, Robert M., Stanley R. Sternberg, and Xinhua Zhuang. 1987. <span>“Image Analysis Using Mathematical Morphology.”</span> <em><span>IEEE</span> Transactions on Pattern Analysis and Machine Intelligence</em> <span>PAMI</span>-9 (4): 532–50. <a href="https://doi.org/10.1109/tpami.1987.4767941">https://doi.org/10.1109/tpami.1987.4767941</a>.
</div>
<div id="ref-hare_automatic_2016" class="csl-entry">
Hare, Eric, Heike Hofmann, and Alicia Carriquiry. 2017. <span>“Automatic <span>Matching</span> of <span>Bullet</span> <span>Land</span> <span>Impressions</span>.”</span> <em>The Annals of Applied Statistics</em> 11 (4): 2332–56. <a href="http://arxiv.org/abs/1601.05788">http://arxiv.org/abs/1601.05788</a>.
</div>
<div id="ref-Huber2015" class="csl-entry">
Huber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. <span>“Orchestrating High-Throughput Genomic Analysis with Bioconductor.”</span> <em>Nature Methods</em> 12 (2): 115–21. <a href="https://doi.org/10.1038/nmeth.3252">https://doi.org/10.1038/nmeth.3252</a>.
</div>
<div id="ref-trueAlleleTestimony" class="csl-entry">
Indiana County Court of Common Pleas. 2009. <em>Commonwealth of Pennsylvania Vs. Kevin j. Foley</em>.
</div>
<div id="ref-Iqbal2016" class="csl-entry">
Iqbal, Shareen A., Joshua D. Wallach, Muin J. Khoury, Sheri D. Schully, and John P. A. Ioannidis. 2016. <span>“Reproducible Research Practices and Transparency Across the Biomedical Literature.”</span> Edited by David L Vaux. <em><span>PLOS</span> Biology</em> 14 (1): e1002333. <a href="https://doi.org/10.1371/journal.pbio.1002333">https://doi.org/10.1371/journal.pbio.1002333</a>.
</div>
<div id="ref-Knowles2021" class="csl-entry">
Knowles, Laura, Daniel Hockey, and John Marshall. 2021. <span>“The Validation of 3d Virtual Comparison Microscopy (<span>VCM</span>) in the Comparison of Expended Cartridge Cases.”</span> <em>Journal of Forensic Sciences</em> 67 (2): 516–23. <a href="https://doi.org/10.1111/1556-4029.14942">https://doi.org/10.1111/1556-4029.14942</a>.
</div>
<div id="ref-Krishnan2018" class="csl-entry">
Krishnan, Ganesh, and Heike Hofmann. 2018. <span>“Adapting the Chumbley Score to Match Striae on Land Engraved Areas (LEAs) of Bullets,”</span> <em>Journal of Forensic Sciences</em> 64 (3): 728–40. <a href="https://doi.org/10.1111/1556-4029.13950">https://doi.org/10.1111/1556-4029.13950</a>.
</div>
<div id="ref-MATTIJSSEN2020" class="csl-entry">
Mattijssen, Erwin J. A. T., Cilia L. M. Witteman, Charles E. H. Berger, Nicolaas W. Brand, and Reinoud D. Stoel. 2020. <span>“Validity and Reliability of Forensic Firearm Examiners.”</span> <em>Forensic Science International</em> 307: 110112. <a href="https://www.sciencedirect.com/science/article/pii/S0379073819305249">https://www.sciencedirect.com/science/article/pii/S0379073819305249</a>.
</div>
<div id="ref-Midway2020" class="csl-entry">
Midway, Stephen R. 2020. <span>“Principles of Effective Data Visualization.”</span> <em>Patterns</em> 1 (9): 100141. <a href="https://doi.org/10.1016/j.patter.2020.100141">https://doi.org/10.1016/j.patter.2020.100141</a>.
</div>
<div id="ref-nasem_2019" class="csl-entry">
National Academy of Sciences, Engineering, and Medicine. 2019. <em>Reproducibility and Replicability in Science</em>. National Academies Press. <a href="https://doi.org/10.17226/25303">https://doi.org/10.17226/25303</a>.
</div>
<div id="ref-council_strengthening_2009" class="csl-entry">
National Research Council. 2009. <em>Strengthening <span>Forensic</span> <span>Science</span> in the <span>United</span> <span>States</span>: <span>A</span> <span>Path</span> <span>Forward</span></em>. Washington, DC: The National Academies Press. <a href="https://doi.org/10.17226/12589">https://doi.org/10.17226/12589</a>.
</div>
<div id="ref-Neuman2022" class="csl-entry">
Neuman, Maddisen, Callan Hundl, Aimee Grimaldi, Donna Eudaley, Darrell Stein, and Peter Stout. 2022. <span>“Blind Testing in Firearms: Preliminary Results from a Blind Quality Control Program.”</span> <em>Journal of Forensic Sciences</em> 67 (3): 964–74. <a href="https://doi.org/10.1111/1556-4029.15031">https://doi.org/10.1111/1556-4029.15031</a>.
</div>
<div id="ref-Ommen2018" class="csl-entry">
Ommen, Danica M, and Christopher P Saunders. 2018. <span>“Building a Unified Statistical Framework for the Forensic Identification of Source Problems.”</span> <em>Law, Probability and Risk</em> 17 (2): 179–97. <a href="https://doi.org/10.1093/lpr/mgy008">https://doi.org/10.1093/lpr/mgy008</a>.
</div>
<div id="ref-HumanFactorsCommittee2020" class="csl-entry">
OSAC Human Factors Committee. 2020. <span>“Human Factors in Validation and Performance Testing of Forensic Science.”</span> Organization of Scientific Area Committees (<span>OSAC</span>) for Forensic Science. <a href="https://doi.org/10.29325/osac.ts.0004">https://doi.org/10.29325/osac.ts.0004</a>.
</div>
<div id="ref-park_algorithm_2020" class="csl-entry">
Park, Soyoung, and Alicia Carriquiry. 2020. <span>“An Algorithm to Compare Two-Dimensional Footwear Outsole Images Using Maximum Cliques and Speeded-up Robust Feature.”</span> <em>Statistical Analysis and Data Mining: The ASA Data Science Journal</em> 13 (2): 188–99. <a href="https://doi.org/10.1002/sam.11449">https://doi.org/10.1002/sam.11449</a>.
</div>
<div id="ref-Park2019" class="csl-entry">
Park, Soyoung, and Sam Tyner. 2019. <span>“Evaluation and Comparison of Methods for Forensic Glass Source Conclusions.”</span> <em>Forensic Science International</em> 305 (December): 110003. <a href="https://doi.org/10.1016/j.forsciint.2019.110003">https://doi.org/10.1016/j.forsciint.2019.110003</a>.
</div>
<div id="ref-Piccolo2016" class="csl-entry">
Piccolo, Stephen R., and Michael B. Frampton. 2016. <span>“Tools and Techniques for Computational Reproducibility.”</span> <em><span>GigaScience</span></em> 5 (1). <a href="https://doi.org/10.1186/s13742-016-0135-4">https://doi.org/10.1186/s13742-016-0135-4</a>.
</div>
<div id="ref-pcast2016" class="csl-entry">
President’s Council of Advisors on Sci. &amp; Tech. 2016. <span>“Forensic Science in Criminal Courts: Ensuring Scientific Validity of Feature-Comparison Methods.”</span> <a href="https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf">https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/PCAST/pcast_forensic_science_report_final.pdf</a>.
</div>
<div id="ref-Puiutta2020" class="csl-entry">
Puiutta, Erika, and Eric M. S. P. Veith. 2020. <span>“Explainable Reinforcement Learning: A Survey.”</span> In <em>Lecture Notes in Computer Science</em>, 77–95. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-57321-8_5">https://doi.org/10.1007/978-3-030-57321-8_5</a>.
</div>
<div id="ref-Rlanguage" class="csl-entry">
R Core Team. 2017. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-rattenbury" class="csl-entry">
Rattenbury, Richard C. 2015. <span>“Semiautomatic Pistol.”</span> <em>Encyclopædia Britannica</em>. Encyclopædia Britannica, inc.
</div>
<div id="ref-LIME" class="csl-entry">
Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. <span>“"Why Should i Trust You?": Explaining the Predictions of Any Classifier.”</span> In <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 1135–44. KDD ’16. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/2939672.2939778">https://doi.org/10.1145/2939672.2939778</a>.
</div>
<div id="ref-Rice2020" class="csl-entry">
Rice, Kiegan E. 2020. <span>“A Framework for Statistical and Computational Reproducibility in Large-Scale Data Analysis Projects with a Focus on Automated Forensic Bullet Evidence Comparison.”</span> <em>ProQuest Dissertations and Theses</em>. PhD thesis.
</div>
<div id="ref-Riva2014" class="csl-entry">
Riva, Fabiano, and Christophe Champod. 2014. <span>“Automatic Comparison and Evaluation of Impressions Left by a Firearm on Fired Cartridge Cases.”</span> <em>Journal of Forensic Sciences</em> 59 (3): 637–47. <a href="https://doi.org/10.1111/1556-4029.12382">https://doi.org/10.1111/1556-4029.12382</a>.
</div>
<div id="ref-Riva2016" class="csl-entry">
Riva, Fabiano, Rob Hermsen, Erwin Mattijssen, Pascal Pieper, and Christophe Champod. 2016. <span>“Objective Evaluation of Subclass Characteristics on Breech Face Marks.”</span> <em>Journal of Forensic Sciences</em> 62 (2): 417–22. <a href="https://doi.org/10.1111/1556-4029.13274">https://doi.org/10.1111/1556-4029.13274</a>.
</div>
<div id="ref-Riva2020" class="csl-entry">
Riva, Fabiano, Erwin J. A. T. Mattijssen, Rob Hermsen, Pascal Pieper, W. Kerkhoff, and Christophe Champod. 2020. <span>“Comparison and Interpretation of Impressed Marks Left by a Firearm on Cartridge Cases <span></span> Towards an Operational Implementation of a Likelihood Ratio Based Technique.”</span> <em>Forensic Science International</em> 313 (August): 110363. <a href="https://doi.org/10.1016/j.forsciint.2020.110363">https://doi.org/10.1016/j.forsciint.2020.110363</a>.
</div>
<div id="ref-Roth2015" class="csl-entry">
Roth, Joseph, Andrew Carriveau, Xiaoming Liu, and Anil K. Jain. 2015. <span>“Learning-Based Ballistic Breech Face Impression Image Matching.”</span> In <em>2015 IEEE 7th International Conference on Biometrics Theory, Applications and Systems (BTAS)</em>, 1–8. <a href="https://doi.org/10.1109/BTAS.2015.7358774">https://doi.org/10.1109/BTAS.2015.7358774</a>.
</div>
<div id="ref-Smith2016" class="csl-entry">
Smith, Tasha P., G. Andrew Smith, and Jeffrey B. Snipes. 2016. <span>“A Validation Study of Bullet and Cartridge Case Comparisons Using Samples Representative of Actual Casework.”</span> <em>Journal of Forensic Sciences</em> 61 (4): 939–46. <a href="https://doi.org/10.1111/1556-4029.13093">https://doi.org/10.1111/1556-4029.13093</a>.
</div>
<div id="ref-song_proposed_2013" class="csl-entry">
Song, John. 2013. <span>“Proposed <span>‘<span>NIST</span> <span>Ballistics</span> <span>Identification</span> <span>System</span> (<span>NBIS</span>)’</span> <span>Based</span> on <span>3d</span> <span>Topography</span> <span>Measurements</span> on <span>Correlation</span> <span>Cells</span>.”</span> <em>American Firearm and Tool Mark Examiners Journal</em> 45 (2): 11. <a href="https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868">https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=910868</a>.
</div>
<div id="ref-song_3d_2014" class="csl-entry">
Song, John, Wei Chu, Mingsi Tong, and Johannes Soons. 2014. <span>“<span>3d</span> Topography Measurements on Correlation Cells—a New Approach to Forensic Ballistics Identifications.”</span> <em>Measurement Science and Technology</em> 25 (6): 064005. <a href="https://doi.org/10.1088/0957-0233/25/6/064005">https://doi.org/10.1088/0957-0233/25/6/064005</a>.
</div>
<div id="ref-song_estimating_2018" class="csl-entry">
Song, John, Theodore V. Vorburger, Wei Chu, James Yen, Johannes A. Soons, Daniel B. Ott, and Nien Fan Zhang. 2018. <span>“Estimating Error Rates for Firearm Evidence Identifications in Forensic Science.”</span> <em>Forensic Science International</em> 284 (March): 15–32. <a href="https://doi.org/10.1016/j.forsciint.2017.12.013">https://doi.org/10.1016/j.forsciint.2017.12.013</a>.
</div>
<div id="ref-Stodden2018b" class="csl-entry">
Stodden, Victoria, Matthew S. Krafczyk, and Adhithya Bhaskar. 2018. <span>“Enabling the Verification of Computational Results.”</span> In <em>Proceedings of the First International Workshop on Practical Reproducible Evaluation of Computer Systems</em>. <span>ACM</span>. <a href="https://doi.org/10.1145/3214239.3214242">https://doi.org/10.1145/3214239.3214242</a>.
</div>
<div id="ref-Stodden2018a" class="csl-entry">
Stodden, Victoria, Jennifer Seiler, and Zhaokun Ma. 2018. <span>“An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility.”</span> <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2584–89. <a href="https://doi.org/10.1073/pnas.1708290115">https://doi.org/10.1073/pnas.1708290115</a>.
</div>
<div id="ref-Stroman2014" class="csl-entry">
Stroman, A. 2014. <span>“Empirically Determined Frequency of Error in Cartridge Case Examinations Using a Declared Double-Blind Format.”</span> <em>AFTE Journal</em> 46 (January): 157–75.
</div>
<div id="ref-Swofford2021" class="csl-entry">
Swofford, H., and C. Champod. 2021. <span>“Implementation of Algorithms in Pattern <span>&amp;</span> Impression Evidence: A Responsible and Practical Roadmap.”</span> <em>Forensic Science International: Synergy</em> 3: 100142. <a href="https://doi.org/10.1016/j.fsisyn.2021.100142">https://doi.org/10.1016/j.fsisyn.2021.100142</a>.
</div>
<div id="ref-Tai2019" class="csl-entry">
Tai, Xiao Hui. 2019. <span>“Matching Problems in Forensics.”</span> PhD thesis, Carnegie Mellon University. <a href="https://kilthub.cmu.edu/articles/Matching_Problems_in_Forensics/9963596/1">https://kilthub.cmu.edu/articles/Matching_Problems_in_Forensics/9963596/1</a>.
</div>
<div id="ref-tai_fully_2018" class="csl-entry">
Tai, Xiao Hui, and William F. Eddy. 2018. <span>“A <span>Fully</span> <span>Automatic</span> <span>Method</span> for <span>Comparing</span> <span>Cartridge</span> <span>Case</span> <span>Images</span>,”</span> <em>Journal of Forensic Sciences</em> 63 (2): 440–48. <a href="http://doi.wiley.com/10.1111/1556-4029.13577">http://doi.wiley.com/10.1111/1556-4029.13577</a>.
</div>
<div id="ref-telea2014data" class="csl-entry">
Telea, Alexandru C. 2014. <em>Data Visualization: Principles and Practice</em>. CRC Press.
</div>
<div id="ref-Thompson2017" class="csl-entry">
Thompson, Robert. 2017. <em>Firearm Identification in the Forensic Science Laboratory</em>. National District Attorneys Association. <a href="https://doi.org/10.13140/RG.2.2.16250.59846">https://doi.org/10.13140/RG.2.2.16250.59846</a>.
</div>
<div id="ref-tong_improved_2015" class="csl-entry">
Tong, Mingsi, John Song, and Wei Chu. 2015. <span>“An <span>Improved</span> <span>Algorithm</span> of <span>Congruent</span> <span>Matching</span> <span>Cells</span> (<span>CMC</span>) <span>Method</span> for <span>Firearm</span> <span>Evidence</span> <span>Identifications</span>.”</span> <em>Journal of Research of the National Institute of Standards and Technology</em> 120 (April): 102. <a href="https://doi.org/10.6028/jres.120.008">https://doi.org/10.6028/jres.120.008</a>.
</div>
<div id="ref-tong_fired_2014" class="csl-entry">
Tong, Mingsi, John Song, Wei Chu, and Robert M. Thompson. 2014. <span>“Fired <span>Cartridge</span> <span>Case</span> <span>Identification</span> <span>Using</span> <span>Optical</span> <span>Images</span> and the <span>Congruent</span> <span>Matching</span> <span>Cells</span> (<span>CMC</span>) <span>Method</span>.”</span> <em>Journal of Research of the National Institute of Standards and Technology</em> 119 (November): 575. <a href="https://doi.org/10.6028/jres.119.023">https://doi.org/10.6028/jres.119.023</a>.
</div>
<div id="ref-openForSciR" class="csl-entry">
Tyner, Sam, Soyoung Park, Ganesh Krishnan, Karen Pan, Eric Hare, Amanda Luby, Xiao Hui Tai, Heike Hofmann, and Guillermo Basulto-Elias. 2019. <span>“Sctyner/OpenForSciR: Create DOI for Open Forensic Science in r.”</span> Zenodo. <a href="https://zenodo.org/record/3418141">https://zenodo.org/record/3418141</a>.
</div>
<div id="ref-Ulery2011" class="csl-entry">
Ulery, Bradford T., R. Austin Hicklin, JoAnn Buscaglia, and Maria Antonia Roberts. 2011. <span>“Accuracy and Reliability of Forensic Latent Fingerprint Decisions.”</span> <em>Proceedings of the National Academy of Sciences</em> 108 (19): 7733–38. <a href="https://doi.org/10.1073/pnas.1018707108">https://doi.org/10.1073/pnas.1018707108</a>.
</div>
<div id="ref-Ulery2012" class="csl-entry">
———. 2012. <span>“Repeatability and Reproducibility of Decisions by Latent Fingerprint Examiners.”</span> Edited by Chuhsing Kate Hsiao. <em><span>PLoS</span> <span>ONE</span></em> 7 (3): e32800. <a href="https://doi.org/10.1371/journal.pone.0032800">https://doi.org/10.1371/journal.pone.0032800</a>.
</div>
<div id="ref-Ulery2014" class="csl-entry">
Ulery, Bradford T., R. Austin Hicklin, Maria Antonia Roberts, and JoAnn Buscaglia. 2014. <span>“Measuring What Latent Fingerprint Examiners Consider Sufficient Information for Individualization Determinations.”</span> Edited by Francesco Pappalardo. <em><span>PLoS</span> <span>ONE</span></em> 9 (11): e110179. <a href="https://doi.org/10.1371/journal.pone.0110179">https://doi.org/10.1371/journal.pone.0110179</a>.
</div>
<div id="ref-weller_2012" class="csl-entry">
Weller, Todd J., Alan Zheng, Robert Thompson, and Fred Tulleners. 2012. <span>“Confocal Microscopy Analysis of Breech Face Marks on Fired Cartridge Cases from 10 Consecutively Manufactured Pistol Slides”</span> 57 (4). <a href="https://doi.org/10.1111/j.1556-4029.2012.02072.x">https://doi.org/10.1111/j.1556-4029.2012.02072.x</a>.
</div>
<div id="ref-topmatch" class="csl-entry">
Weller, Todd, Marcus Brubaker, Pierre Duez, and Ryan Lilien. 2015. <span>“Introduction and Initial Evaluation of a Novel Three-Dimensional Imaging and Analysis System for Firearm Forensics.”</span> <em>AFTE Journal</em> 47 (January): 198.
</div>
<div id="ref-firearmManufacturing" class="csl-entry">
Werner, Denis, Romain Berthod, Damien Rhumorbarbe, and Alain Gallusser. 2021. <span>“Manufacturing of Firearms Parts: Relevant Sources of Information and Contribution in a Forensic Context.”</span> <em>WIREs Forensic Science</em> 3 (3): e1401. <a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wfs2.1401">https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wfs2.1401</a>.
</div>
<div id="ref-ggplot2" class="csl-entry">
Wickham, Hadley. 2009. <em>Ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York. <a href="http://ggplot2.org">http://ggplot2.org</a>.
</div>
<div id="ref-tidy-data" class="csl-entry">
———. 2014. <span>“Tidy Data.”</span> <em>The Journal of Statistical Software</em> 59. <a href="http://www.jstatsoft.org/v59/i10/">http://www.jstatsoft.org/v59/i10/</a>.
</div>
<div id="ref-tidyverse" class="csl-entry">
Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. <span>“Welcome to the <span class="nocase">tidyverse</span>.”</span> <em>Journal of Open Source Software</em> 4 (43): 1686.
</div>
<div id="ref-Wilkinson2005" class="csl-entry">
Wilkinson, Leland. 2005. <em>The Grammar of Graphics</em>. Berlin, Heidelberg: Springer-Verlag.
</div>
<div id="ref-Xie2014" class="csl-entry">
Xie, Yihui. 2014a. <span>“Knitr: A Comprehensive Tool for Reproducible Research in <span>R</span>.”</span> In <em>Implementing Reproducible Computational Research</em>, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC. <a href="http://www.crcpress.com/product/isbn/9781466561595">http://www.crcpress.com/product/isbn/9781466561595</a>.
</div>
<div id="ref-zhang_convergence_2021" class="csl-entry">
Zhang, Hao, Jialing Zhu, Rongjing Hong, Hua Wang, Fuzhong Sun, and Anup Malik. 2021. <span>“Convergence-Improved Congruent Matching Cells (<span>CMC</span>) Method for Firing Pin Impression Comparison.”</span> <em>Journal of Forensic Sciences</em> 66 (2): 571–82. <a href="https://doi.org/10.1111/1556-4029.14634">https://doi.org/10.1111/1556-4029.14634</a>.
</div>
<div id="ref-Zheng2020" class="csl-entry">
Zheng, Xiaoyu, Johannes Soons, Robert Thompson, Sushama Singh, and Cerasela Constantin. 2020. <span>“<span>NIST</span> Ballistics Toolmark Research Database.”</span> <em>Journal of Research of the National Institute of Standards and Technology</em> 125 (January). <a href="https://doi.org/10.6028/jres.125.004">https://doi.org/10.6028/jres.125.004</a>.
</div>
<div id="ref-Zheng2014" class="csl-entry">
Zheng, X, J Soons, T V Vorburger, J Song, T Renegar, and R Thompson. 2014. <span>“Applications of Surface Metrology in Firearm Identification.”</span> <em>Surface Topography: Metrology and Properties</em> 2 (1): 014012. <a href="https://doi.org/10.1088/2051-672x/2/1/014012">https://doi.org/10.1088/2051-672x/2/1/014012</a>.
</div>
<div id="ref-reproducibleScience" class="csl-entry">
Zimmerman, Naupaka, Greg Wilson, Raniere Silva, Scott Ritchie, François Michonneau, Jeffrey Oliver, Harriet Dashnow, et al. 2019. <span>“Swcarpentry/r-Novice-Gapminder: Software Carpentry: R for Reproducible Scientific Analysis, June 2019.”</span> Zenodo. <a href="https://zenodo.org/record/3265164">https://zenodo.org/record/3265164</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="a-study-in-reproducibility-the-congruent-matching-cells-algorithm-and-cmcr-package.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["thesis.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
