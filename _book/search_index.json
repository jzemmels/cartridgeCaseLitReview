[["index.html", "A Cartridge Case Comparison Pipeline 1 LITERATURE REVIEW 1.1 Preliminaries: Forensic Examinations 1.2 Forensic Comparison Pipelines 1.3 Diagnostic Tools 1.4 Automating and Improving the Cartridge Case Identification Pipeline", " A Cartridge Case Comparison Pipeline Joseph Zemmels Abstract As the need for computationally-intensive methods to analyze forensic data has grown, so too has the need for user-friendly tools to experiment with and improve upon these methods. In this work, we discuss an algorithm used to objectively measure the similarity between cartridge cases. Chapter 2 discusses a modularization of the algorithm into a ``pipeline\" that enables reproducibility, experimentation, and comprehension. Chapter 3 details a suite of diagnostic tools that illuminate the inner-workings of the algorithm and help determine when and why the algorithm does or does not ``work\" correctly. Chapter 4 introduces novel pieces of this pipeline that we demonstrate are improvements to the current state-of-the-art. 1 LITERATURE REVIEW 1.1 Preliminaries: Forensic Examinations A primary goal of a forensic examination is to determine the source of a piece of evidence. This is referred to as the problem . A common setting for source identification problems involves obtaining evidence of unknown source from a crime scene and either evidence from a known source or other evidence of unknown source. For example, a bullet found at a crime scene may be compared to a suspects firearm. Such evidence is sent to a forensic lab where a trained forensic examiner compares the ``questioned\" bullet to bullets fired from the suspects firearm to determine whether the suspects firearm was the original source. In this work, we develop a method to supplement such an examination by providing an objective measure of similarity between the two pieces of evidence. 1.1.1 Firearms and Toolmarks Identification Firearms and toolmarks (F &amp; T) identification involves studying markings or impressions left by a hard surface, such as the metal of a firearm or other tool (e.g., screwdriver), on a softer surface . For example, a firearm barrel leaves toolmarks on a bullet as it travels through the barrel. In this work, we focus on impressions left on a cartridge during the firing process. 1.1.1.1 The Firing Process In this section, we describe the basic process of firing a handgun or rifle using a cartridge. A consists of a metal casing containing primer, gunpowder, and a bullet. Figure ?? shows a cross-section of a cartridge featuring these components . Figure 1.1: A cartridge containing primer, powder, and a bullet. The firing process is initiated by loading a cartridge into the barrel of a firearm. To initiate the firing process, a cartridge is loaded into an area in the back of the barrel known as the . Figure ?? shows an example of a cartridge loaded into the chamber of a pistol . In this example, the hammer of the pistol is pulled back such that the firing pin is held back under spring tension. Upon squeezing the trigger, the firing pin is released and travels forwards at a high velocity. The firing pin strikes the primer of the cartridge case, causing it to explode. Figure 1.2: Cross-section of a pistol with a chambered cartridge and drawn-back hammer. Pulling the trigger releases the firing pin which strikes the cartridge case primer. As shown in Figure ??, the explosion of the primer ignites the powder in the cartridge . Gas rapidly expands in the cartridge causing the bullet to travel down the barrel. At the same time, the rest of the cartridge is sent towards the back of the barrel. Figure 1.3: A cartridge after a firing pin has struck the primer. The explosion of the primer ignites the powder within the cartridge, causing gas to rapidly expand and force the bullet down the barrel. As the bullet leaves the barrel, the cartridge case strikes the back wall of the barrel, known as the , with considerable force. Any markings on the breech face are imprinted onto the cartridge case, creating the so-called . These impressions are analogous to a barrels ``fingerprint\" left on the cartridge case. Figure ?? shows cartoon examples of breech face markings that appear on cartridge cases . Figure 1.4: Examples of common breech face impression patterns. These are considered analogous to a breech face fingerprint left on the cartridge surface. Figure ?? shows the base of a fired cartridge . The hole to the south-east of the center of the primer is the impression left by the firing pin. Note the horizontal striated breech face markings on the primer to the left of the firing pin impression. Figure 1.5: A fired 9mm Luger cartridge case with visible firing pin and breech face impressions. After the bullet has left the barrel, the extractor pin and ejector push the cartridge case out of the chamber. As shown in Figure ??, these can leave additional markings on the cartridge . Firing pin, breech face, extractor pin and ejector, and other possible markings are all used in a forensic examination to determine whether two cartridge cases were fired from the same firearm. Note that the focus of this work is on the comparison of breech face impressions specifically. Figure 1.6: Examples of common extractor pin and ejector markings. These, impressions on the cartridge, are used in a forensic examination to determine the source of the fired cartridge. 1.1.1.2 An Overview of Firearms and Toolmarks Examinations Forensic firearms and toolmarks (F &amp; T) identification has been performed in the United States for over 100 years . For most of this time, trained F &amp; T examiners have used a , such as the one shown in Figure ??, to perform these examinations . A comparison microscope consists of two compound microscopes that are joined via an which allows for viewing of the stages below each microscope simultaneously under the same eyepiece. The right image of Figure ?? shows an example of the view under a comparison microscope of two bullets with the white dotted line separating the two fields of view. Figure 1.7: A comparison microscope consists of two stages upon which evidence is placed. These stages are placed under two compound microscopes that are joined together via an optical bridge and allow for viewing of both stages simultaneously under a single eyepiece. The image on the right shows an example of a bullet viewed under a comparison microscope. Firearm examiners distinguish between three broad categories when characterizing a fired bullet or cartridge case: class, subclass, and individual characteristics. are associated with the manufacturer of the firearm that fired the bullet or cartridge case. These include, but are not limited to, the size of ammunition chambered by the firearm, the orientation of the extractor and ejector, or the width and twist direction of the barrel rifling. Class characteristics are often the first to be examined because they can narrow the relevant population of potential firearm sources . For example, a 9mm cartridge case must have been fired by a firearm that can chamber 9mm ammunition. If the discernible class characteristics match between two pieces of evidence, for example a cartridge case found at a crime scene and a different cartridge case fired by a suspects gun, then the examiner uses a comparison microscope to compare the of the evidence. Individual characteristics are markings attributed to imperfections on the firearm surface due to the manufacturing process, use, and wear of the tool. For example, markings on the breech face of a barrel may form after repeated fires of the firearm. Individual characteristics are assumed to occur randomly across different firearms. In an examination, the examiner independently rotates and translates the stages of a comparison microscope to find the optimal matching position of the markings on the two pieces of evidence . If the individual characteristics on two pieces of evidence are determined to agree ``sufficiently,\" then the examiner can conclude that they originated from the same firearm . exist between the macro-level class and micro-level individual characteristics. These characteristics relate to markings that are reproduced across a subgroup of firearms. For example, breech faces using the same milling machine may include markings that are unique to the milling machine . As it can be difficult to distinguish between individual and subclass characteristics during an examination, an examiners decision process may be affected if the existence of subclass characteristics is suspected. Many F &amp; T examiners in the United States adhere to the Association of Firearms and Toolmarks Examiners (AFTE) Range of Conclusions when making their evidentiary conclusions . According to these guidelines, there are six possible conclusions that can be made in a F &amp; T examination: In general, forensic examinations first involve an examination of a ``questioned\" bullet or cartridge case for identifiable toolmarks . Markings including breech face, firing pin, chamber marks, extractor pin, and ejector impressions are categorized by their class, individual, or subclass characteristics. If available, this information is compared to ``known source\" fires obtained from a suspects firearm. If known source evidence is unavailable, class characteristics from the questioned bullet can be used to narrow the relevant population and provide potential leads. Ultimately, an examiners decision may be used as part of an ongoing investigation or presented at trial as expert testimony. It should be noted that standard operating procedures for assessing and comparing evidence differ between forensic laboratories. For example, some labs collapse the three possible inconclusive decisions into a single decision or prohibit examiners from making an elimination based on differences in individual characteristics . 1.1.2 Why Should Firearms and Toolmarks Identification Change? In 2009, the National Research Council released a report assessing a number of forensic disciplines including Firearms and Toolmarks analysis. The report pointed out that F &amp; T analysis lacked a precisely defined process and that little research had been done to determine the reliability or repeatability of the methods. Part of the recommendations from this study were to establish rigorously-validated laboratory procedures and ``develop automated techniques capable of enhancing forensic technologies .\" A number of studies have been performed to assess the reliability and repeatability of a firearms and toolmarks examination (non-exhaustively: ). All of these studies report extremely low error rates when examiners are asked to make conclusions for evidence for which the authors know ground truth (i.e., whether the bullets or cartridge cases are truly matching or non-matching). However, as pointed out in a 2016 report from the Presidents Council of Advisors on Science and Technology, many of these studies, save , were not ``appropriately designed to test the foundational validity and estimate reliability .\" The report asserts that additional, properly-designed studies should be performed to more rigorously establish the scientific validity of the discipline. Due to the opacity in the decision-making process, examiners have been referred to as ``black boxes\" in a similar sense to black-box algorithms . Their evidentiary conclusions are fundamentally subjective, and there is empirical evidence to suggest that conclusions differ across examiners when presented with the same evidence and even for a single examiner when presented with the same evidence on two different occasions . Examiners rarely need to provide quantitative justification for their conclusion. Even for qualitative justifications, it can be difficult to determine what the examiner is actually ``looking at\" to arrive at their conclusion . This suggests the need to supplement these black box decisions with transparent, objective techniques that quantitatively measure the similarity between pieces of evidence. As stated in , efforts should be made to ``convert firearms analysis from a subjective method to an objective method\" including ``developing and testing image-analysis algorithms for comparing the similarity of tool marks.\" The focus of this work is on the development of an algorithm for comparing breech face impressions on cartridge cases. 1.2 Forensic Comparison Pipelines Recent work in many forensic disciplines has focused on the development of algorithms to measure the similarity between pieces of evidence including glass , handwriting , shoe prints , ballistics , and toolmarks . These algorithms often result in a numerical, non-binary (dis)similarity score for two pieces of evidence. A non-binary score adds more nuance to an evidentiary conclusion beyond simply stating whether the evidence did or did not originate from the same source as would be the case in binary classification. For example, the larger the similarity score, the ``more similar\" the evidence. However, a binary (or ternary, if admitting inconclusives) conclusion must ultimately be reached by an examiner. Whether a decision should be reached based solely on results of a comparison algorithm (e.g., defining a score-based decision boundary) or if an examiner should incorporate the similarity score into their own decision-making process is still up for debate . In this work, we view forensic comparison algorithms as a supplement to, rather than a replacement of, the forensic examination. We conceptualize forensic comparison algorithms as evidence-to-classification ``pipelines.\" Broadly, the steps of the pipeline include: This is similar to the structure discussed in . We add to this structure the emphasis that each step of the pipeline can be further broken-down into modularized pieces. For example, the preprocessing step may include multiple sub-procedures to isolate a region of interest of the evidence. Figure ?? shows three possible variations of the cartridge case comparison pipeline as well as the parameters requiring manual specification and alternative modules. The benefits of this modularization include easing the process of experimenting with different parameters/sub-procedures and improving the comprehensibility of the pipeline. [Update the pipeline diagram] Figure 1.8: Variations upon the cartridge case comparison pipeline. The first three columns detail the pipeline with different sub-procedures. The fourth columns shows the parameters that require manual specification at each step. The fifth column shows alternative processing steps that could replace steps in the existing pipeline. In the following sections, we detail recent advances to each of the five steps in the pipeline outlined above. We narrow our focus to advances made in comparing F &amp; T evidence. 1.2.1 Digital Representations of Evidence Digital representations of cartridge case evidence commonly come in one of two modes: 2D optical images or 3D topographic scans. A common way to take 2D optical images is to take a picture of the cartridge case under a microscope. This implies that the digital representation of the cartridge case surface is dependent on the lighting conditions under which the picture was taken. Some recent work has focused on comparing 2D optical images , although the use of 3D microscopes has become more prevalent to capture the surface of ballistics evidence. Using a 3D microscope, we obtain scans at the micron (or micrometer) level that are more light-agnostic than a 2D image . One common 3D scanning procedure is disc scanning confocal microscopy. This procedure works by shining a focused beam of light on the cartridge case surface. This light is reflected back onto a pinhole allowing a limited height range to pass through. The microscope scans through different height range ``slices\" and compiles all these slices into a single 3D topography of the cartridge case primer surface. The Microdisplay Scan Confocal Microscope from Sensofar~Metrology is shown in Figure ?? . Figure 1.9: The Microdisplay Scan Confocal Microscope from Sensofar Metrology. The cartridge case surface is captured by scanning through a range of vertical slices and compiling these slices into a single 3D topography. Figure ?? shows a 2D image and 3D topography of the same cartridge case primer from . Figure 1.10: A cartridge case captured using 2D confocal reflectance microscopy (left) and 3D disc scanning confocal microscopy (right). More recently, Cadre Forensics~introduced the TopMatch-3D High-Capacity Scanner . A tray of 15 fired cartridge cases and the scanner are shown in Figure ?? . This scanner collects images under various lighting conditions of a gel pad into which the cartridge case surface is impressed and combines these images into a regular 2D array called a . The physical dimensions of these objects are about 5.5 \\(mm^2\\) captured at a resolution of 1.84 microns per pixel (1000 microns equals 1 mm). Figure 1.11: The TopMatch-3D High-Capacity Scanner from Cadre Forensics . The scanner captures topographic scans of a gel pad into which a cartridge case surface is impressed. When applied to ballistics evidence, these 3D scans are commonly stored in the ISO standard x3p file format . x3p is a container consisting of a single surface matrix representing the height value of the surface and metadata concerning the parameters under which the scan was taken as shown in Figure ?? . It has been empirically demonstrated that comparing 3D topographic scans of cartridge case evidence leads to more accurate conclusions compared to comparing 2D optical images of the same evidence . Figure 1.12: The hierarchy of information stored in the x3p file format for both bullet and cartridge case evidence. 1.2.2 Preprocessing Procedures for Forensic Data When capturing the surface of a cartridge case, the result is bound to contain extraneous regions due to the incongruity between the circular primer and the rectangular array in which the surface data are stored. Figure ?? shows an example of a 2D image and 3D scan of the same cartridge case. We can see, for example, that the corners of these arrays include non-primer regions of the cartridge case surface. Additionally, the center of the cartridge case primer features an impression left by the firing pin during the firing process. In most applications, impressions left by the firing pin are compared separately from the breech face impressions . As the focus of this work is on the comparison of breech face impressions between two cartridge cases, only the annular region surrounding the firing pin impression is of interest. The annular breech face impression region must be segmented away from the rest of the captured surface. Both the 2D optical and 3D topographic representations of cartridge case surfaces are fundamentally pictorial in nature. As such, many image processing and computer vision techniques are used to automatically isolate the breech face impression region. uses a combination of histogram equalization, Canny edge detection, and morphological operations to isolate the breech face impressions in 2D images. Various types of Gaussian filters are commonly employed to remove unwanted structure. uses a low-pass Gaussian filter that removes noise via a Gaussian-weighted moving average operation. use a bandpass Gaussian filter, which simultaneously performs the function of a low-pass filter along with a high-pass filter to remove global structure from the scan. Other versions of the bandpass filter are used in that accomplish tasks such as omitting outlier surface values or addressing boundary effects . Instead of using automatic procedures, others have used subjective human intervention to isolate the breech face impressions. For example, indicate that cartridge cases are ``manually trimming to extract the breech face impression of interest.\" In , examiners manually identify the borders of the breech face impression region by placing points around an image of the cartridge case primer. 1.2.3 Forensic Data Feature Extraction After applying the preprocessing procedures to two cartridge case scans, their breech face impressions are compared and similarity features are extracted. Given that the cartridge cases at this point are represented as high-dimensional matrices, this can be thought of as a dimensionality reduction of the high-dimensional surface arrays to a set of similarity statistics. A variety of features have been proposed to quantify the similarity between two cartridge case surface arrays. propose calculating the cross-correlation function (CCF) value between two cartridge cases across a grid of rotations. It is assumed that the CCF will to be larger around the ``true\" rotation for matching cartridge case pairs than for non-matching pairs. proposed combining the CCF between the two aligned scans with the element-wise median Euclidean distance and median difference between the normal vectors at each point of the surface. Later, applied Principal Component Analysis to reduce these three features down to two principal components onto which a 2D Kernel Density Estimator could be fit. Pertinent to this work is the cell-based comparison procedure originally outlined in . The underlying assumption of is similar to that of : that two matching cartridge cases will exhibit higher similarity when they are ``close\" to being correctly aligned. While measured similarity using the CCF between the two full scans, proposes partitioning the scans into a grid of ``correlation cells\" and counting the number of similar cells between the two scans. The rationale behind this procedure is that many cartridge case scans have regions that do not contain discriminatory markings. As such, comparing full scans may result in a lower correlation than if one were to focus on the highly-discriminatory regions. In theory, dividing the scans into cells allows for the identification of these regions. After breaking a scan into a grid of cells, each cell is compared to the other scan to identify the rotation and translation, known together as the , at which the cross-correlation is maximized. assume that the cells from a truly matching pair of cartridge cases will ``agree\" on their registration in the other scan. Details of this procedure are provided in Chapter 2. 1.2.4 Similarity Scores for Forensic Data Following feature extraction, the dimensionality of these features is further reduced to a low-dimensional, usually univariate, similarity score. After calculating the CCF across various possible registrations, propose using the maximum observed CCF value as the univariate similarity score. In this case, a binary classification can be achieved by setting a CCF threshold above which pairs are classified as ``matches\" and below which as ``non-matches.\" proposes setting a CCF cut-off that maximizes the precision and recall in a training set of pairwise comparisons. use a training set to fit two 2D kernel density estimates to a set of features from matching and non-matching comparisons. Using these estimates, they are able to estimate the score-based likelihood ratio (SLR) for a new set of features. This SLR can be viewed as a similarity score . In the case of the cell-based comparison procedure discussed above, the total number of cells that are deemed ``congruent matching\" is used as a similarity score. The criteria used to define ``congruent matching\" has changed across papers and will be discussed in greater detail in [Chapter]. The authors of these papers have consistently used a decision boundary of six ``Congruent Matching Cells\" to distinguish matches from non-matches. applies the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to the features from the cell-based comparison procedure to determine if any clusters form amongst the per-cell estimated registration values. This is based on the assumption that any cells that come to a consensus on their registration should form a cluster in translation \\((x,y)\\) and rotation \\(\\theta\\) space. proposes a binary classifier based on whether any clusters are identified by the DBSCAN algorithm . If a cluster is found for a particular pairwise comparison, then that pair is classified as a ``match\" and otherwise as a ``non-match.\" 1.2.5 Reproducibility of Comparison Pipelines defines reproducibility as ``obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis.\" While not exact in their definition of ``consistent,\" the authors do assert that, barring a few exceptions, it is reasonable to expect that the results obtained by a second researcher, after applying the exact same processing steps to the exact same data, be the exact same as the original results. Among the exceptions given is if the original researcher had made a mistake in writing the original source code. In either case, they assert that ``a studys data and code have to be available in order for others to reproduce and confirm results.\" Researchers can not only easily verify the results given data and code, they can also incorporate the materials into their own research and thus improve or accelerate discovery . A number of studies indicate that computationally reproducible research is sparse across various disciplines. and studied the reproducibility of articles sampled from the journals and the , respectively. In the former, found that only 3 of 204 randomly selected articles from were ``straightforward to reproduce with minimal effort;\" despite a journal policy requiring that all code and data used in the paper be made available to any reader. In the latter, found that zero of 306 randomly selected articles from the were ``straightforward to reproduce with minimal effort\" and, at best, that five articles were ``reproducible after some tweaking.\" Similar findings were found in (29 of 59 economic papers reproducible), (zero of 268 biomedical papers provided raw data and 1 in 268 linked to a full study protocol), (50% or more published articles include data or code in only 27 of 333 economics journals), and (24 of 400 AI conference papers included code). A common recommendation amongst these authors is the establishment of rigorous standards for reproducibility. This includes making code and data used in a paper easily-accessible to readers. Infrastructure already exists to ease the processing of developing, maintaining, and sharing open-source code and data. Data repositories such as the NIST Ballistics Toolmark Research Database provide open access to raw data. discuss the use of package managers such as Conda , container software such as Docker (https://www.docker.com/), and virtual machine software to preserve the entire data analysis environment in-perpetuity. For situations in which VMs or containers arent available, software such as the manager R package allows users to ``compare package inventories across machines, users, and time to identify changes in functions and objects .\" reference repositories like Bioconductor that make it easy to document and distribute code. Further, software such as the knitr R package enable ``literate programming\" in which prose and executed code can be interwoven to make it easier to understand the codes function. These tools make data, code, and derivative research findings more accessible, in terms of both acquisition and comprehensibility, to consumers and fellow researchers. 1.3 Diagnostic Tools Forensic examiners often provide expert testimony in court cases. As part of this testimony, an examiner is allowed to not only provide facts about the outcome of a forensic examination, but also their opinion about what the results mean. A party to a court may challenge the examiner on the validity of the underlying scientific method or whether they interpreted the results correctly . In these situations, examiners need to be able to explain the process by which they reached an evidentiary conclusion to the fact finders of the case; namely, the judge or jury. As algorithms are more often used in forensic examinations, the technical knowledge required to understand and explain an algorithm to lay-people has increased. While in some cases the authors of the algorithm have been willing to provide testimony to establish the validity of the algorithm , this will become less viable as algorithms become more prevalent. Indeed, even the most elegant improvements to an algorithm may be moot if an examiner cant explain the improvements in their testimony. The resources required to educate examiners on the use of highly technical algorithms makes additional training seem currently implausible. An alternative is to develop algorithms from the ground-up to be intuitive for examiners to understand and explain to others. refers to the ability to identify the factors that contributed to the results of an algorithm . For example, understanding ``why\" a classifier predicted one class over another. Myriad techniques exist to explain the results of an algorithm. These range from identifying instances of the training set that illuminate how the model operates to fitting more transparent models that approximate the complex model accurately to explaining the behavior of the algorithm in a small region of interest . Many of these methods require additional technical knowledge to interpret these explanations. A less technical approach is to use visualizations that facilitate understanding of model behavior. Properly constructed visuals enable both exploratory data analysis and diagnostics . Given that many of the procedures by which cartridge case evidence is captured, processed, and compared are based on image processing techniques, a visual diagnostic is an intuitive mode of explanation for experts and lay-people alike. In this work, we develop a suite of visual diagnostic tools that can be used to explain the behavior of the cartridge case comparison pipeline. [Discuss tidyverse tools for visual diagnostics. ggplot2, etc. Emphasize that rshiny enables users to actually physically engage with the analysis process.] 1.4 Automating and Improving the Cartridge Case Identification Pipeline In this section, we review preliminaries needed to understand various sub-routines of the cartridge case comparison pipeline. 1.4.1 Image Processing Techniques We first review image processing and computer vision algorithms that are commonly used in cartridge case comparison algorithms. Throughout this section, let \\(A\\) and \\(B\\) denote two images. Define these images to be 2D arrays of a given size where \\(A[m,n]\\) and \\(B[m,n]\\) each map to a spatially-ordered measurement value. For example, the measurement may be the height \\(h\\) value of a cartridge case surface at a particular \\([m,n]\\) location. 1.4.1.1 Image Registration Image registration involves transforming one image to align with another image . For example, in the case of object or facial recognition, one may be interested in finding a template image in another image. For images \\(A\\) and \\(B\\), image registration can be defined as a mapping between two images: \\[\\begin{align*} B[m,n] = f(A[m,n]) \\end{align*}\\] where \\(f\\) is a 2D spatial-coordinate transformation. In our application, \\(f\\) will represent an affine transformation of the Cartesian coordinate space composed of a translation and rotation. This transformation commonly has three parameters: \\(\\Delta x, \\Delta y, \\theta\\) which map a point \\((x_1, y_1)\\) of the first image to a point \\((x_2,y_2)\\) of the second image: \\[\\begin{align*} \\begin{pmatrix} x_2 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} \\Delta x \\\\ \\Delta y \\end{pmatrix} + \\begin{pmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ y_1 \\end{pmatrix}. \\end{align*}\\] A transformation \\(f(\\cdot,\\cdot;\\pmb{t}^*)\\), equivalently a parameter vector \\(\\pmb{t}^* \\in \\pmb{T}\\), is selected such that it optimizes similarity metric \\(s(\\cdot,\\cdot)\\) between the two images: \\[\\begin{align*} \\pmb{t}^* \\equiv \\arg \\max_{\\pmb{t} \\in \\pmb{T}} s(A[m,n],f(B[m,n];\\pmb{t})). \\end{align*}\\] In our application, the set of possible parameters is \\(\\pmb{T} = \\mathbb{Z} \\times \\mathbb{Z} \\times [0,2\\pi)\\) representing discrete-index horizontal and vertical translations (positive meaning up/right and negative meaning down/left) and a rotation. Commonly, the (CCF) is used as the similarity metric. For a \\(P \\times Q\\) ``reference\" image \\(A\\) and \\(M \\times N\\) ``template\" image \\(B\\), the cross-correlation function, denoted \\(A \\star B\\), is used as a similarity metric. The cross-correlation function measures the similarity between \\(A\\) and \\(B\\) for each translation: \\[\\begin{align*} (A \\star B)[m,n] = \\sum_{i=1}^M \\sum_{j=1}^N A[i,j]B[(i + m),(j + n)] \\end{align*}\\] where \\(1 \\leq m \\leq M + P - 1\\) and \\(1 \\leq n \\leq N + Q - 1\\). By this definition, \\(A \\star B\\) is a 2D array of dimension \\(M + P - 1 \\times N + Q - 1\\) in which the \\([m,n]\\)-th element quantifies the similarity between \\(A\\) and \\(B\\) when \\(B\\) is translated \\(m\\) elements horizontally and \\(n\\) elements vertically. For interpretability, the CCF is commonly normalized between -1 and 1. Using the CCF as a similarity metric, we can determine the translations \\([m^*, n^*]\\) at which images \\(A\\) and \\(B\\) attain the maximum CCF value: \\[\\begin{align*} [m^{\\dagger},n^{\\dagger}] \\equiv \\arg \\max_{[m,n]} (A \\star B)[m,n]. \\end{align*}\\] To determine the optimal rotation, we calculate the maximum CCF value across a range of rotations of image \\(B\\). If \\(B_\\theta\\) denotes image \\(B\\) rotated by an angle \\(\\theta \\in [0,2\\pi)\\), then the estimated registration \\([m^*,n^*,\\theta^*]\\) is given by: \\[\\begin{align*} [m^*,n^*,\\theta^*] \\equiv \\arg \\max_{[m,n,\\theta]} (A \\star B_{\\theta})[m,n]. \\end{align*}\\] In implementation we consider a discrete grid of rotations \\(\\pmb{\\Theta} \\subset [0,2 \\pi)\\). The overall registration procedure is given by: Based on the definition given above, the CCF is computationally taxing. In image processing, it is common to use an implementation based on the Fast Fourier Transform . This implementation leverages the Cross-Correlation Theorem, which states that for images \\(A\\) and \\(B\\) the CCF can be expressed in terms of a frequency-domain pointwise product: \\[ (A \\star B)[m,n] = \\mathcal{F}^{-1}\\left(\\overline{\\mathcal{F}(A)} \\odot \\mathcal{F}(B)\\right)[m,n] \\] where \\(\\mathcal{F}\\) and \\(\\mathcal{F}^{-1}\\) denote the discrete Fourier and inverse discrete Fourier transforms, respectively, and \\(\\overline{\\mathcal{F}(A)}\\) denotes the complex conjugate . Because the product on the right-hand side is calculated pointwise, this result allows us to trade the moving sum computations from the definition of the CCF for two forward Fourier transformations, a pointwise product, and an inverse Fourier transformation. The Fast Fourier Transform (FFT) algorithm can be used to reduce the computational load considerably. Figure ?? shows an example of two images \\(A\\) and \\(B\\) of dimension \\(100 \\times 100\\) and \\(21 \\times 21\\), respectively. The white boxes in both of the images are of dimension \\(10 \\times 10\\). The box in image A is centered on index [30,50] while the box in image B is centered on index [11,11]. The right image shows the result of calculating the CCF using image \\(A\\) as reference and \\(B\\) as template. We see that the CCF achieves a maximum of 1, indicating a perfect match, at the translation value of \\([m^\\dagger,n^\\dagger] = [22,-2]\\). This represents that if image B were overlaid onto image A such that their center indices coincided, then image B would need to be shifted 22 units ``up\" and 2 units ``left\" to match perfectly with image A. Figure 1.13: (Left) A reference image \\(A\\) and template image \\(B\\) both featuring a white box of dimension \\(10 \\times 10\\). (Right) The cross-correlation function (CCF) between \\(A\\) and \\(B\\). The index at which the CCF is maximized represents the translation at which \\(A\\) and \\(B\\) are most similar. 1.4.1.2 Gaussian Filters In image processing, a Gaussian filter (equivalently, blur or smoother) is mathematical operator that imputes the values in an image using a locally-weighted sum of surrounding values. In our application, a Gaussian filter, specifically a Gaussian filter, is used to smooth the surface values of a cartridge case scan. The weights are dictated according to the Gaussian function of a chosen standard deviation \\(\\sigma\\) given by: \\[ f(x,y;\\sigma) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{1}{2\\sigma^2}(x^2 + y^2)\\right). \\] It is common to populate a 2D array with the values of the Gaussian function treating the center index as the origin. Such an array is called a . An example of a \\(3 \\times 3\\) Gaussian kernel with standard deviation \\(\\sigma = 1\\) is given below. \\[ K = \\begin{pmatrix} 0.075 &amp; 0.124 &amp; 0.075 \\\\ 0.124 &amp; 0.204 &amp; 0.124 \\\\ 0.075 &amp; 0.124 &amp; 0.075 \\end{pmatrix}. \\] For an image \\(A\\) and Gaussian kernel \\(K\\) with standard deviation \\(\\sigma\\), the lowpass filtered version of \\(A\\), denoted \\(A_{lp,\\sigma}\\) is given by: \\[ A_{lp,\\sigma}[m,n] = \\mathcal{F}^{-1}\\left(\\mathcal{F}(A) \\odot \\mathcal{F}(K)\\right)[m,n]. \\] You will note the similarity between this operation, known as , and the calculation of the CCF given above . Figure ?? shows an image \\(A\\) of a box undergoing the injection of Gaussian noise (noise standard deviation \\(\\sigma_n = 0.3\\)) followed by the application of various filters. While the box is obscured due to noise in the middle image, the lowpass filter (kernel standard deviation \\(\\sigma_k = 2\\)) recovers some of the definition of the box seen in the original image \\(A\\). If a lowpass filter ``smooths\" the values of an image, then a filter performs a ``sharpening\" operation. More specifically, for image \\(A\\) and kernel standard deviation \\(\\sigma\\), the highpass filtered version \\(A_{hp}\\) can be defined as: \\[\\begin{align*} A_{hp,\\sigma} = A - A_{lp,\\sigma}. \\end{align*}\\] The highpass filter therefore removes larger-scale (smooth) structure from an image and retains high-frequency structure such as noise or edges. An example of a highpass-filtered image \\(A\\) is shown in Figure ??. We can see that the smooth interior of the box is effectively removed from the image while the edges are preserved. Finally, the bandpass filter performs the highpass sharpening followed by the lowpass smoothing operations. Generally, the highpass kernels standard deviation will be considerably larger than that of the lowpass kernel. This leads to retaining sharp edges while also reducing noise. An example of a bandpass filtered image \\(A\\) is shown in Figure ??. We see that the edges of the box are better-preserved compared to the lowpass filter figure while the interior of the box is better-preserved compared to the highpass filter figure. Figure 1.14: An image \\(A\\) of a box undergoing various filtering operations. Variations on the standard Gaussian filter include the ``robust\" Gaussian regression filter. This filter fluctuates between a filter step, which applies a Gaussian filter, and outlier step, which identifies and omits outlier observations from the next filter step . Another alternative, the ``edge preserving\" filter, adapts the kernel weights when approaching the boundary of an image to mitigate so-called . 1.4.1.3 Morphological Operations Mathematical morphology refers to a theory and collection of image processing techniques for geometrical structures . In our application, these geometrical structures are cartridge case scans; specifically, binarized versions of these scans representing whether a particular pixel does or does not contain part of the cartridge case surface. Two fundamental operations in mathematical morphology are and . For our purposes, these are both set operations on binary (black and white) images. We classify the set of black and white pixels as the background and foreground of the image, respectively. For an image \\(A\\), let \\(W = \\{[m,n] : A[m,n] = 1\\}\\) denote the foreground of \\(A\\), meaning \\(W^c\\) represents the background. An example of a \\(7 \\times 7\\) binary image \\(A\\) with \\(W = \\{[3,3],[3,4],[3,5],[4,3],[4,4],[4,5],[5,3],[5,4],[5,5]\\}\\) is given below. \\[ A = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] A is a second, typically small, array \\(B\\) of ones that affects the amount of dilation or erosion applied to \\(W\\) within \\(A\\). For simplicity, the indexing of the structuring element uses the center index as the origin. For example, a \\(3 \\times 3\\) structuring element is given by \\(B = \\{(-1,-1),(-1,0),(-1,1),(-1,0),(0,0),(0,1),(1,-1),(1,0),(1,1)\\}\\) or visually: \\[ B = \\begin{pmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 \\end{pmatrix} \\] The dilation of \\(W\\) by \\(B\\), denoted \\(W \\oplus B\\), is defined by \\[ W \\oplus B = \\{[m,n] \\in A : [m,n] = [i,j] + [k,l] \\text{ for } [i,j] \\in W \\text{ and } [k,l] \\in B\\} \\] where the index arithmetic is performed element-wise. Alternatively, if \\(W_{[k,l]}\\) represents the translation of region \\(W\\) within \\(A\\) by \\(k\\) units row-wise and \\(l\\) units column-wise for \\([k,l] \\in B\\) In this example, \\[W \\oplus B = \\{[3,2],[3,3],[3,4],[3,5],[3,6],[4,2],[4,3],[4,4],[4,5],[4,6],[5,2],[5,3],[5,4],[5,5],[5,6]\\}\\] or visually: \\[ W \\oplus B = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\] We see that the dilation operation by \\(B\\) has the effect of growing the region \\(W\\) inside of \\(A\\) by one index in each direction. In contrast, erosion has the effect of shrinking a selected region. More precisely, the erosion of \\(W\\) by \\(B\\) is defined by \\[ A \\ominus B = \\{[m,n] \\in A: [m,n] + [k,l] \\in A \\text{ for every } [k,l] \\in B\\}. \\] Using the same example as above, \\(W \\ominus B = \\{[3,3]\\}\\) or visually: \\[ W \\ominus B = \\begin{pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix}. \\] Erosion by \\(B\\) therefore shrinks the region \\(W\\) in \\(A\\) by one index in each direction. Figure ?? shows the example considered here in terms of black and white representations of \\(A\\) undergoing dilation and erosion by \\(B\\). In practice, there may be two or more disconnected foreground regions in \\(A\\) to which dilation or erosion can be independently applied. Figure 1.15: A \\(7 \\times 7\\) image \\(A\\) featuring a \\(3 \\times 3\\) box undergoing dilation and erosion by a \\(3 \\times 3\\) structuring element \\(B\\). 1.4.2 Density-Based Spatial Clustering of Applications with Noise The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is a clustering procedure that assigns observations to clusters if they are in a region of high observation density . Otherwise they are classified as ``noise\" points. Let \\(D\\) represent a \\(n \\times p\\) data set (\\(n\\) observations, each of dimension \\(p\\)) and consider observations \\(x,y,z \\in D\\). The DBSCAN algorithm relies on the notion of \\(\\epsilon\\)-neighborhoods. Given some neighborhood radius \\(\\epsilon \\in \\mathbb{R}\\) and distance metric \\(d\\), \\(y\\) is in the \\(\\epsilon\\)-neighborhood of \\(x\\) if \\(d(x,y) \\leq \\epsilon\\). The of \\(x\\) is defined as the set \\(N_{\\epsilon}(x) = \\{y \\in D : d(x,y) \\leq \\epsilon\\}\\). Given a minimum number of points \\(Minpts \\in \\mathbb{N}\\), observation \\(x\\) is called a with respect to \\(\\epsilon\\) and \\(Minpts\\) if \\(|N_{\\epsilon}(x)| \\geq Minpts\\). Both \\(\\epsilon\\) and \\(Minpts\\) are selected by the user. Figure ?? shows an example of 10 points on the Cartesian plane. An \\(\\epsilon\\)-neighborhood using the Euclidean distance metric and \\(\\epsilon = 3\\) is drawn around an observation \\(x\\) located at \\((3,2)\\). Points inside the circle are neighbors of \\(x\\). If, for example, \\(Minpts = 2\\), then \\(x\\) would be considered a core point. Figure 1.16: An \\(\\epsilon\\)-neighborhood around a observation located at \\((3,2)\\) for \\(\\epsilon = 3\\). Points are labeled based on whether they are neighbors to this observation or not. To identify regions of high observation density, two relational notions, and , are used. A point \\(y\\) is to a point \\(x\\) if \\(x\\) is a core point and \\(y \\in N_{\\epsilon}(x)\\). In the example in Figure ??, the observation located at \\((1,0)\\) is directly density-reachable to the observation located at \\((3,2)\\). More broadly, a point \\(x_m\\) is to a point \\(x_1\\) if there exists a chain of observations \\(x_1,x_2,...,x_{m-1},x_m\\) such that \\(x_{i+1}\\) is directly density-reachable from \\(x_i\\), \\(i = 1,...,n\\). Figure ?? shows an example of three density-reachable points located at \\((1,0), (3,2)\\), and \\((4,4)\\) using \\(\\epsilon = 3\\) and \\(Minpts = 2\\). All three points are core points and although the points located at \\((4,4)\\) and \\((1,3)\\) are not neighbors, they share a neighbor in the point located at \\((3,2)\\) and are thus density-reachable. Figure 1.17: An example of three points that are density-reachable with respect to \\(\\epsilon = 3\\) and \\(Minpts = 2\\). Finally, a point \\(y\\) is to a point \\(x\\) with respect to \\(\\epsilon\\) and \\(Minpts\\) if there is a point \\(z\\) such that both \\(x\\) and \\(y\\) are density-reachable to \\(z\\) (with respect to \\(\\epsilon\\) and \\(Minpts\\)). While density-reachability requires that all points in-between two points be core points, density-connectivity extends the notion of ``neighbors of neighbors\" to include points that are merely within the neighborhood of density-reachable points. Figure ?? illustrates how the points located at \\((4,7)\\) and \\((0,-2)\\) are density-connected while not being density-reachable. Figure 1.18: An example of two points that are density-connected, but not density-reachable, with respect to \\(\\epsilon = 3\\) and \\(Minpts = 2\\). A \\(C \\subset D\\) with respect to \\(\\epsilon\\) and \\(Minpts\\) satisfies the following conditions: Points that are not assigned to a cluster are classified as . For a data set \\(D\\), the DBSCAN algorithm determines clusters based on the above definition. Figure ?? shows the labels return by DBSCAN for the example considered above with respect to \\(\\epsilon = 3\\) and \\(Minpts = 2\\).. We see that seven points are classified in a single cluster and three points are classified as noise. Figure 1.19: Cluster labeling for 10 data points using the DBSCAN algorithm with parameters \\(\\epsilon = 3\\) and \\(Minpts = 2\\). Seven points are assigned to a single cluster and the remaining three are classified as noise. 1.4.3 Features Based on Visual Diagnostics Much of the literature on ``explainable\" algorithms are focused on black-box machine learning algorithms such as Random Forests or Multi-layer Neural Networks. [More to say here?] Less focused is placed on constructing explainable features. Feature selection and engineering is a critical, often time-intensive step in the data analysis process that isnt often We use the visual diagnostic tools discussed in Chapter [5] to develop a set of features. By definition, these features are human-interpretable unlike, for example, features that are calculated in the convolution layer of a convolutional neural network. The interpretability of these features imply that they can be explained to forensic examiners or lay-people. This will make it easier to introduce such methods into forensic labs and court rooms. 1.4.4 Implementation Considerations This cartridge case comparison pipeline is similar to other data analysis pipelines. Much like other data analysis pipelines, the procedural details can be obscured as the goals of the analysis become more sophisticated. This is helpful neither for the individual performing the analysis nor for any consumer of the results. As such, it is worthwhile to design tools that make the data analysis procedure easier to implement and understand . Beyond conceptualizing the cartridge case comparison procedure as a pipeline, we also implement the procedure in the R statistical programming as a sequence of algorithms that can programatically be connected together . In particular, we utilize the pipe operator available from the R package . This operator allows the output of one function to be passed as input to another without assigning a new variable. Data can be incrementally transformed as they move from one function to another. Implementing a data analysis procedure using the pipe operator allows the user to think intuitively in terms of verbs applied to the data. Table ?? illustrates two examples of pipelines that utilize the pipe operator. The left-hand example shows how an R data frame can be manipulated using functions from the package. Functions like , , and are simple building blocks that can be strung together to create complicated workflows. The right-hand example similarly illustrates a cartridge case object passing through the comparison pipeline. While the full comparison procedure is complex, the modularization to the , , and steps, which can further be broken-down into simple building blocks, renders the process more understandable to, and flexible for, the user. Figure ??, Figure ??, Figure ??, and Figure ?? illustrate how various forensic comparison algorithms use a modularized structure in their preprocessing procedures. In each figure, a sequence of modular procedures are applied to a piece of evidence. Figure ?? shows the morphological and image processing preprocessing procedures used to remove the firing pin region from a 2D image of a cartridge case . Figure ?? shows the procedure by which a 2D ``signature\" of a bullet scan is extracted from a 3D topographical scan . Figure ?? shows how an image of the written word ``csafe\" is processed using the handwriter R package to break the word into individual that can be further processed . Finally, Figure ?? shows a 3D topographical cartridge case scan undergoing various procedures to isolate and highlight the breech face impressions. These procedures are discussed in greater detail in Chapter 2. By breaking the broader preprocessing step into modularized pieces, we can devise other arrangements of these preprocessing procedures that may improve the segmenting or emphasizing of the region of interest. The modularity of the pipeline makes it easier to understand what the algorithm is doing ``under the hood\" while a modularized implementation enables others to experiment with alternative versions of the pipeline. Figure 1.20: A preprocessing procedure applied to a 2D image of a cartridge case to identify the firing pin impression. The procedure results in a 2D image of a cartridge case without the firing pin impression region. Figure 1.21: A preprocessing procedure for extracting 2D bullet ``signatures\" from a 3D topographic bullet scan. The procedure results in an ordered sequence of values representing the local variations in the surface of the bullet. Figure 1.22: A preprocessing procedure applied to a handwriting image of the word ``csafe.\" The procedure results in a skeletonized version of the word that has been separated into graphemes as represented by orange nodes. Figure 1.23: A cartridge case undergoing various preprocessing steps. The procedure results in a cartridge case scan in which the breech face impressions have been segmented and highlighted. Our implementation is structured to adhere to the ``tidy\" principles of design . The is a collection of R packages that share an underlying design philosophy and structure. The four principles of a tidy API are: Adherence to these principles makes it easier to engage with and understand the overall data analysis pipeline. In our application it also enables experimentation by making it easy to change one step of the pipeline and measure the downstream effects . Each step of the cartridge case comparison pipeline requires the user to define parameters. These can range from minor, such as the standard deviation used in a Gaussian filter, to substantial, such as choosing the algorithm used to calculate the similarity score. So far, there is no consensus on the ``best\" parameter settings. A large amount of experimentation is yet required to establish these parameters. A tidy implementation of the cartridge case comparison pipeline allows more people to engage in the validation and improvement of the procedure. "],["this-is-the-title-of-the-first-paper.html", "2 THIS IS THE TITLE OF THE FIRST PAPER Abstract 2.1 Introduction", " 2 THIS IS THE TITLE OF THE FIRST PAPER published in the Journal of ABC Jane Smith and Jesse Adams Abstract This is the abstract to my paper which explains in general terms the concepts and hypothesis that will be used. 2.1 Introduction I use the wonderful software R and ggplot2 for computation and visualization (R Core Team 2017; Wickham 2009). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
