<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>A Cartridge Case Comparison Pipeline</title>
  <meta name="description" content="A Cartridge Case Comparison Pipeline" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="A Cartridge Case Comparison Pipeline" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="jzemmels/cartridgeCaseLitReview" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Cartridge Case Comparison Pipeline" />
  
  
  

<meta name="author" content="Joseph Zemmels" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="this-is-the-title-of-the-first-paper.html"/>
<style type="text/css">
p.abstract{
  text-align: center;
  font-weight: bold;
}
div.abstract{
  margin: auto;
  width: 90%;
}
</style>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> LITERATURE REVIEW</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#preliminaries-forensic-examinations"><i class="fa fa-check"></i><b>1.1</b> Preliminaries: Forensic Examinations</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#firearms-and-toolmarks-identification"><i class="fa fa-check"></i><b>1.1.1</b> Firearms and Toolmarks Identification</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#why-should-firearms-and-toolmarks-identification-change"><i class="fa fa-check"></i><b>1.1.2</b> Why Should Firearms and Toolmarks Identification Change?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#forensic-comparison-pipelines"><i class="fa fa-check"></i><b>1.2</b> Forensic Comparison Pipelines</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#digital-representations-of-evidence"><i class="fa fa-check"></i><b>1.2.1</b> Digital Representations of Evidence</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#preprocessing-procedures-for-forensic-data"><i class="fa fa-check"></i><b>1.2.2</b> Preprocessing Procedures for Forensic Data</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#forensic-data-feature-extraction"><i class="fa fa-check"></i><b>1.2.3</b> Forensic Data Feature Extraction</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#similarity-scores-for-forensic-data"><i class="fa fa-check"></i><b>1.2.4</b> Similarity Scores for Forensic Data</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#reproducibility-of-comparison-pipelines"><i class="fa fa-check"></i><b>1.2.5</b> Reproducibility of Comparison Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#diagnostic-tools"><i class="fa fa-check"></i><b>1.3</b> Diagnostic Tools</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#automating-and-improving-the-cartridge-case-identification-pipeline"><i class="fa fa-check"></i><b>1.4</b> Automating and Improving the Cartridge Case Identification Pipeline</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#image-processing-techniques"><i class="fa fa-check"></i><b>1.4.1</b> Image Processing Techniques</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#density-based-spatial-clustering-of-applications-with-noise"><i class="fa fa-check"></i><b>1.4.2</b> Density-Based Spatial Clustering of Applications with Noise</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#features-based-on-visual-diagnostics"><i class="fa fa-check"></i><b>1.4.3</b> Features Based on Visual Diagnostics</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#implementation-considerations"><i class="fa fa-check"></i><b>1.4.4</b> Implementation Considerations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="this-is-the-title-of-the-first-paper.html"><a href="this-is-the-title-of-the-first-paper.html"><i class="fa fa-check"></i><b>2</b> THIS IS THE TITLE OF THE FIRST PAPER</a>
<ul>
<li class="chapter" data-level="" data-path="this-is-the-title-of-the-first-paper.html"><a href="this-is-the-title-of-the-first-paper.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="2.1" data-path="this-is-the-title-of-the-first-paper.html"><a href="this-is-the-title-of-the-first-paper.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Cartridge Case Comparison Pipeline</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">A Cartridge Case Comparison Pipeline</h1>
<p class="author"><em>Joseph Zemmels</em></p>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>As the need for computationally-intensive methods to analyze forensic data has grown, so too has the need for user-friendly tools to experiment with and improve upon these methods. In this work, we discuss an algorithm used to objectively measure the similarity between cartridge cases. Chapter 2 discusses a modularization of the algorithm into a ``pipeline" that enables reproducibility, experimentation, and comprehension. Chapter 3 details a suite of diagnostic tools that illuminate the inner-workings of the algorithm and help determine when and why the algorithm does or does not ``work" correctly. Chapter 4 introduces novel pieces of this pipeline that we demonstrate are improvements to the current state-of-the-art.</p>
</div>
</div>
<div id="literature-review" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> LITERATURE REVIEW<a href="index.html#literature-review" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- \hypertarget{preliminaries-forensic-examinations}{% -->
<!-- \section{Preliminaries: Forensic Examinations}\label{preliminaries-forensic-examinations}} -->
<div id="preliminaries-forensic-examinations" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Preliminaries: Forensic Examinations<a href="index.html#preliminaries-forensic-examinations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>A primary goal of a forensic examination is to determine the source of a piece of evidence.
This is referred to as the  problem .
A common setting for source identification problems involves obtaining evidence of unknown source from a crime scene and either evidence from a known source or other evidence of unknown source.
For example, a bullet found at a crime scene may be compared to a suspect’s firearm.
Such evidence is sent to a forensic lab where a trained forensic examiner compares the ``questioned" bullet to bullets fired from the suspect’s firearm to determine whether the suspect’s firearm was the original source.
In this work, we develop a method to supplement such an examination by providing an objective measure of similarity between the two pieces of evidence.</p>
<!-- \hypertarget{firearms-and-toolmarks-identification}{% -->
<!-- \subsection{Firearms and Toolmarks Identification}\label{firearms-and-toolmarks-identification}} -->
<div id="firearms-and-toolmarks-identification" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Firearms and Toolmarks Identification<a href="index.html#firearms-and-toolmarks-identification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Firearms and toolmarks (F &amp; T) identification involves studying markings or impressions left by a hard surface, such as the metal of a firearm or other tool (e.g., screwdriver), on a softer surface .
For example, a firearm barrel leaves toolmarks on a bullet as it travels through the barrel.
In this work, we focus on impressions left on a cartridge during the firing process.</p>
<!-- \hypertarget{the-firing-process}{% -->
<!-- \subsubsection{The Firing Process}\label{the-firing-process}} -->
<div id="the-firing-process" class="section level4 hasAnchor" number="1.1.1.1">
<h4><span class="header-section-number">1.1.1.1</span> The Firing Process<a href="index.html#the-firing-process" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this section, we describe the basic process of firing a handgun or rifle using a cartridge.
A  consists of a metal casing containing primer, gunpowder, and a bullet.
Figure <a href="#fig:cartridgeDiagram"><strong>??</strong></a> shows a cross-section of a cartridge featuring these components .</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.5\textwidth]{images/bulletdiagram1}  -->
<!-- } -->
<!-- \caption{\label{fig:cartridgeDiagram} A cartridge containing primer, powder, and a bullet. The firing process is initiated by loading a cartridge into the barrel of a firearm.}\label{fig:unnamed-chunk-1} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="images/bulletdiagram1.png" alt="\label{fig:cartridgeDiagram} A cartridge containing primer, powder, and a bullet. The firing process is initiated by loading a cartridge into the barrel of a firearm." width=".5\textwidth" />
<p class="caption">
Figure 1.1:  A cartridge containing primer, powder, and a bullet. The firing process is initiated by loading a cartridge into the barrel of a firearm.
</p>
</div>
<p>To initiate the firing process, a cartridge is loaded into an area in the back of the barrel known as the .
Figure <a href="#fig:pistolParts"><strong>??</strong></a> shows an example of a cartridge loaded into the chamber of a pistol .
In this example, the hammer of the pistol is pulled back such that the firing pin is held back under spring tension.
Upon squeezing the trigger, the firing pin is released and travels forwards at a high velocity.
The firing pin strikes the primer of the cartridge case, causing it to explode.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.5\textwidth]{images/Parts-pistol}  -->
<!-- } -->
<!-- \caption{\label{fig:pistolParts} Cross-section of a pistol with a chambered cartridge and drawn-back hammer. Pulling the trigger releases the firing pin which strikes the cartridge case primer.}\label{fig:unnamed-chunk-2} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="images/Parts-pistol.png" alt="\label{fig:pistolParts} Cross-section of a pistol with a chambered cartridge and drawn-back hammer. Pulling the trigger releases the firing pin which strikes the cartridge case primer." width=".5\textwidth" />
<p class="caption">
Figure 1.2:  Cross-section of a pistol with a chambered cartridge and drawn-back hammer. Pulling the trigger releases the firing pin which strikes the cartridge case primer.
</p>
</div>
<p>As shown in Figure <a href="#fig:firingCartridge"><strong>??</strong></a>, the explosion of the primer ignites the powder in the cartridge .
Gas rapidly expands in the cartridge causing the bullet to travel down the barrel.
At the same time, the rest of the cartridge is sent towards the back of the barrel.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.5\textwidth]{images/firingCartridgeDiagram}  -->
<!-- } -->
<!-- \caption{\label{fig:firingCartridge} A cartridge after a firing pin has struck the primer. The explosion of the primer ignites the powder within the cartridge, causing gas to rapidly expand and force the bullet down the barrel.}\label{fig:unnamed-chunk-3} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="images/firingCartridgeDiagram.jpg" alt="\label{fig:firingCartridge} A cartridge after a firing pin has struck the primer. The explosion of the primer ignites the powder within the cartridge, causing gas to rapidly expand and force the bullet down the barrel." width=".5\textwidth" />
<p class="caption">
Figure 1.3:  A cartridge after a firing pin has struck the primer. The explosion of the primer ignites the powder within the cartridge, causing gas to rapidly expand and force the bullet down the barrel.
</p>
</div>
<p>As the bullet leaves the barrel, the cartridge case strikes the back wall of the barrel, known as the , with considerable force.
Any markings on the breech face are imprinted onto the cartridge case, creating the so-called .
These impressions are analogous to a barrel’s ``fingerprint" left on the cartridge case.
Figure <a href="#fig:impressionDiagram"><strong>??</strong></a> shows cartoon examples of breech face markings that appear on cartridge cases .</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.6\textwidth]{images/breechFaceImpressionDiagram}  -->
<!-- } -->
<!-- \caption{\label{fig:impressionDiagram} Examples of common breech face impression patterns. These are considered analogous to a breech face fingerprint left on the cartridge surface.}\label{fig:unnamed-chunk-4} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="images/breechFaceImpressionDiagram.jpg" alt="\label{fig:impressionDiagram} Examples of common breech face impression patterns. These are considered analogous to a breech face fingerprint left on the cartridge surface." width=".6\textwidth" />
<p class="caption">
Figure 1.4:  Examples of common breech face impression patterns. These are considered analogous to a breech face fingerprint left on the cartridge surface.
</p>
</div>
<p>Figure <a href="#fig:realCartridgeCase"><strong>??</strong></a> shows the base of a fired cartridge .
The hole to the south-east of the center of the primer is the impression left by the firing pin.
Note the horizontal striated breech face markings on the primer to the left of the firing pin impression.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.7\textwidth]{images/realCartridgeCaseImage}  -->
<!-- } -->
<!-- \caption{\label{fig:realCartridgeCase} A fired 9mm Luger cartridge case with visible firing pin and breech face impressions.}\label{fig:unnamed-chunk-5} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="images/realCartridgeCaseImage.jpg" alt="\label{fig:realCartridgeCase} A fired 9mm Luger cartridge case with visible firing pin and breech face impressions." width=".7\textwidth" />
<p class="caption">
Figure 1.5:  A fired 9mm Luger cartridge case with visible firing pin and breech face impressions.
</p>
</div>
<p>After the bullet has left the barrel, the extractor pin and ejector push the cartridge case out of the chamber.
As shown in Figure <a href="#fig:extractorMarkings"><strong>??</strong></a>, these can leave additional markings on the cartridge .
Firing pin, breech face, extractor pin and ejector, and other possible markings are all used in a forensic examination to determine whether two cartridge cases were fired from the same firearm.
Note that the focus of this work is on the comparison of breech face impressions specifically.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.6\textwidth]{images/extractorPinDiagram}  -->
<!-- } -->
<!-- \caption{\label{fig:extractorMarkings} Examples of common extractor pin and ejector markings. These, impressions on the cartridge, are used in a forensic examination to determine the source of the fired cartridge.}\label{fig:unnamed-chunk-6} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="images/extractorPinDiagram.png" alt="\label{fig:extractorMarkings} Examples of common extractor pin and ejector markings. These, impressions on the cartridge, are used in a forensic examination to determine the source of the fired cartridge." width=".6\textwidth" />
<p class="caption">
Figure 1.6:  Examples of common extractor pin and ejector markings. These, impressions on the cartridge, are used in a forensic examination to determine the source of the fired cartridge.
</p>
</div>
<!-- \hypertarget{an-overview-of-firearms-and-toolmarks-examinations}{% -->
<!-- \subsubsection{An Overview of Firearms and Toolmarks Examinations}\label{an-overview-of-firearms-and-toolmarks-examinations}} -->
</div>
<div id="an-overview-of-firearms-and-toolmarks-examinations" class="section level4 hasAnchor" number="1.1.1.2">
<h4><span class="header-section-number">1.1.1.2</span> An Overview of Firearms and Toolmarks Examinations<a href="index.html#an-overview-of-firearms-and-toolmarks-examinations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Forensic firearms and toolmarks (F &amp; T) identification has been performed in the United States for over 100 years .
For most of this time, trained F &amp; T examiners have used a , such as the one shown in Figure <a href="#fig:comparisonMicroscope"><strong>??</strong></a>, to perform these examinations .
A comparison microscope consists of two compound microscopes that are joined via an  which allows for viewing of the stages below each microscope simultaneously under the same eyepiece.
The right image of Figure <a href="#fig:comparisonMicroscope"><strong>??</strong></a> shows an example of the view under a comparison microscope of two bullets with the white dotted line separating the two fields of view.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.6\textwidth]{images/comparisonMicroscope}  -->
<!-- } -->
<!-- \caption{\label{fig:comparisonMicroscope} A comparison microscope consists of two stages upon which evidence is placed. These stages are placed under two compound microscopes that are joined together via an optical bridge and allow for viewing of both stages simultaneously under a single eyepiece. The image on the right shows an example of a bullet viewed under a comparison microscope.}\label{fig:unnamed-chunk-7} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="images/comparisonMicroscope.png" alt="\label{fig:comparisonMicroscope} A comparison microscope consists of two stages upon which evidence is placed. These stages are placed under two compound microscopes that are joined together via an optical bridge and allow for viewing of both stages simultaneously under a single eyepiece. The image on the right shows an example of a bullet viewed under a comparison microscope." width=".6\textwidth" />
<p class="caption">
Figure 1.7:  A comparison microscope consists of two stages upon which evidence is placed. These stages are placed under two compound microscopes that are joined together via an optical bridge and allow for viewing of both stages simultaneously under a single eyepiece. The image on the right shows an example of a bullet viewed under a comparison microscope.
</p>
</div>
<p>Firearm examiners distinguish between three broad categories when characterizing a fired bullet or cartridge case: class, subclass, and individual characteristics.
 are associated with the manufacturer of the firearm that fired the bullet or cartridge case.
These include, but are not limited to, the size of ammunition chambered by the firearm, the orientation of the extractor and ejector, or the width and twist direction of the barrel rifling.
Class characteristics are often the first to be examined because they can narrow the relevant population of potential firearm sources .
For example, a 9mm cartridge case must have been fired by a firearm that can chamber 9mm ammunition.</p>
<p>If the discernible class characteristics match between two pieces of evidence, for example a cartridge case found at a crime scene and a different cartridge case fired by a suspect’s gun, then the examiner uses a comparison microscope to compare the  of the evidence.
Individual characteristics are markings attributed to imperfections on the firearm surface due to the manufacturing process, use, and wear of the tool.
For example, markings on the breech face of a barrel may form after repeated fires of the firearm.
Individual characteristics are assumed to occur randomly across different firearms.
In an examination, the examiner independently rotates and translates the stages of a comparison microscope to find the optimal matching position of the markings on the two pieces of evidence .
If the individual characteristics on two pieces of evidence are determined to agree ``sufficiently," then the examiner can conclude that they originated from the same firearm .</p>
<p> exist between the macro-level class and micro-level individual characteristics.
These characteristics relate to markings that are reproduced across a subgroup of firearms.
For example, breech faces using the same milling machine may include markings that are unique to the milling machine .
As it can be difficult to distinguish between individual and subclass characteristics during an examination, an examiner’s decision process may be affected if the existence of subclass characteristics is suspected.</p>
<p>Many F &amp; T examiners in the United States adhere to the Association of Firearms and Toolmarks Examiners (AFTE) Range of Conclusions when making their evidentiary conclusions .
According to these guidelines, there are six possible conclusions that can be made in a F &amp; T examination:</p>
<p>In general, forensic examinations first involve an examination of a ``questioned" bullet or cartridge case for identifiable toolmarks .
Markings including breech face, firing pin, chamber marks, extractor pin, and ejector impressions are categorized by their class, individual, or subclass characteristics.
If available, this information is compared to ``known source" fires obtained from a suspect’s firearm.
If known source evidence is unavailable, class characteristics from the questioned bullet can be used to narrow the relevant population and provide potential leads.
Ultimately, an examiner’s decision may be used as part of an ongoing investigation or presented at trial as expert testimony.</p>
<p>It should be noted that standard operating procedures for assessing and comparing evidence differ between forensic laboratories.
For example, some labs collapse the three possible inconclusive decisions into a single decision  or prohibit examiners from making an elimination based on differences in individual characteristics .</p>
<!-- \hypertarget{why-should-firearm-and-toolmark-identification-change}{% -->
<!-- \subsection{Why Should Firearms and Toolmarks Identification Change?}\label{why-should-firearm-and-toolmark-identification-change}} -->
</div>
</div>
<div id="why-should-firearms-and-toolmarks-identification-change" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Why Should Firearms and Toolmarks Identification Change?<a href="index.html#why-should-firearms-and-toolmarks-identification-change" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In 2009, the National Research Council released a report assessing a number of forensic disciplines including Firearms and Toolmarks analysis.
The report pointed out that F &amp; T analysis lacked a precisely defined process and that little research had been done to determine the reliability or repeatability of the methods.
Part of the recommendations from this study were to establish rigorously-validated laboratory procedures and ``develop automated techniques capable of enhancing forensic technologies ."</p>
<p>A number of studies have been performed to assess the reliability and repeatability of a firearms and toolmarks examination (non-exhaustively: ).
All of these studies report extremely low error rates when examiners are asked to make conclusions for evidence for which the authors know ground truth (i.e., whether the bullets or cartridge cases are truly matching or non-matching).
However, as pointed out in a 2016 report from the President’s Council of Advisors on Science and Technology, many of these studies, save , were not ``appropriately designed to test the foundational validity and estimate reliability ."
The report asserts that additional, properly-designed studies should be performed to more rigorously establish the scientific validity of the discipline.</p>
<p>Due to the opacity in the decision-making process, examiners have been referred to as ``black boxes" in a similar sense to black-box algorithms .
Their evidentiary conclusions are fundamentally subjective, and there is empirical evidence to suggest that conclusions differ across examiners when presented with the same evidence and even for a single examiner when presented with the same evidence on two different occasions .
Examiners rarely need to provide quantitative justification for their conclusion.
Even for qualitative justifications, it can be difficult to determine what the examiner is actually ``looking at" to arrive at their conclusion .
This suggests the need to supplement these black box decisions with transparent, objective techniques that quantitatively measure the similarity between pieces of evidence.
As stated in , efforts should be made to ``convert firearms analysis from a subjective method to an objective method" including ``developing and testing image-analysis algorithms for comparing the similarity of tool marks."
The focus of this work is on the development of an algorithm for comparing breech face impressions on cartridge cases.</p>
<!-- \hypertarget{forensic-comparison-pipelines}{% -->
<!-- \section{Forensic Comparison Pipelines}\label{forensic-comparison-pipelines}} -->
</div>
</div>
<div id="forensic-comparison-pipelines" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Forensic Comparison Pipelines<a href="index.html#forensic-comparison-pipelines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recent work in many forensic disciplines has focused on the development of algorithms to measure the similarity between pieces of evidence including glass , handwriting , shoe prints , ballistics , and toolmarks .
These algorithms often result in a numerical, non-binary (dis)similarity score for two pieces of evidence.
A non-binary score adds more nuance to an evidentiary conclusion beyond simply stating whether the evidence did or did not originate from the same source as would be the case in binary classification.
For example, the larger the similarity score, the ``more similar" the evidence.
However, a binary (or ternary, if admitting inconclusives) conclusion must ultimately be reached by an examiner.
Whether a decision should be reached based solely on results of a comparison algorithm (e.g., defining a score-based decision boundary) or if an examiner should incorporate the similarity score into their own decision-making process is still up for debate .
In this work, we view forensic comparison algorithms as a supplement to, rather than a replacement of, the forensic examination.</p>
<p>We conceptualize forensic comparison algorithms as evidence-to-classification ``pipelines."
Broadly, the steps of the pipeline include:</p>
<p>This is similar to the structure discussed in .
We add to this structure the emphasis that each step of the pipeline can be further broken-down into modularized pieces.
For example, the preprocessing step may include multiple sub-procedures to isolate a region of interest of the evidence.
Figure <a href="#fig:pipelineDiagram"><strong>??</strong></a> shows three possible variations of the cartridge case comparison pipeline as well as the parameters requiring manual specification and alternative modules.
The benefits of this modularization include easing the process of experimenting with different parameters/sub-procedures and improving the comprehensibility of the pipeline.</p>
<p>[Update the pipeline diagram]</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{images/pipelineDiagram_9-9-21}  -->
<!-- } -->
<!-- \caption{\label{fig:pipelineDiagram} Variations upon the cartridge case comparison pipeline. The first three columns detail the pipeline with different sub-procedures. The fourth columns shows the parameters that require manual specification at each step. The fifth column shows  alternative processing steps that could replace steps in the existing pipeline.}\label{fig:unnamed-chunk-8} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="images/pipelineDiagram_9-9-21.jpg" alt="\label{fig:pipelineDiagram} Variations upon the cartridge case comparison pipeline. The first three columns detail the pipeline with different sub-procedures. The fourth columns shows the parameters that require manual specification at each step. The fifth column shows  alternative processing steps that could replace steps in the existing pipeline." width="\textwidth" />
<p class="caption">
Figure 1.8:  Variations upon the cartridge case comparison pipeline. The first three columns detail the pipeline with different sub-procedures. The fourth columns shows the parameters that require manual specification at each step. The fifth column shows alternative processing steps that could replace steps in the existing pipeline.
</p>
</div>
<p>In the following sections, we detail recent advances to each of the five steps in the pipeline outlined above.
We narrow our focus to advances made in comparing F &amp; T evidence.</p>
<!-- \hypertarget{digital-representations-of-evidence}{% -->
<!-- \subsection{Digital Representations of Evidence}\label{digital-representations-of-evidence}} -->
<div id="digital-representations-of-evidence" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Digital Representations of Evidence<a href="index.html#digital-representations-of-evidence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Digital representations of cartridge case evidence commonly come in one of two modes: 2D optical images or 3D topographic scans.
A common way to take 2D optical images is to take a picture of the cartridge case under a microscope.
This implies that the digital representation of the cartridge case surface is dependent on the lighting conditions under which the picture was taken.
Some recent work has focused on comparing 2D optical images , although the use of 3D microscopes has become more prevalent to capture the surface of ballistics evidence.</p>
<p>Using a 3D microscope, we obtain scans at the micron (or micrometer) level that are more light-agnostic than a 2D image .
One common 3D scanning procedure is disc scanning confocal microscopy.
This procedure works by shining a focused beam of light on the cartridge case surface.
This light is reflected back onto a pinhole allowing a limited height range to pass through.
The microscope scans through different height range ``slices" and compiles all these slices into a single 3D topography of the cartridge case primer surface.
The Microdisplay Scan Confocal Microscope from Sensofar~Metrology is shown in Figure <a href="#fig:sensofarScanner"><strong>??</strong></a> .</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.5\textwidth]{images/sensofarScanner}  -->
<!-- } -->
<!-- \caption{\label{fig:sensofarScanner} The Microdisplay Scan Confocal Microscope from Sensofar\texttrademark\ Metrology. The cartridge case surface is captured by scanning through a range of vertical slices and compiling these slices into a single 3D topography.}\label{fig:unnamed-chunk-9} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="images/sensofarScanner.png" alt="\label{fig:sensofarScanner} The Microdisplay Scan Confocal Microscope from Sensofar\texttrademark\ Metrology. The cartridge case surface is captured by scanning through a range of vertical slices and compiling these slices into a single 3D topography." width=".5\textwidth" />
<p class="caption">
Figure 1.9:  The Microdisplay Scan Confocal Microscope from Sensofar Metrology. The cartridge case surface is captured by scanning through a range of vertical slices and compiling these slices into a single 3D topography.
</p>
</div>
<p>Figure <a href="#fig:cartridgeCaseImages"><strong>??</strong></a> shows a 2D image and 3D topography of the same cartridge case primer from .</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{images/fadul1-1_sidebyside}  -->
<!-- } -->
<!-- \caption{\label{fig:cartridgeCaseImages} A cartridge case captured using 2D confocal reflectance microscopy (left) and 3D disc scanning confocal microscopy (right).}\label{fig:unnamed-chunk-10} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="images/fadul1-1_sidebyside.PNG" alt="\label{fig:cartridgeCaseImages} A cartridge case captured using 2D confocal reflectance microscopy (left) and 3D disc scanning confocal microscopy (right)." width="\textwidth" />
<p class="caption">
Figure 1.10:  A cartridge case captured using 2D confocal reflectance microscopy (left) and 3D disc scanning confocal microscopy (right).
</p>
</div>
<p>More recently, Cadre Forensics~introduced the TopMatch-3D High-Capacity Scanner .
A tray of 15 fired cartridge cases and the scanner are shown in Figure <a href="#fig:topMatchScanner"><strong>??</strong></a> .
This scanner collects images under various lighting conditions of a gel pad into which the cartridge case surface is impressed and combines these images into a regular 2D array called a .
The physical dimensions of these objects are about 5.5 <span class="math inline">\(mm^2\)</span> captured at a resolution of 1.84 microns per pixel (1000 microns equals 1 mm).</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.7\textwidth]{images/TopMatchSystem7}  -->
<!-- } -->
<!-- \caption{\label{fig:topMatchScanner} The TopMatch-3D High-Capacity Scanner from Cadre Forensics\texttrademark\ . The scanner captures topographic scans of a gel pad into which a cartridge case surface is impressed.}\label{fig:unnamed-chunk-11} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-11"></span>
<img src="images/TopMatchSystem7.png" alt="\label{fig:topMatchScanner} The TopMatch-3D High-Capacity Scanner from Cadre Forensics\texttrademark\ . The scanner captures topographic scans of a gel pad into which a cartridge case surface is impressed." width=".7\textwidth" />
<p class="caption">
Figure 1.11:  The TopMatch-3D High-Capacity Scanner from Cadre Forensics . The scanner captures topographic scans of a gel pad into which a cartridge case surface is impressed.
</p>
</div>
<p>When applied to ballistics evidence, these 3D scans are commonly stored in the ISO standard x3p file format .
x3p is a container consisting of a single surface matrix representing the height value of the surface and metadata concerning the parameters under which the scan was taken as shown in Figure <a href="#fig:x3pFlowchart"><strong>??</strong></a> .
It has been empirically demonstrated that comparing 3D topographic scans of cartridge case evidence leads to more accurate conclusions compared to comparing 2D optical images of the same evidence .</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.7\textwidth]{images/x3pFlowchart}  -->
<!-- } -->
<!-- \caption{\label{fig:x3pFlowchart} The hierarchy of information stored in the x3p file format for both bullet and cartridge case evidence.}\label{fig:unnamed-chunk-12} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-12"></span>
<img src="images/x3pFlowchart.jpg" alt="\label{fig:x3pFlowchart} The hierarchy of information stored in the x3p file format for both bullet and cartridge case evidence." width=".7\textwidth" />
<p class="caption">
Figure 1.12:  The hierarchy of information stored in the x3p file format for both bullet and cartridge case evidence.
</p>
</div>
<!-- \hypertarget{preprocessing-procedures-for-forensic-data}{% -->
<!-- \subsection{Preprocessing Procedures for Forensic Data}\label{preprocessing-procedures-for-forensic-data}} -->
</div>
<div id="preprocessing-procedures-for-forensic-data" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Preprocessing Procedures for Forensic Data<a href="index.html#preprocessing-procedures-for-forensic-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When capturing the surface of a cartridge case, the result is bound to contain extraneous regions due to the incongruity between the circular primer and the rectangular array in which the surface data are stored.
Figure <a href="#fig:cartridgeCaseImages"><strong>??</strong></a> shows an example of a 2D image and 3D scan of the same cartridge case.
We can see, for example, that the corners of these arrays include non-primer regions of the cartridge case surface.
Additionally, the center of the cartridge case primer features an impression left by the firing pin during the firing process.
In most applications, impressions left by the firing pin are compared separately from the breech face impressions .
As the focus of this work is on the comparison of breech face impressions between two cartridge cases, only the annular region surrounding the firing pin impression is of interest.
The annular breech face impression region must be segmented away from the rest of the captured surface.</p>
<p>Both the 2D optical and 3D topographic representations of cartridge case surfaces are fundamentally pictorial in nature.
As such, many image processing and computer vision techniques are used to automatically isolate the breech face impression region.
 uses a combination of histogram equalization, Canny edge detection, and morphological operations to isolate the breech face impressions in 2D images.
Various types of Gaussian filters are commonly employed to remove unwanted structure.
 uses a low-pass Gaussian filter that removes noise via a Gaussian-weighted moving average operation.
 use a bandpass Gaussian filter, which simultaneously performs the function of a low-pass filter along with a high-pass filter to remove global structure from the scan.
Other versions of the bandpass filter are used in  that accomplish tasks such as omitting outlier surface values or addressing boundary effects .</p>
<p>Instead of using automatic procedures, others have used subjective human intervention to isolate the breech face impressions.
For example,  indicate that cartridge cases are ``manually trimming to extract the breech face impression of interest."
In , examiners manually identify the borders of the breech face impression region by placing points around an image of the cartridge case primer.</p>
<!-- \hypertarget{forensic-data-feature-extraction}{% -->
<!-- \subsection{Forensic Data Feature Extraction}\label{forensic-data-feature-extraction}} -->
</div>
<div id="forensic-data-feature-extraction" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Forensic Data Feature Extraction<a href="index.html#forensic-data-feature-extraction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After applying the preprocessing procedures to two cartridge case scans, their breech face impressions are compared and similarity features are extracted.
Given that the cartridge cases at this point are represented as high-dimensional matrices, this can be thought of as a dimensionality reduction of the high-dimensional surface arrays to a set of similarity statistics.</p>
<p>A variety of features have been proposed to quantify the similarity between two cartridge case surface arrays.
 propose calculating the cross-correlation function (CCF) value between two cartridge cases across a grid of rotations.
It is assumed that the CCF will to be larger around the ``true" rotation for matching cartridge case pairs than for non-matching pairs.
 proposed combining the CCF between the two aligned scans with the element-wise median Euclidean distance and median difference between the normal vectors at each point of the surface.
Later,  applied Principal Component Analysis to reduce these three features down to two principal components onto which a 2D Kernel Density Estimator could be fit.</p>
<p>Pertinent to this work is the cell-based comparison procedure originally outlined in .
The underlying assumption of  is similar to that of : that two matching cartridge cases will exhibit higher similarity when they are ``close" to being correctly aligned.
While  measured similarity using the CCF between the two full scans,  proposes partitioning the scans into a grid of ``correlation cells" and counting the number of similar cells between the two scans.
The rationale behind this procedure is that many cartridge case scans have regions that do not contain discriminatory markings.
As such, comparing full scans may result in a lower correlation than if one were to focus on the highly-discriminatory regions.
In theory, dividing the scans into cells allows for the identification of these regions.</p>
<p>After breaking a scan into a grid of cells, each cell is compared to the other scan to identify the rotation and translation, known together as the , at which the cross-correlation is maximized.
 assume that the cells from a truly matching pair of cartridge cases will ``agree" on their registration in the other scan.
Details of this procedure are provided in Chapter 2.</p>
<!-- \hypertarget{similarity-scores-for-forensic-data}{% -->
<!-- \subsection{Similarity Scores for Forensic Data}\label{similarity-scores-for-forensic-data}} -->
</div>
<div id="similarity-scores-for-forensic-data" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Similarity Scores for Forensic Data<a href="index.html#similarity-scores-for-forensic-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Following feature extraction, the dimensionality of these features is further reduced to a low-dimensional, usually univariate, similarity score.</p>
<p>After calculating the CCF across various possible registrations,  propose using the maximum observed CCF value as the univariate similarity score.
In this case, a binary classification can be achieved by setting a CCF threshold above which pairs are classified as ``matches" and below which as ``non-matches."
 proposes setting a CCF cut-off that maximizes the precision and recall in a training set of pairwise comparisons.</p>
<p> use a training set to fit two 2D kernel density estimates to a set of features from matching and non-matching comparisons.
Using these estimates, they are able to estimate the score-based likelihood ratio (SLR) for a new set of features.
This SLR can be viewed as a similarity score .</p>
<p>In the case of the cell-based comparison procedure discussed above, the total number of cells that are deemed ``congruent matching" is used as a similarity score.
The criteria used to define ``congruent matching" has changed across papers  and will be discussed in greater detail in [Chapter].
The authors of these papers have consistently used a decision boundary of six ``Congruent Matching Cells" to distinguish matches from non-matches.</p>
<p> applies the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm  to the features from the cell-based comparison procedure to determine if any clusters form amongst the per-cell estimated registration values.
This is based on the assumption that any cells that come to a consensus on their registration should form a cluster in translation <span class="math inline">\((x,y)\)</span> and rotation <span class="math inline">\(\theta\)</span> space.
 proposes a binary classifier based on whether any clusters are identified by the DBSCAN algorithm .
If a cluster is found for a particular pairwise comparison, then that pair is classified as a ``match" and otherwise as a ``non-match."</p>
<!-- \hypertarget{reproducibility-of-comparison-pipelines}{% -->
<!-- \subsection{Reproducibility of Comparison Pipelines}\label{reproducibility-of-comparison-pipelines}} -->
</div>
<div id="reproducibility-of-comparison-pipelines" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Reproducibility of Comparison Pipelines<a href="index.html#reproducibility-of-comparison-pipelines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p> defines reproducibility as ``obtaining consistent computational results using the same input data, computational steps, methods, code, and conditions of analysis."
While not exact in their definition of ``consistent," the authors do assert that, barring a few exceptions, it is reasonable to expect that the results obtained by a second researcher, after applying the exact same processing steps to the exact same data, be the exact same as the original results.
Among the exceptions given is if the original researcher had made a mistake in writing the original source code.
In either case, they assert that ``a study’s data and code have to be available in order for others to reproduce and confirm results."
Researchers can not only easily verify the results given data and code, they can also incorporate the materials into their own research and thus improve or accelerate discovery .</p>
<p>A number of studies indicate that computationally reproducible research is sparse across various disciplines.
 and  studied the reproducibility of articles sampled from the journals  and the , respectively.
In the former,  found that only 3 of 204 randomly selected articles from  were ``straightforward to reproduce with minimal effort;" despite a journal policy requiring that all code and data used in the paper be made available to any reader.
In the latter,  found that zero of 306 randomly selected articles from the  were ``straightforward to reproduce with minimal effort" and, at best, that five articles were ``reproducible after some tweaking."
Similar findings were found in  (29 of 59 economic papers reproducible),  (zero of 268 biomedical papers provided raw data and 1 in 268 linked to a full study protocol),  (50% or more published articles include data or code in only 27 of 333 economics journals), and  (24 of 400 AI conference papers included code).
A common recommendation amongst these authors is the establishment of rigorous standards for reproducibility.
This includes making code and data used in a paper easily-accessible to readers.</p>
<p>Infrastructure already exists to ease the processing of developing, maintaining, and sharing open-source code and data.
Data repositories such as the NIST Ballistics Toolmark Research Database  provide open access to raw data.
 discuss the use of package managers such as Conda , container software such as Docker (<a href="https://www.docker.com/">https://www.docker.com/</a>), and virtual machine software to preserve the entire data analysis environment in-perpetuity.
For situations in which VMs or containers aren’t available, software such as the <code>manager</code> R package allows users to ``compare package inventories across machines, users, and time to identify changes in functions and objects ."
 reference repositories like Bioconductor  that make it easy to document and distribute code.
Further, software such as the <code>knitr</code> R package  enable ``literate programming" in which prose and executed code can be interwoven to make it easier to understand the code’s function.
These tools make data, code, and derivative research findings more accessible, in terms of both acquisition and comprehensibility, to consumers and fellow researchers.</p>
<!-- \hypertarget{diagnostic-tools}{% -->
<!-- \section{Diagnostic Tools}\label{diagnostic-tools}} -->
</div>
</div>
<div id="diagnostic-tools" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> Diagnostic Tools<a href="index.html#diagnostic-tools" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Forensic examiners often provide expert testimony in court cases.
As part of this testimony, an examiner is allowed to not only provide facts about the outcome of a forensic examination, but also their opinion about what the results mean.
A party to a court may challenge the examiner on the validity of the underlying scientific method or whether they interpreted the results correctly .
In these situations, examiners need to be able to explain the process by which they reached an evidentiary conclusion to the fact finders of the case; namely, the judge or jury.
As algorithms are more often used in forensic examinations, the technical knowledge required to understand and explain an algorithm to lay-people has increased.
While in some cases the authors of the algorithm have been willing to provide testimony to establish the validity of the algorithm , this will become less viable as algorithms become more prevalent.
Indeed, even the most elegant improvements to an algorithm may be moot if an examiner can’t explain the improvements in their testimony.</p>
<p>The resources required to educate examiners on the use of highly technical algorithms makes additional training seem currently implausible.
An alternative is to develop algorithms from the ground-up to be intuitive for examiners to understand and explain to others.
 refers to the ability to identify the factors that contributed to the results of an algorithm .
For example, understanding ``why" a classifier predicted one class over another.</p>
<p>Myriad techniques exist to explain the results of an algorithm.
These range from identifying instances of the training set that illuminate how the model operates  to fitting more transparent models that approximate the complex model accurately  to explaining the behavior of the algorithm in a small region of interest .
Many of these methods require additional technical knowledge to interpret these explanations.</p>
<p>A less technical approach is to use visualizations that facilitate understanding of model behavior.
Properly constructed visuals enable both exploratory data analysis and diagnostics .
Given that many of the procedures by which cartridge case evidence is captured, processed, and compared are based on image processing techniques, a visual diagnostic is an intuitive mode of explanation for experts and lay-people alike.
In this work, we develop a suite of visual diagnostic tools that can be used to explain the behavior of the cartridge case comparison pipeline.</p>
<p>[Discuss tidyverse tools for visual diagnostics. ggplot2, etc. Emphasize that rshiny enables users to actually physically engage with the analysis process.]</p>
<!-- \hypertarget{automating-and-improving-the-cartridge-case-identification-pipeline}{% -->
<!-- \section{Automating and Improving the Cartridge Case Identification Pipeline}\label{automating-and-improving-the-cartridge-case-identification-pipeline}} -->
</div>
<div id="automating-and-improving-the-cartridge-case-identification-pipeline" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> Automating and Improving the Cartridge Case Identification Pipeline<a href="index.html#automating-and-improving-the-cartridge-case-identification-pipeline" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we review preliminaries needed to understand various sub-routines of the cartridge case comparison pipeline.</p>
<!-- \hypertarget{image-processing-techniques}{% -->
<!-- \subsection{Image Processing Techniques}\label{image-processing-techniques}} -->
<div id="image-processing-techniques" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Image Processing Techniques<a href="index.html#image-processing-techniques" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We first review image processing and computer vision algorithms that are commonly used in cartridge case comparison algorithms.
Throughout this section, let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> denote two images.
Define these images to be 2D arrays of a given size where <span class="math inline">\(A[m,n]\)</span> and <span class="math inline">\(B[m,n]\)</span> each map to a spatially-ordered measurement value.
For example, the measurement may be the height <span class="math inline">\(h\)</span> value of a cartridge case surface at a particular <span class="math inline">\([m,n]\)</span> location.</p>
<!-- \hypertarget{image-registration}{% -->
<!-- \subsubsection{Image Registration}\label{image-registration}} -->
<div id="image-registration" class="section level4 hasAnchor" number="1.4.1.1">
<h4><span class="header-section-number">1.4.1.1</span> Image Registration<a href="index.html#image-registration" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Image registration involves transforming one image to align with another image .
For example, in the case of object or facial recognition, one may be interested in finding a template image in another image.
For images <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, image registration can be defined as a mapping between two images:
<span class="math display">\[\begin{align*}
B[m,n] = f(A[m,n])
\end{align*}\]</span>
where <span class="math inline">\(f\)</span> is a 2D spatial-coordinate transformation.</p>
<p>In our application, <span class="math inline">\(f\)</span> will represent an affine transformation of the Cartesian coordinate space composed of a translation and rotation.
This transformation commonly has three parameters: <span class="math inline">\(\Delta x, \Delta y, \theta\)</span> which map a point <span class="math inline">\((x_1, y_1)\)</span> of the first image to a point <span class="math inline">\((x_2,y_2)\)</span> of the second image:
<span class="math display">\[\begin{align*}
\begin{pmatrix}
x_2 \\
y_2
\end{pmatrix} = 
\begin{pmatrix}
\Delta x \\
\Delta y
\end{pmatrix} + 
\begin{pmatrix}
\cos(\theta) &amp; -\sin(\theta) \\
\sin(\theta) &amp; \cos(\theta)
\end{pmatrix}
\begin{pmatrix}
x_1 \\
y_1
\end{pmatrix}.
\end{align*}\]</span></p>
<p>A transformation <span class="math inline">\(f(\cdot,\cdot;\pmb{t}^*)\)</span>, equivalently a parameter vector <span class="math inline">\(\pmb{t}^* \in \pmb{T}\)</span>, is selected such that it optimizes similarity metric <span class="math inline">\(s(\cdot,\cdot)\)</span> between the two images:
<span class="math display">\[\begin{align*}
\pmb{t}^* \equiv \arg \max_{\pmb{t} \in \pmb{T}} s(A[m,n],f(B[m,n];\pmb{t})).
\end{align*}\]</span></p>
<p>In our application, the set of possible parameters is <span class="math inline">\(\pmb{T} = \mathbb{Z} \times \mathbb{Z} \times [0,2\pi)\)</span> representing discrete-index horizontal and vertical translations (positive meaning up/right and negative meaning down/left) and a rotation.
Commonly, the  (CCF) is used as the similarity metric.
For a <span class="math inline">\(P \times Q\)</span> ``reference" image <span class="math inline">\(A\)</span> and <span class="math inline">\(M \times N\)</span> ``template" image <span class="math inline">\(B\)</span>, the cross-correlation function, denoted <span class="math inline">\(A \star B\)</span>, is used as a similarity metric.
The cross-correlation function measures the similarity between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for each translation:
<span class="math display">\[\begin{align*}
(A \star B)[m,n] = \sum_{i=1}^M \sum_{j=1}^N A[i,j]B[(i + m),(j + n)]
\end{align*}\]</span>
where <span class="math inline">\(1 \leq m \leq M + P - 1\)</span> and <span class="math inline">\(1 \leq n \leq N + Q - 1\)</span>.
By this definition, <span class="math inline">\(A \star B\)</span> is a 2D array of dimension <span class="math inline">\(M + P - 1 \times N + Q - 1\)</span> in which the <span class="math inline">\([m,n]\)</span>-th element quantifies the similarity between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> when <span class="math inline">\(B\)</span> is translated <span class="math inline">\(m\)</span> elements horizontally and <span class="math inline">\(n\)</span> elements vertically.
For interpretability, the CCF is commonly normalized between -1 and 1.</p>
<p>Using the CCF as a similarity metric, we can determine the translations <span class="math inline">\([m^*, n^*]\)</span> at which images <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> attain the maximum CCF value:
<span class="math display">\[\begin{align*}
[m^{\dagger},n^{\dagger}] \equiv \arg \max_{[m,n]} (A \star B)[m,n].
\end{align*}\]</span>
To determine the optimal rotation, we calculate the maximum CCF value across a range of rotations of image <span class="math inline">\(B\)</span>.
If <span class="math inline">\(B_\theta\)</span> denotes image <span class="math inline">\(B\)</span> rotated by an angle <span class="math inline">\(\theta \in [0,2\pi)\)</span>, then the estimated registration <span class="math inline">\([m^*,n^*,\theta^*]\)</span> is given by:
<span class="math display">\[\begin{align*}
[m^*,n^*,\theta^*] \equiv \arg \max_{[m,n,\theta]} (A \star B_{\theta})[m,n].
\end{align*}\]</span>
In implementation we consider a discrete grid of rotations <span class="math inline">\(\pmb{\Theta} \subset [0,2 \pi)\)</span>.
The overall registration procedure is given by:</p>
<p>Based on the definition given above, the CCF is computationally taxing.
In image processing, it is common to use an implementation based on the Fast Fourier Transform .
This implementation leverages the Cross-Correlation Theorem, which states that for images <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> the CCF can be expressed in terms of a frequency-domain pointwise product:
<span class="math display">\[
(A \star B)[m,n] = \mathcal{F}^{-1}\left(\overline{\mathcal{F}(A)} \odot \mathcal{F}(B)\right)[m,n]
\]</span>
where <span class="math inline">\(\mathcal{F}\)</span> and <span class="math inline">\(\mathcal{F}^{-1}\)</span> denote the discrete Fourier and inverse discrete Fourier transforms, respectively, and <span class="math inline">\(\overline{\mathcal{F}(A)}\)</span> denotes the complex conjugate .
Because the product on the right-hand side is calculated pointwise, this result allows us to trade the moving sum computations from the definition of the CCF for two forward Fourier transformations, a pointwise product, and an inverse Fourier transformation.
The Fast Fourier Transform (FFT) algorithm can be used to reduce the computational load considerably.</p>
<p>Figure <a href="#fig:ccfTranslationExample"><strong>??</strong></a> shows an example of two images <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> of dimension <span class="math inline">\(100 \times 100\)</span> and <span class="math inline">\(21 \times 21\)</span>, respectively.
The white boxes in both of the images are of dimension <span class="math inline">\(10 \times 10\)</span>.
The box in image A is centered on index [30,50] while the box in image B is centered on index [11,11].
The right image shows the result of calculating the CCF using image <span class="math inline">\(A\)</span> as reference and <span class="math inline">\(B\)</span> as template.
We see that the CCF achieves a maximum of 1, indicating a perfect match, at the translation value of <span class="math inline">\([m^\dagger,n^\dagger] = [22,-2]\)</span>.
This represents that if image B were overlaid onto image A such that their center indices coincided, then image B would need to be shifted 22 units ``up" and 2 units ``left" to match perfectly with image A.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-13-1}  -->
<!-- } -->
<!-- \caption{\label{fig:ccfTranslationExample} (Left) A reference image $A$ and template image $B$ both featuring a white box of dimension $10 \times 10$. (Right) The cross-correlation function (CCF) between $A$ and $B$. The index at which the CCF is maximized represents the translation at which $A$ and $B$ are most similar.}\label{fig:unnamed-chunk-13} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-13"></span>
<img src="figures/unnamed-chunk-13-1.png" alt="\label{fig:ccfTranslationExample} (Left) A reference image $A$ and template image $B$ both featuring a white box of dimension $10 \times 10$. (Right) The cross-correlation function (CCF) between $A$ and $B$. The index at which the CCF is maximized represents the translation at which $A$ and $B$ are most similar." width="\textwidth" />
<p class="caption">
Figure 1.13:  (Left) A reference image <span class="math inline">\(A\)</span> and template image <span class="math inline">\(B\)</span> both featuring a white box of dimension <span class="math inline">\(10 \times 10\)</span>. (Right) The cross-correlation function (CCF) between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. The index at which the CCF is maximized represents the translation at which <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are most similar.
</p>
</div>
<!-- \hypertarget{gaussian-filters}{% -->
<!-- \subsubsection{Gaussian Filters}\label{gaussian-filters}} -->
</div>
<div id="gaussian-filters" class="section level4 hasAnchor" number="1.4.1.2">
<h4><span class="header-section-number">1.4.1.2</span> Gaussian Filters<a href="index.html#gaussian-filters" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In image processing, a Gaussian filter (equivalently, blur or smoother) is mathematical operator that imputes the values in an image using a locally-weighted sum of surrounding values.
In our application, a Gaussian filter, specifically a  Gaussian filter, is used to smooth the surface values of a cartridge case scan.
The weights are dictated according to the Gaussian function of a chosen standard deviation <span class="math inline">\(\sigma\)</span> given by:
<span class="math display">\[
f(x,y;\sigma) = \frac{1}{2\pi\sigma^2} \exp\left(-\frac{1}{2\sigma^2}(x^2 + y^2)\right).
\]</span>
It is common to populate a 2D array with the values of the Gaussian function treating the center index as the origin.
Such an array is called a .
An example of a <span class="math inline">\(3 \times 3\)</span> Gaussian kernel with standard deviation <span class="math inline">\(\sigma = 1\)</span> is given below.
<span class="math display">\[
K = 
\begin{pmatrix}
0.075 &amp; 0.124 &amp; 0.075 \\
0.124 &amp; 0.204 &amp; 0.124 \\
0.075 &amp; 0.124 &amp; 0.075
\end{pmatrix}.
\]</span></p>
<p>For an image <span class="math inline">\(A\)</span> and Gaussian kernel <span class="math inline">\(K\)</span> with standard deviation <span class="math inline">\(\sigma\)</span>, the lowpass filtered version of <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(A_{lp,\sigma}\)</span> is given by:
<span class="math display">\[
A_{lp,\sigma}[m,n] = \mathcal{F}^{-1}\left(\mathcal{F}(A) \odot \mathcal{F}(K)\right)[m,n].
\]</span>
You will note the similarity between this operation, known as , and the calculation of the CCF given above .</p>
<p>Figure <a href="#fig:gaussianFilterExample"><strong>??</strong></a> shows an image <span class="math inline">\(A\)</span> of a box undergoing the injection of Gaussian noise (noise standard deviation <span class="math inline">\(\sigma_n = 0.3\)</span>) followed by the application of various filters.
While the box is obscured due to noise in the middle image, the lowpass filter (kernel standard deviation <span class="math inline">\(\sigma_k = 2\)</span>) recovers some of the definition of the box seen in the original image <span class="math inline">\(A\)</span>.</p>
<p>If a lowpass filter ``smooths" the values of an image, then a  filter performs a ``sharpening" operation.
More specifically, for image <span class="math inline">\(A\)</span> and kernel standard deviation <span class="math inline">\(\sigma\)</span>, the highpass filtered version <span class="math inline">\(A_{hp}\)</span> can be defined as:
<span class="math display">\[\begin{align*}
A_{hp,\sigma} = A - A_{lp,\sigma}.
\end{align*}\]</span>
The highpass filter therefore removes larger-scale (smooth) structure from an image and retains high-frequency structure such as noise or edges.
An example of a highpass-filtered image <span class="math inline">\(A\)</span> is shown in Figure <a href="#fig:gaussianFilterExample"><strong>??</strong></a>.
We can see that the smooth interior of the box is effectively removed from the image while the edges are preserved.</p>
<p>Finally, the bandpass filter performs the highpass sharpening followed by the lowpass smoothing operations.
Generally, the highpass kernel’s standard deviation will be considerably larger than that of the lowpass kernel.
This leads to retaining sharp edges while also reducing noise.
An example of a bandpass filtered image <span class="math inline">\(A\)</span> is shown in Figure <a href="#fig:gaussianFilterExample"><strong>??</strong></a>.
We see that the edges of the box are better-preserved compared to the lowpass filter figure while the interior of the box is better-preserved compared to the highpass filter figure.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-14-1}  -->
<!-- } -->
<!-- \caption{\label{fig:gaussianFilterExample} An image $A$ of a box undergoing various filtering operations.}\label{fig:unnamed-chunk-14} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="figures/unnamed-chunk-14-1.png" alt="\label{fig:gaussianFilterExample} An image $A$ of a box undergoing various filtering operations." width="\textwidth" />
<p class="caption">
Figure 1.14:  An image <span class="math inline">\(A\)</span> of a box undergoing various filtering operations.
</p>
</div>
<p>Variations on the standard Gaussian filter include the ``robust" Gaussian regression filter.
This filter fluctuates between a filter step, which applies a Gaussian filter, and outlier step, which identifies and omits outlier observations from the next filter step .
Another alternative, the ``edge preserving" filter, adapts the kernel weights when approaching the boundary of an image to mitigate so-called  .</p>
<!-- \hypertarget{morphological-operations}{% -->
<!-- \subsubsection{Morphological Operations}\label{morphological-operations}} -->
</div>
<div id="morphological-operations" class="section level4 hasAnchor" number="1.4.1.3">
<h4><span class="header-section-number">1.4.1.3</span> Morphological Operations<a href="index.html#morphological-operations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Mathematical morphology refers to a theory and collection of image processing techniques for geometrical structures .
In our application, these geometrical structures are cartridge case scans; specifically, binarized versions of these scans representing whether a particular pixel does or does not contain part of the cartridge case surface.</p>
<p>Two fundamental operations in mathematical morphology are  and  .
For our purposes, these are both set operations on binary (black and white) images.
We classify the set of black and white pixels as the background and foreground of the image, respectively.
For an image <span class="math inline">\(A\)</span>, let <span class="math inline">\(W = \{[m,n] : A[m,n] = 1\}\)</span> denote the foreground of <span class="math inline">\(A\)</span>, meaning <span class="math inline">\(W^c\)</span> represents the background.
An example of a <span class="math inline">\(7 \times 7\)</span> binary image <span class="math inline">\(A\)</span> with <span class="math inline">\(W = \{[3,3],[3,4],[3,5],[4,3],[4,4],[4,5],[5,3],[5,4],[5,5]\}\)</span> is given below.
<span class="math display">\[
A = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}
\]</span></p>
<p>A  is a second, typically small, array <span class="math inline">\(B\)</span> of ones that affects the amount of dilation or erosion applied to <span class="math inline">\(W\)</span> within <span class="math inline">\(A\)</span>.
For simplicity, the indexing of the structuring element uses the center index as the origin.
For example, a <span class="math inline">\(3 \times 3\)</span> structuring element is given by <span class="math inline">\(B = \{(-1,-1),(-1,0),(-1,1),(-1,0),(0,0),(0,1),(1,-1),(1,0),(1,1)\}\)</span> or visually:
<span class="math display">\[
B = 
\begin{pmatrix}
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 \\
1 &amp; 1 &amp; 1 
\end{pmatrix}
\]</span></p>
<p>The dilation of <span class="math inline">\(W\)</span> by <span class="math inline">\(B\)</span>, denoted <span class="math inline">\(W \oplus B\)</span>, is defined by
<span class="math display">\[
W \oplus B = \{[m,n] \in A : [m,n] = [i,j] + [k,l] \text{ for } [i,j] \in W \text{ and } [k,l] \in B\}
\]</span>
where the index arithmetic is performed element-wise.
Alternatively, if <span class="math inline">\(W_{[k,l]}\)</span> represents the translation of region <span class="math inline">\(W\)</span> within <span class="math inline">\(A\)</span> by <span class="math inline">\(k\)</span> units row-wise and <span class="math inline">\(l\)</span> units column-wise for <span class="math inline">\([k,l] \in B\)</span></p>
<p>In this example,
<span class="math display">\[W \oplus B = \{[3,2],[3,3],[3,4],[3,5],[3,6],[4,2],[4,3],[4,4],[4,5],[4,6],[5,2],[5,3],[5,4],[5,5],[5,6]\}\]</span>
or visually:
<span class="math display">\[
W \oplus B = 
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\]</span>
We see that the dilation operation by <span class="math inline">\(B\)</span> has the effect of growing the region <span class="math inline">\(W\)</span> inside of <span class="math inline">\(A\)</span> by one index in each direction.</p>
<p>In contrast, erosion has the effect of shrinking a selected region.
More precisely, the erosion of <span class="math inline">\(W\)</span> by <span class="math inline">\(B\)</span> is defined by
<span class="math display">\[
A \ominus B = \{[m,n] \in A: [m,n] + [k,l] \in A \text{ for every } [k,l] \in B\}.
\]</span></p>
<p>Using the same example as above, <span class="math inline">\(W \ominus B = \{[3,3]\}\)</span> or visually:
<span class="math display">\[
W \ominus B =
\begin{pmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{pmatrix}.
\]</span>
Erosion by <span class="math inline">\(B\)</span> therefore shrinks the region <span class="math inline">\(W\)</span> in <span class="math inline">\(A\)</span> by one index in each direction.</p>
<p>Figure <a href="#fig:dilationErosionExample"><strong>??</strong></a> shows the example considered here in terms of black and white representations of <span class="math inline">\(A\)</span> undergoing dilation and erosion by <span class="math inline">\(B\)</span>.
In practice, there may be two or more disconnected foreground regions in <span class="math inline">\(A\)</span> to which dilation or erosion can be independently applied.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-15-1}  -->
<!-- } -->
<!-- \caption{\label{fig:dilationErosionExample} A $7 \times 7$ image $A$ featuring a $3 \times 3$ box undergoing dilation and erosion by a $3 \times 3$ structuring element $B$.}\label{fig:unnamed-chunk-15} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-15"></span>
<img src="figures/unnamed-chunk-15-1.png" alt="\label{fig:dilationErosionExample} A $7 \times 7$ image $A$ featuring a $3 \times 3$ box undergoing dilation and erosion by a $3 \times 3$ structuring element $B$." width="\textwidth" />
<p class="caption">
Figure 1.15:  A <span class="math inline">\(7 \times 7\)</span> image <span class="math inline">\(A\)</span> featuring a <span class="math inline">\(3 \times 3\)</span> box undergoing dilation and erosion by a <span class="math inline">\(3 \times 3\)</span> structuring element <span class="math inline">\(B\)</span>.
</p>
</div>
<!-- \hypertarget{density-based-spatial-clustering-of-applications-with-noise}{% -->
<!-- \subsection{Density-Based Spatial Clustering of Applications with Noise}\label{density-based-spatial-clustering-of-applications-with-noise}} -->
</div>
</div>
<div id="density-based-spatial-clustering-of-applications-with-noise" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Density-Based Spatial Clustering of Applications with Noise<a href="index.html#density-based-spatial-clustering-of-applications-with-noise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is a clustering procedure that assigns observations to clusters if they are in a region of high observation density .
Otherwise they are classified as ``noise" points.</p>
<p>Let <span class="math inline">\(D\)</span> represent a <span class="math inline">\(n \times p\)</span> data set (<span class="math inline">\(n\)</span> observations, each of dimension <span class="math inline">\(p\)</span>) and consider observations <span class="math inline">\(x,y,z \in D\)</span>.
The DBSCAN algorithm relies on the notion of <span class="math inline">\(\epsilon\)</span>-neighborhoods.
Given some neighborhood radius <span class="math inline">\(\epsilon \in \mathbb{R}\)</span> and distance metric <span class="math inline">\(d\)</span>, <span class="math inline">\(y\)</span> is in the <span class="math inline">\(\epsilon\)</span>-neighborhood of <span class="math inline">\(x\)</span> if <span class="math inline">\(d(x,y) \leq \epsilon\)</span>.
The  of <span class="math inline">\(x\)</span> is defined as the set <span class="math inline">\(N_{\epsilon}(x) = \{y \in D : d(x,y) \leq \epsilon\}\)</span>.
Given a minimum number of points <span class="math inline">\(Minpts \in \mathbb{N}\)</span>, observation <span class="math inline">\(x\)</span> is called a  with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> if <span class="math inline">\(|N_{\epsilon}(x)| \geq Minpts\)</span>.
Both <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> are selected by the user.</p>
<p>Figure <a href="#fig:epsNeighborhoodExample"><strong>??</strong></a> shows an example of 10 points on the Cartesian plane.
An <span class="math inline">\(\epsilon\)</span>-neighborhood using the Euclidean distance metric and <span class="math inline">\(\epsilon = 3\)</span> is drawn around an observation <span class="math inline">\(x\)</span> located at <span class="math inline">\((3,2)\)</span>.
Points inside the circle are neighbors of <span class="math inline">\(x\)</span>.
If, for example, <span class="math inline">\(Minpts = 2\)</span>, then <span class="math inline">\(x\)</span> would be considered a core point.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-16-1}  -->
<!-- } -->
<!-- \caption{\label{fig:epsNeighborhoodExample} An $\epsilon$-neighborhood around a observation located at $(3,2)$ for $\epsilon = 3$. Points are labeled based on whether they are neighbors to this observation or not}\label{fig:unnamed-chunk-16} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="figures/unnamed-chunk-16-1.png" alt="\label{fig:epsNeighborhoodExample} An $\epsilon$-neighborhood around a observation located at $(3,2)$ for $\epsilon = 3$. Points are labeled based on whether they are neighbors to this observation or not." width="\textwidth" />
<p class="caption">
Figure 1.16:  An <span class="math inline">\(\epsilon\)</span>-neighborhood around a observation located at <span class="math inline">\((3,2)\)</span> for <span class="math inline">\(\epsilon = 3\)</span>. Points are labeled based on whether they are neighbors to this observation or not.
</p>
</div>
<p>To identify regions of high observation density, two relational notions,  and , are used.
A point <span class="math inline">\(y\)</span> is  to a point <span class="math inline">\(x\)</span> if <span class="math inline">\(x\)</span> is a core point and <span class="math inline">\(y \in N_{\epsilon}(x)\)</span>.
In the example in Figure <a href="#fig:epsNeighborhoodExample"><strong>??</strong></a>, the observation located at <span class="math inline">\((1,0)\)</span> is directly density-reachable to the observation located at <span class="math inline">\((3,2)\)</span>.
More broadly, a point <span class="math inline">\(x_m\)</span> is  to a point <span class="math inline">\(x_1\)</span> if there exists a chain of observations <span class="math inline">\(x_1,x_2,...,x_{m-1},x_m\)</span> such that <span class="math inline">\(x_{i+1}\)</span> is directly density-reachable from <span class="math inline">\(x_i\)</span>, <span class="math inline">\(i = 1,...,n\)</span>.
Figure <a href="#fig:densityReachableExample"><strong>??</strong></a> shows an example of three density-reachable points located at <span class="math inline">\((1,0), (3,2)\)</span>, and <span class="math inline">\((4,4)\)</span> using <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>.
All three points are core points and although the points located at <span class="math inline">\((4,4)\)</span> and <span class="math inline">\((1,3)\)</span> are not neighbors, they share a neighbor in the point located at <span class="math inline">\((3,2)\)</span> and are thus density-reachable.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-17-1}  -->
<!-- } -->
<!-- \caption{\label{fig:densityReachableExample} An example of three points that are density-reachable with respect to $\epsilon = 3$ and $Minpts = 2$.}\label{fig:unnamed-chunk-17} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-17"></span>
<img src="figures/unnamed-chunk-17-1.png" alt="\label{fig:densityReachableExample} An example of three points that are density-reachable with respect to $\epsilon = 3$ and $Minpts = 2$." width="\textwidth" />
<p class="caption">
Figure 1.17:  An example of three points that are density-reachable with respect to <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>.
</p>
</div>
<p>Finally, a point <span class="math inline">\(y\)</span> is  to a point <span class="math inline">\(x\)</span> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> if there is a point <span class="math inline">\(z\)</span> such that both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are density-reachable to <span class="math inline">\(z\)</span> (with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span>).
While density-reachability requires that all points in-between two points be core points, density-connectivity extends the notion of ``neighbors of neighbors" to include points that are merely within the neighborhood of density-reachable points.
Figure <a href="#fig:densityConnectedExample"><strong>??</strong></a> illustrates how the points located at <span class="math inline">\((4,7)\)</span> and <span class="math inline">\((0,-2)\)</span> are density-connected while not being density-reachable.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-18-1}  -->
<!-- } -->
<!-- \caption{\label{fig:densityConnectedExample} An example of two points that are density-connected, but not density-reachable, with respect to $\epsilon = 3$ and $Minpts = 2$.}\label{fig:unnamed-chunk-18} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-18"></span>
<img src="figures/unnamed-chunk-18-1.png" alt="\label{fig:densityConnectedExample} An example of two points that are density-connected, but not density-reachable, with respect to $\epsilon = 3$ and $Minpts = 2$." width="\textwidth" />
<p class="caption">
Figure 1.18:  An example of two points that are density-connected, but not density-reachable, with respect to <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>.
</p>
</div>
<p>A  <span class="math inline">\(C \subset D\)</span> with respect to <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(Minpts\)</span> satisfies the following conditions:</p>
<p>Points that are not assigned to a cluster are classified as .</p>
<p>For a data set <span class="math inline">\(D\)</span>, the DBSCAN algorithm determines clusters based on the above definition.
Figure <a href="#fig:dbscanResultExample"><strong>??</strong></a> shows the labels return by DBSCAN for the example considered above with respect to <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>..
We see that seven points are classified in a single cluster and three points are classified as noise.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/unnamed-chunk-19-1}  -->
<!-- } -->
<!-- \caption{\label{fig:dbscanResultExample} Cluster labeling for 10 data points using the DBSCAN algorithm with parameters $\epsilon = 3$ and $Minpts = 2$. Seven points are assigned to a single cluster and the remaining three are classified as noise.}\label{fig:unnamed-chunk-19} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-19"></span>
<img src="figures/unnamed-chunk-19-1.png" alt="\label{fig:dbscanResultExample} Cluster labeling for 10 data points using the DBSCAN algorithm with parameters $\epsilon = 3$ and $Minpts = 2$. Seven points are assigned to a single cluster and the remaining three are classified as noise." width="\textwidth" />
<p class="caption">
Figure 1.19:  Cluster labeling for 10 data points using the DBSCAN algorithm with parameters <span class="math inline">\(\epsilon = 3\)</span> and <span class="math inline">\(Minpts = 2\)</span>. Seven points are assigned to a single cluster and the remaining three are classified as noise.
</p>
</div>
<!-- \hypertarget{features-based-on-visual-diagnostics}{% -->
<!-- \subsection{Features Based on Visual Diagnostics}\label{features-based-on-visual-diagnostics}} -->
</div>
<div id="features-based-on-visual-diagnostics" class="section level3 hasAnchor" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Features Based on Visual Diagnostics<a href="index.html#features-based-on-visual-diagnostics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Much of the literature on ``explainable" algorithms are focused on black-box machine learning algorithms such as Random Forests or Multi-layer Neural Networks.
[More to say here?]
Less focused is placed on constructing explainable features.
Feature selection and engineering is a critical, often time-intensive step in the data analysis process that isn’t often</p>
<p>We use the visual diagnostic tools discussed in Chapter [5] to develop a set of features.
By definition, these features are human-interpretable unlike, for example, features that are calculated in the convolution layer of a convolutional neural network.
The interpretability of these features imply that they can be explained to forensic examiners or lay-people.
This will make it easier to introduce such methods into forensic labs and court rooms.</p>
<!-- \hypertarget{implementation-considerations}{% -->
<!-- \subsection{Implementation Considerations}\label{implementation-considerations}} -->
</div>
<div id="implementation-considerations" class="section level3 hasAnchor" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> Implementation Considerations<a href="index.html#implementation-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This cartridge case comparison pipeline is similar to other data analysis pipelines.
Much like other data analysis pipelines, the procedural details can be obscured as the goals of the analysis become more sophisticated.
This is helpful neither for the individual performing the analysis nor for any consumer of the results.
As such, it is worthwhile to design tools that make the data analysis procedure easier to implement and understand .</p>
<p>Beyond conceptualizing the cartridge case comparison procedure as a pipeline, we also implement the procedure in the R statistical programming as a sequence of algorithms that can programatically be connected together .
In particular, we utilize the pipe operator  available from the  R package .
This operator allows the output of one function to be passed as input to another without assigning a new variable.
Data can be incrementally transformed as they move from one function to another.</p>
<p>Implementing a data analysis procedure using the pipe operator allows the user to think intuitively in terms of verbs applied to the data.
Table <a href="#tab:pipelineTable"><strong>??</strong></a> illustrates two examples of pipelines that utilize the pipe operator.
The left-hand example shows how an R data frame can be manipulated using functions from the  package.
Functions like , , and  are simple building blocks that can be strung together to create complicated workflows.
The right-hand example similarly illustrates a cartridge case object passing through the comparison pipeline.
While the full comparison procedure is complex, the modularization to the , , and  steps, which can further be broken-down into simple building blocks, renders the process more understandable to, and flexible for, the user.</p>
<p>Figure <a href="#fig:taiEddyPreprocess"><strong>??</strong></a>, Figure <a href="#fig:ricePreprocess"><strong>??</strong></a>, Figure <a href="#fig:handwriterPreprocess"><strong>??</strong></a>, and Figure <a href="#fig:cmcRPreprocess"><strong>??</strong></a> illustrate how various forensic comparison algorithms use a modularized structure in their preprocessing procedures.
In each figure, a sequence of modular procedures are applied to a piece of evidence.
Figure <a href="#fig:taiEddyPreprocess"><strong>??</strong></a> shows the morphological and image processing preprocessing procedures used to remove the firing pin region from a 2D image of a cartridge case .
Figure <a href="#fig:ricePreprocess"><strong>??</strong></a> shows the procedure by which a 2D ``signature" of a bullet scan is extracted from a 3D topographical scan .
Figure <a href="#fig:handwriterPreprocess"><strong>??</strong></a> shows how an image of the written word ``csafe" is processed using the handwriter R package to break the word into individual  that can be further processed .
Finally, Figure <a href="#fig:cmcRPreprocess"><strong>??</strong></a> shows a 3D topographical cartridge case scan undergoing various procedures to isolate and highlight the breech face impressions.
These procedures are discussed in greater detail in Chapter 2.</p>
<p>By breaking the broader preprocessing step into modularized pieces, we can devise other arrangements of these preprocessing procedures that may improve the segmenting or emphasizing of the region of interest.
The modularity of the pipeline makes it easier to understand what the algorithm is doing ``under the hood" while a modularized implementation enables others to experiment with alternative versions of the pipeline.</p>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.7\textwidth]{images/taiEddyPreprocess}  -->
<!-- } -->
<!-- \caption{\label{fig:taiEddyPreprocess} A preprocessing procedure applied to a 2D image of a cartridge case to identify the firing pin impression. The procedure results in a 2D image of a cartridge case without the firing pin impression region.}\label{fig:unnamed-chunk-21} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-20"></span>
<img src="images/taiEddyPreprocess.png" alt="\label{fig:taiEddyPreprocess} A preprocessing procedure applied to a 2D image of a cartridge case to identify the firing pin impression. The procedure results in a 2D image of a cartridge case without the firing pin impression region." width=".7\textwidth" />
<p class="caption">
Figure 1.20:  A preprocessing procedure applied to a 2D image of a cartridge case to identify the firing pin impression. The procedure results in a 2D image of a cartridge case without the firing pin impression region.
</p>
</div>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.8\textwidth]{images/riceBulletPreprocessDiagram}  -->
<!-- } -->
<!-- \caption{\label{fig:ricePreprocess} A preprocessing procedure for extracting 2D bullet \`\`signatures" from a 3D topographic bullet scan. The procedure results in an ordered sequence of values representing the local variations in the surface of the bullet.}\label{fig:unnamed-chunk-22} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="images/riceBulletPreprocessDiagram.png" alt="\label{fig:ricePreprocess} A preprocessing procedure for extracting 2D bullet \`\`signatures&quot; from a 3D topographic bullet scan. The procedure results in an ordered sequence of values representing the local variations in the surface of the bullet." width=".8\textwidth" />
<p class="caption">
Figure 1.21:  A preprocessing procedure for extracting 2D bullet ``signatures" from a 3D topographic bullet scan. The procedure results in an ordered sequence of values representing the local variations in the surface of the bullet.
</p>
</div>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=.35\textwidth]{images/handwriterPreprocessDiagram}  -->
<!-- } -->
<!-- \caption{\label{fig:handwriterPreprocess} A preprocessing procedure applied to a handwriting image of the word \`\`csafe." The procedure results in a skeletonized version of the word that has been separated into graphemes as represented by orange nodes.}\label{fig:unnamed-chunk-23} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="images/handwriterPreprocessDiagram.png" alt="\label{fig:handwriterPreprocess} A preprocessing procedure applied to a handwriting image of the word ``csafe.&quot; The procedure results in a skeletonized version of the word that has been separated into graphemes as represented by orange nodes." width=".35\textwidth" />
<p class="caption">
Figure 1.22:  A preprocessing procedure applied to a handwriting image of the word ``csafe." The procedure results in a skeletonized version of the word that has been separated into graphemes as represented by orange nodes.
</p>
</div>
<!-- \begin{figure}[!htbp] -->
<!-- {\centering \includegraphics[width=\textwidth]{figures/preProcessPlots}  -->
<!-- } -->
<!-- \caption{\label{fig:cmcRPreprocess} A cartridge case undergoing various preprocessing steps. The procedure results in a cartridge case scan in which the breech face impressions have been segmented and highlighted.}\label{fig:unnamed-chunk-24} -->
<!-- \end{figure} -->
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="figures/preProcessPlots.png" alt="\label{fig:cmcRPreprocess} A cartridge case undergoing various preprocessing steps. The procedure results in a cartridge case scan in which the breech face impressions have been segmented and highlighted." width="\textwidth" />
<p class="caption">
Figure 1.23:  A cartridge case undergoing various preprocessing steps. The procedure results in a cartridge case scan in which the breech face impressions have been segmented and highlighted.
</p>
</div>
<p>Our implementation is structured to adhere to the ``tidy" principles of design .
The  is a collection of R packages that share an underlying design philosophy and structure.
The four principles of a tidy API are:</p>
<p>Adherence to these principles makes it easier to engage with and understand the overall data analysis pipeline.
In our application it also enables experimentation by making it easy to change one step of the pipeline and measure the downstream effects .
Each step of the cartridge case comparison pipeline requires the user to define parameters.
These can range from minor, such as the standard deviation used in a Gaussian filter, to substantial, such as choosing the algorithm used to calculate the similarity score.
So far, there is no consensus on the ``best" parameter settings.
A large amount of experimentation is yet required to establish these parameters.
A tidy implementation of the cartridge case comparison pipeline allows more people to engage in the validation and improvement of the procedure.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="this-is-the-title-of-the-first-paper.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
